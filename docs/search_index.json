[["index.html", "Model Your World: Introduction to Modeling and Simulation Chapter 1 Welcome 1.1 Teaching Team Introductions Gaj Sivandran 1.2 Course Highlights 1.3 What are the learning outcomes 1.4 Notes and Textbooks 1.5 Course Topics 1.6 Classroom Etiquette 1.7 Assessment", " Model Your World: Introduction to Modeling and Simulation Gaj Sivandran 2025-11-10 Chapter 1 Welcome 1.1 Teaching Team Introductions Gaj Sivandran Teaching Interests Design (freshman and senior) Fundamental engineering (statics, fluids) Environmental labs Water resources Research Interests Climate change Active learning pedagogy Simulation modeling Decision support (socio-economic modelling) Random Facts I have an 11yr old daughter that helps me write exam questions Dogs &gt;Cats, Cricket &gt; Baseball, AFL &gt; NFL, Vegemite &gt; Peanut butter I like to run very very long distances, stilling working on why 1.2 Course Highlights This is the first offering of this course - what does that mean? I’m looking for your continuous feedback on what is working and what is not I’m going apply “Just-In-Time” teaching philosophy. What this essentially means is the course material is dynamic, if there is a need from the class to cover a topic - we can add it in Accessibility requirements for the Spring. 1.3 What are the learning outcomes By the end of the course, students will be able to: Explain the philosophy of modeling: abstraction, assumptions, and trade-offs. Use AI tools responsibly to support model design, coding, and analysis. Build and analyze basic mathematical and computational models (e.g., growth, predator–prey, carbon cycles). Implement simulation techniques in R, including discrete-event, continuous-time, and agent-based models. Explore the roles of uncertainty, sensitivity, and validation in modeling. Communicate modeling results clearly through visualizations, reports, and presentations. Class Discussion Anything you want to add/remove/emphasize/de-emphasize? Class Discussion Lets find out why you all have decided to be here. With the people around you, discuss why you are taking this class. We’ll share back in 5mins. Class Discussion The way we code is changing rapidly right now with the development of LLMs. You might rightly ask “why do we need to know how to code?” Discuss it with the people at your table We are scientists! So it would be great to see arguments from all perspectives - play ‘devil’s advocate’ its more fun if we don’t all agree What should our goal be in light of your discussion? One of my favorite education quotes: “We are currently preparing students for jobs that don’t yet exist… using technologies that haven’t been invented… in order to solve problems we don’t even know are problems yet” - Richard Riley U.S. Secretary of Education (1993-2001) 1.4 Notes and Textbooks Quick Canvas tour I am writing and publishing the textbook on the fly. To make sure you are working with the most up-to-date version hit refresh or close the tab from time to time. 1.5 Course Topics This being the first year – this is my ambitious list of topics – we’ll see how far we get (maybe further) Introduction to Modeling – abstraction, assumptions, model types. AI &amp; Modeling Tools – using LLMs to support coding and model design. Mathematical Models – parameters, functions, growth models. Difference Equations – discrete-time dynamics, feedback, stability. Differential Equations – continuous-time systems in ecology/environment. Simulation as a Tool – discrete-event and continuous-time simulation. Uncertainty &amp; Sensitivity – calibration, robustness, validation. Spatial &amp; Agent-Based Models – individuals and space in systems. Communicating Models – visualization, ethics, limits. Capstone Showcase – final project presentations. Class Discussion Anything you want to add/remove/emphasize/de-emphasize? 1.6 Classroom Etiquette Please feel free to bring your breakfast/lunch to class – just be sure to clean up before you leave Bring whatever tech you need to take notes and engage. We will code in class so setting up R Studio is a good idea Ask questions – but please be respectful of all voices and views - wrong answers have more value than right ones! Norms - Class Discussion Lets come up with a set of rules and expectations for this class and then lets agree to follow them. 1.7 Assessment My goal with the assessments is to encourage you to put energy in the right places. I do not want to create busy work in this class. But - At the same time - sometimes having a due date forces us to do the things that are good for us, but not as much fun. Class Discussion Right now the assessment breakdown is: Homework &amp; Labs (40%) – Weekly assignments and lab reports where students build, test, and reflect on models. Final Project (40%) – Develop and present a model of a real-world environmental system (individual or group). Includes a written report and in-class presentation. Participation &amp; Preparation (20%) – In-class activities, peer feedback, and engagement in discussions. What should the project be worth? Here are the basics Milestone 1: Proposal &amp; Scoping (Week 3) Topic idea (1–2 paragraphs) Research question(s) Initial model concept (sketch or description) Deliverable: 1–2 page written proposal + brief in-class discussion Milestone 2: Background &amp; Model Design (Week 5) Short background review (1–2 pages, with at least 3–5 references) Model framework (diagram of variables, flows, assumptions) Plan for methods (what kind of model and why) Deliverable: Background report + 3–5 minute pitch with peer Q&amp;A Milestone 3: Prototype Model (Week 7) First working version of your model in R At least one test run with outputs 1-page reflection on challenges and next steps Deliverable: Code + reflection memo Milestone 4: Model Refinement &amp; Analysis (Week 9) Improved and more complete model Sensitivity tests, scenario comparisons, or uncertainty analysis At least 2–3 polished visualizations Deliverable: Draft results section (1–2 pages) with figures Milestone 5: Final Report &amp; Showcase (Week 10) Final written report (6–8 pages, including intro, methods, results, discussion, references, and code appendix) A poster of your work – we’ll have our own digital poster session at the end of this course. Deliverable: Report + poster presentation Grading Detailed rubrics will be provided for each milestone. For Milestones 1 through 4, you will be given the opportunity to address feedback to earn back any points lost during the first submission. The project grade will be broken down as follows: Proposal &amp; Scoping: 10% Background &amp; Model Design: 15% Prototype Model: 15% Refinement &amp; Analysis: 20% Final Report &amp; Presentation: 40% Being an elective, it means you all have different levels of preparation for this course. Grades will focus on your growth rather than comparisons to other students. Admin Every Friday will be project work. Either setting the groundwork for your project or delivering a milestone. This Friday - come with a rough idea of what system you’d like to model/simulate. We’ll use the class time to workshop the idea. There will be a graded Canvas discussion board where you will need to drop your idea into. "],["week-1-what-is-modeling.html", "Chapter 2 Week 1: What is Modeling? 2.1 What we will be doing this week 2.2 Learning Objectives 2.3 Do, Watch, Listen, Read: Student Preparation For The Week 2.4 Let’s Build a Model of Learning 2.5 What Is Systems Thinking and Simulation? 2.6 Abstraction—and Why Is It So Hard? 2.7 The Problem-Solving Process 2.8 Intro to Prompt Engineering", " Chapter 2 Week 1: What is Modeling? Theme: Orientation, purpose of modeling, and first steps in RStudio Goal: Set a positive tone, introduce modeling as a mindset, and start building confidence with tools like RStudio and LLMs. 2.1 What we will be doing this week Wed: Welcome, course expectations, and an introduction to modeling as a way of thinking about systems. We’ll define what a model is and why it’s useful in environmental science. You’ll also begin brainstorming the systems you’re most interested in. Thu: Build and sketch simple conceptual models. We’ll talk about abstraction, simplification, and assumptions. You’ll draw your own system and explore a visual tool like LOOPY. Fri: Begin working in RStudio! We’ll introduce the platform, walk through a tutorial, and use an LLM to help you write your first line of R code. This day is all about taking that first step—together. Brainstorm and team up for class projects. 2.2 Learning Objectives By the end of Week 1, students should be able to: Define what a model is and explain its purpose in environmental science. Identify abstraction, simplification, and assumptions in a given model. Sketch and describe a simple conceptual model of a natural system. Critically assess the usefulness and limitations of models. Navigate basic RStudio functions and use LLMs to scaffold a simple model script. Start to formulate your projects 2.3 Do, Watch, Listen, Read: Student Preparation For The Week 2.3.1 Download and Install RStudio Go the the following link to download RStudio RStudio Desktop Download Note: Scroll down to find windows/mac version Be sure to get RStudio - not R, RStudio includes everything you need and it creates a much easier user interface to work with Now don’t worry - the expectation for this course is you have never worked with a coding language before. The purpose of this course is to offer a low stakes way to engage with coding and modeling, see its potential, and then take several other courses that focus on coding. 2.3.2 Do this tutorial Before we can explore the environmental systems we care about—like climate change, conservation, and sustainability, we need tools to help us think clearly and creatively. One of those tools is RStudio. Let’s be honest: learning a new programming environment can feel overwhelming. That’s okay. It’s normal to feel stuck, confused, or even frustrated when you’re starting out. But here’s the truth: We don’t get to do cool science or explore new ideas without doing hard things first. This tutorial will walk you through the basics of RStudio: how it looks, how to run code, and how to get started. You don’t need to master everything right away—just take it one step at a time. We’ll practice together in class, and you’ll have support from me, your peers, and even AI tools along the way. 2.3.3 5min Read Study Confirms Climate Models are Getting Future Warming Projections Right 2.4 Let’s Build a Model of Learning This is the way I see your learning. It’s a simple feedback between coming to class and office hours. Activity: What Drives Learning, Enjoyment, and Science? Your First Model: A Feedback Loop The diagram you’re looking at is called a systems model—specifically, a causal loop diagram. It’s the kind of model we use to: Represent complex systems using simple relationships Visualize feedback (both positive and negative) Experiment with how change flows through a system In this case, the system is about you as a learner. What’s Going On Here? Learning increases Enjoyment, which in turn increases Science. Factors can be added (green nodes) or subtracted (red nodes) from each part of the system. You can interact with the model by dragging the slider or clicking the nodes to see what happens. Task This is the first model of the course—and it’s about you. Your task: Play with the model: What happens when you increase the positive input? What if something subtracts from learning? As a group, add ideas to these categories: -What adds to your learning? What subtracts from your learning? Sketch your own version of the loop: What would you add to the system to help you succeed in this class? Can you identify a feedback loop (something that reinforces itself)? What’s something that could make the system spiral negatively? Wrap-up Discussion What surprised you about how small changes affected the system? How could this approach be used to model an environmental system? 2.5 What Is Systems Thinking and Simulation? Systems thinking is about seeing the world as an interconnected web of relationships—where change in one part of a system can ripple through others in surprising ways. It’s a mindset that helps us understand the feedback loops, delays, and patterns that shape everything from ecosystems and climate to cities and communities. Simulation is how we bring those systems to life. It’s the process of building simplified, dynamic representations of complex systems so we can ask questions, test scenarios, and explore “what if” ideas—without needing to experiment in the real world. Simulations help us: Focus on what really matters in a system Explore how change unfolds over time Make the invisible visible Together, systems thinking and simulation allow us to understand environmental challenges more deeplyand to imagine and test solutions before we act. In this course, we’ll use both to explore systems you care about, from forest ecosystems to climate strategies and sustainable cities. 2.6 Abstraction—and Why Is It So Hard? Abstraction is the process of simplifying a complex reality by focusing only on the parts that matter most for a specific question or purpose. It’s about stripping away detail so you can see the system more clearly. That might sound simple—but it’s not. In science (and in life), we’re surrounded by messy, interconnected realities. When we build a model or design a simulation, we have to decide: What do we include? What do we leave out? What assumptions are we making? Those are hard decisions. There’s no single “right” answer. Every abstraction involves a trade-off between realism and usability. Too simple, and the model might be useless. Too complex, and it might be impossible to understand or use. But here’s why abstraction matters: It’s the starting point for every scientific model, every simulation, every breakthrough idea. Abstraction helps us: Make sense of overwhelming complexity Communicate ideas clearly Focus our attention on what’s driving change Build models we can analyze, test, and improve Every person’s model will be different because of the unique structure and assumptions they make along the way. That’s not a problem—it’s what makes modeling so powerful and flexible. In climate modeling for instance, they way we create projections of future climates is by taking the average response of many climate models to a expected change in forcings. Sketching Exercise A sketch is a visual model. Task: Draw a picture of a chair Share your picture with those around you What assumptions did you all make in your model Which sketch was right? 2.7 The Problem-Solving Process At the heart of modeling and simulation is a desire to solve real-world problems—questions about climate, conservation, cities, and sustainability that don’t have simple answers. To tackle those problems effectively, we need a clear and flexible process. Here’s a step-by-step framework we’ll use throughout this course: 2.7.1 Define the Problem Start by clearly stating: What system are you trying to understand or improve? What question are you trying to answer? Who is affected by this problem, and why does it matter? Example: “How can we reduce the urban heat island effect in our city?” 2.7.2 Simplify and Abstract No model can capture every detail. So ask: What are the key components of this system? What can we leave out (at least for now)? What assumptions are we making? This is where abstraction comes in—choosing what to keep and what to simplify so that the model is useful without being overwhelming. 2.7.3 Build a Conceptual or Mathematical Model Now sketch or code a model that represents the system: Use diagrams, equations, or simulations Identify feedbacks, delays, and influences Decide how time and change are represented This is where your system takes shape. 2.7.4 Simulate and Explore Run your model. Ask: What happens when you tweak variables? Are the results stable, surprising, or sensitive? Does this align with what you expected—or challenge it? Simulation helps us test our ideas before we act in the real world. 2.7.5 Evaluate and Refine A model is never “finished.” It’s a tool for learning. What are the model’s strengths and weaknesses? How could it be improved? What new questions did it raise? Sometimes refining the problem is just as important as refining the solution. Science is often getting to the next why? 2.7.6 Communicate Even the best model won’t make a difference if no one understands it. Can you explain your model clearly—visually, verbally, or interactively? Who needs to hear this, and how should you frame it? Good science isn’t just about building models—it’s about sharing them. By following this process, you’re not just solving technical problems—you’re learning how to think critically, collaborate effectively, and design solutions that matter. 2.8 Intro to Prompt Engineering Before we dive into RStudio, we need to learn a surprisingly powerful skill: how to talk to AI. When we use tools like ChatGPT or Copilot, the results we get depend entirely on how we frame the question. This practice—designing questions and instructions for an AI—is called prompt engineering. Good prompts: Give clear context State what you’re trying to do Are specific, but not overloaded with details Might include examples or data structure Here’s the difference: ❌ Less helpful prompt: “How do I R?” ✅ More helpful prompt: “I’m working in RStudio. I have a dataset called CO2, and I want to make a scatter plot of uptake vs. concentration. Can you show me the code?” 2.8.1 Why This Matters Prompt engineering isn’t just about getting the right code—it’s about learning how to collaborate with AI to think through problems. In this course, you’ll use LLMs to: Get unstuck when your code won’t run Try new ideas quickly Explore how changing your prompt changes the response And just like with any tool, the more thoughtfully you use it, the better it works. Dataset Spotlight: CO₂ Uptake in Grass Plants About the Experiment The \\(CO_2\\) dataset comes from a classic plant physiology experiment that measured how grass plants respond to different concentrations of carbon dioxide under varying conditions. Researchers wanted to understand how \\(CO_2\\) levels , plant type , and treatment (chilled vs. non-chilled) affected the rate at which plants take up carbon from the atmosphere—a key part of understanding photosynthesis and climate-plant interactions. Specifically, they measured \\(CO_2\\) uptake in grass plants from two groups: Quebec (cooler climate) Mississippi (warmer climate) Some plants were kept at normal temperatures, while others were chilled to simulate colder conditions. The goal was to see how these factors influenced carbon uptake at different CO₂ concentrations. Dataset Structure This dataset includes 84 observations and 5 variables: Variable Description Plant Identifier for the individual plant (Qn or Mn, where Q = Quebec, M = Mississippi) Type Origin of the plant: \"Quebec\" or \"Mississippi\" Treatment \"chilled\" or \"nonchilled\" (i.e., whether the plant was cooled) conc CO₂ concentration in the ambient air (in mL/L) uptake Rate of CO₂ uptake (μmol/m²/s) – this is the response variable Why It’s Useful for Us This dataset is a great first modeling tool because it’s: Small and easy to understand Rich enough for meaningful patterns Great for visualization, group comparisons, and *basic regression modeling** A real-world example of how environmental variables interact "],["week-2-llms-and-modeling-support.html", "Chapter 3 Week 2: LLMs and Modeling Support 3.1 Learning Goals 3.2 Do, Watch, Listen, Read: Student Preparation for the Week 3.3 Introduction to LLMs in Modeling (Mon) 3.4 Capabilities and Limits of LLMs 3.5 LLMs in Environmental Modeling Workflows 3.6 Prompt Engineering for Model Development (Tue &amp; Wed) 3.7 Learning goals 3.8 Principles of effective prompts: clarity, context, constraints 3.9 Prompt types: zero-shot, few-shot, role-based (with modeling examples) 3.10 Iterative refinement and debugging prompts (the LEI loop) 3.11 ADE Prompt Library &amp; Activity 3.12 Critical Reflection: When to Use AI Tools 3.13 Reproducibility, Documentation, and Troubleshooting (Fri) 3.14 First Model - ADE", " Chapter 3 Week 2: LLMs and Modeling Support 3.1 Learning Goals By the end of this week, students should be able to: Explain what large language models (LLMs) are and how they can support simulation and coding. Apply prompt engineering techniques to improve model development. Use LLMs to reframe and clarify environmental modeling challenges. Critically evaluate when and how it is appropriate to use AI tools in science. Incorporate LLMs into workflows for reproducibility, documentation, and troubleshooting in R. 3.2 Do, Watch, Listen, Read: Student Preparation for the Week 3.2.1 Do Open an LLM tool (ChatGPT, Claude, Gemini and ask it to: Explain a simple scientific concept (e.g., greenhouse effect) in plain language. Write R code to simulate 10 years of daily temperature with a slight warming trend. Compare the outputs: What worked well? What didn’t? 3.2.2 Watch: Video for Discussion Video: Master 80% of Prompt Engineering In 10 Minutes! (YouTube, 2024) Intro: This short video introduces the foundations of prompt engineering and then builds into practical strategies. It explains the core elements of a good prompt, highlights three advanced techniques, and shares a few bonus tips for refining outputs. The goal is to show how structured, intentional prompting can make AI much more useful in practice—especially for coding, modeling, and explanation tasks. Questions to Ponder While Watching: What are the “core elements” of prompt design highlighted in the video? Why are they essential? Which advanced technique seems most useful for environmental modeling tasks (e.g., generating R code, simulation explanations)? How might you apply one of the bonus tips when working on your Week 2 activities? Can you think of a situation where a poorly written prompt could cause confusion or errors in scientific modeling? How would you fix it? 3.2.3 Read: Reading for Discussion Article: Reconciling the contrasting narratives on the environmental impact of large language models (Nature Scientific Reports, 2024) Intro: As we consider how to use large language models (LLMs) in environmental science, it’s important to recognize that these tools themselves have environmental costs. This article examines the energy use and carbon footprint of training and running LLMs, compares them with human labor, and discusses how different assumptions lead to contrasting narratives about their sustainability. It’s a nuanced look at the trade-offs between the benefits of AI and the resources required to power it. Questions to Ponder While Reading: What are the main sources of environmental impact from LLMs (training vs. deployment)? How do the authors compare the impacts of LLMs to traditional human-driven approaches? Which assumptions (e.g., electricity sources, hardware lifespans) most influence the conclusions? How might these trade-offs shape your perspective on using LLMs in scientific modeling workflows? These materials will help you see both sides of using LLMs: as a tool for speeding up coding and simulation, and as a system with important limits you must understand as a scientist. 3.3 Introduction to LLMs in Modeling (Mon) Large language models (LLMs) are a new class of tools that can transform the way scientists and students approach coding, simulation, and communication. They are not replacements for human expertise, but they can act as collaborators—helping us generate ideas, translate complex models into accessible explanations, and troubleshoot code. In this chapter, we explore what LLMs are, how they developed, and how they connect to environmental modeling workflows. 3.3.1 What Are LLMs? A large language model (LLM) is a type of artificial intelligence system trained on massive collections of text. The core idea is surprisingly simple: given a sequence of words, the model predicts the most likely next word. By stacking billions of parameters and training on billions of words, LLMs learn patterns of grammar, style, reasoning, and even coding syntax. Think of an LLM as a highly advanced autocomplete. Instead of only finishing a single word, it can continue a sentence, draft an essay, write a block of R code, or explain a scientific model in plain language. 3.3.2 A Brief History Early 2010s: Models such as word2vec and GloVe mapped words into mathematical space, capturing similarities (“river” is close to “stream”). 2017: The transformer architecture was introduced, allowing models to learn long-range patterns in text. 2018–2020: OpenAI’s GPT-2 and GPT-3 showed that scaling up data and parameters led to dramatic improvements in fluency. 2022–present: Public releases of GPT-4, Claude, Gemini, and other models made LLMs widely accessible for coding, writing, and research. This rapid evolution means today’s students can use tools that did not exist even a few years ago. Discussion What do you think an appropriate use of LLMs are. Discuss with your table and lets see if we can build some community guidelines (for this class and beyond) 3.4 Capabilities and Limits of LLMs 3.4.1 Capabilities One of the reasons LLMs have gained such traction in research and education is their versatility. They are capable of generating readable text in many different styles, ranging from concise scientific summaries to conversational explanations that make technical concepts more approachable. This flexibility allows them to adapt their tone depending on the intended audience, whether it is a group of peers, policymakers, or the general public. Beyond writing, LLMs are particularly useful for producing and troubleshooting code across multiple languages, including R, Python, and MATLAB. Students working through simulations can quickly draft starter scripts, identify syntax errors, or explore alternative approaches to the same problem. In addition, LLMs can act as powerful summarization tools, condensing long articles, large datasets, or complex equations into clear, digestible insights that highlight key trends or ideas. Finally, perhaps one of their most accessible features is the ability to translate technical content into plain language, making specialized knowledge understandable to non-experts. This capacity to bridge the gap between complexity and clarity is especially valuable in environmental science, where communicating models and data to diverse audiences is critical for impact. 3.4.2 Limits Despite their impressive capabilities, LLMs come with important limitations that must be acknowledged. A well-documented issue is hallucination, where the model generates text that sounds plausible but is factually incorrect or even entirely fabricated. This can be especially problematic in scientific work, where accuracy is paramount. LLMs also reflect the biases present in their training data, which means that outputs may unintentionally reproduce stereotypes or emphasize certain perspectives while ignoring others. Another limitation is that these systems lack true reasoning or understanding—they do not “know” science in the way humans do, but instead predict patterns based on statistical relationships in text. This means their explanations can oversimplify concepts or miss critical assumptions. Finally, there are reproducibility challenges, since the same prompt can yield slightly different outputs depending on the model and context, making it harder to standardize results for scientific workflows. Recognizing these limits helps ensure that we use LLMs critically, as aids to human reasoning rather than substitutes for it. Reflection Prompt Think about a task in environmental modeling you’ve worked on recently (coding, data analysis, or communicating results). Which of the capabilities described here could have supported your work? Which limitations would you need to watch out for? How might you balance the efficiency of using an LLM with the need for accuracy and scientific rigor? 3.5 LLMs in Environmental Modeling Workflows How do LLMs connect to the practice of environmental modeling? Their utility lies not in replacing the scientist, but in serving as a flexible assistant at different stages of the workflow. One of the most immediate applications is code generation. For instance, a student working on a logistic population growth model in R can prompt an LLM to draft the basic script. Even if the generated code is not perfect, it provides a working foundation that can be edited and refined, reducing the time spent on tedious setup. Similarly, LLMs can assist with documentation support. In RMarkdown or other coding environments, clear documentation is essential for reproducibility, yet students often overlook it. By asking an LLM to add explanatory comments, create section headers, or translate code into step-by-step descriptions, the workflow becomes easier to follow both for the original author and for future collaborators. Another important use is simulation explanation. Equations that may appear abstract to non-specialists—such as those describing exponential growth, diffusion, or temperature response—can be reframed by an LLM into accessible narratives. For example, the logistic growth equation can be explained as a story of a population that grows quickly at first but slows as resources become scarce, eventually leveling off at a carrying capacity. These applications highlight the potential of LLMs to streamline scientific work: they can help students start coding more quickly, reduce frustration by catching errors or filling in gaps, and make models more communicable to a wider audience. At the same time, none of these tasks can be fully delegated without oversight. Generated code must be tested for accuracy, documentation must be checked for completeness, and plain-language explanations must be reviewed to ensure they do not omit critical assumptions. In this way, LLMs become collaborative tools that support efficiency and clarity while keeping the responsibility for scientific rigor firmly in human hands. Activity: Explain a Complex Model with Stepwise Prompting We’ll use stepwise (chain-of-thought–style) prompting to unpack a very complex partial differential equation into clear, audience-appropriate language without asking the AI to reveal its private reasoning. The goal is to force a structured, term-by-term explanation and surface assumptions. Note: we are purposefully using a complex example here so that we can really see the value and dangers of utilizing a LLM for environmental modeling. Model The Advection–Diffusion (or Dispersion) Equation for pollutant transport in a river: \\[ \\frac{\\partial C}{\\partial t} = D \\frac{\\partial^2 C}{\\partial x^2} - v \\frac{\\partial C}{\\partial x} - kC \\] - \\(C\\): concentration at position \\(x\\) and time \\(t\\) - \\(D\\): diffusion coefficient (mixing) - \\(v\\): flow velocity (downstream transport) - \\(k\\): decay rate (removal) Step 1 — Your Own Explanation Write a plain-language explanation for a non-scientist audience (e.g., a community group). If you have no idea whats going on - take a guess. Go term by term and see if you can decipher whats going on. Step 2 — Baseline AI Explanation Ask an LLM for a plain-language explanation. Save the response. Baseline prompt: Explain the equation below in plain language for a non-scientist audience. \\[ \\frac{\\partial C}{\\partial t} = D \\frac{\\partial^2 C}{\\partial x^2} - v \\frac{\\partial C}{\\partial x} - kC \\] Keep it to 6–8 sentences. Take a second here and compare your result with those at your table? Are thy idenitical? Step 3 — Stepwise Prompting (Structured Sections) Now force structure so the AI unpacks complexity term-by-term and surfaces assumptions. Stepwise prompt template (copy-paste) Explain the equation below using labeled sections. Do not show your internal reasoning; present only your final explanation. Sections (use headings): 1) Term-by-term meaning — explain each term in one sentence. 2) Physical interpretation — connect each term to a river process with a brief analogy. 3) Assumptions — list key modeling assumptions (e.g., dimensionality, parameter constancy, uniform mixing). 4) Units &amp; parameters — specify typical units for \\(C, D, v, k\\). 5) Edge cases — describe what happens if \\(D=0\\), \\(v=0\\), or \\(k=0\\). 6) Plain-language summary — 3 sentences for a public audience. Equation: \\[ \\frac{\\partial C}{\\partial t} = D \\frac{\\partial^2 C}{\\partial x^2} - v \\frac{\\partial C}{\\partial x} - kC \\] Step 4 — Compare &amp; Critique Clarity: Which version (baseline vs. stepwise) is clearer and why? Completeness: Did the stepwise version expose assumptions or units the baseline missed? Accuracy: Note any incorrect claims or overconfidence. Most importantly - which version did you learn something from? Step 5 — Constraint Refinement Re-prompt with tighter constraints to match a specific audience. Audience-tuning examples Policy brief style (≤150 words, 8th-grade reading level). Technical appendix style (include parameter ranges and citations placeholder). Infographic caption style (≤90 words, 3 bullets + 1 summary sentence). Step 6 — Mini-Deliverable Submit: (1) your own explanation, (2) baseline AI output, (3) stepwise output, (4) a 3–5 bullet critique comparing them, and (5) one audience-tuned version. Extension (optional) Ask the AI to propose a simple diagram description (no image needed): axes, arrows for diffusion/advection, and a decay cue. Use this as a storyboard for a figure you might create later. 3.6 Prompt Engineering for Model Development (Tue &amp; Wed) Principles of effective prompts: clarity, context, constraints. Prompt types: zero-shot, few-shot, role-based prompts. Iterative refinement and debugging prompts. Activity: Students write and test prompts for generating R code to simulate temperature data. 3.7 Learning goals By the end of today, students will be able to: - craft clear, contextualized, and constrained prompts that produce correct code and documentation; - select and combine zero-shot, few-shot, and role-based prompts for modeling tasks; - iteratively refine outputs using structured critique, unit tests, and debugging prompts; - apply these skills to build and explain a simple advection–diffusion–decay (ADE) model in R. 3.8 Principles of effective prompts: clarity, context, constraints Clarity (goal + audience): State the task and who it is for. “Write R code for first-year environmental science students.” Context (problem + assumptions): Include the equation, domain, units, and assumptions. “1-D ADE, L = 10 km, v = 0.2 m/s, D = 10 m²/s, k = 1e-5 s⁻¹, Dirichlet at x = 0, Neumann at x = L.” Constraints (format + checks): Specify interfaces, style, tests, plots, and failure modes. “Return a function run_ade(params); add a CFL check; produce one line plot and one time-series plot; comment every major step.” Success criteria: Tell the model how you will judge success. “Code must run without additional packages beyond ggplot2 and dplyr.” Prompt template ASK: &lt;what to build/explain&gt; CONTEXT: &lt;equations, domain, units, assumptions&gt; CONSTRAINTS: &lt;APIs, style, allowed packages, runtime, plots&gt; CHECKS: &lt;tests/diagnostics to include&gt; OUTPUT FORMAT: &lt;function name, file structure, markdown section, etc.&gt; AUDIENCE: &lt;novice, advanced, instructor notes&gt; 3.9 Prompt types: zero-shot, few-shot, role-based (with modeling examples) Zero-shot — no examples, just a precise specifiction (spec) “Implement a Crank–Nicolson diffusion step and 1st-order upwind advection for the 1-D ADE; include a CFL diagnostic and return a tibble of (time_h, x, C).” Few-shot — show a small, high-quality example to anchor style/format. Provide a short example of a function signature and one test, then ask for an analogous function for ADE. Role-based — assign the model a persona to set expectations and tone. “You are a hydrology TA. Produce commented, teachable R code and insert two discussion questions that probe assumptions.” When to mix: Start role-based + zero-shot to draft; add few-shot when you need consistent structure (e.g., identical plotting themes across labs). 3.10 Iterative refinement and debugging prompts (the LEI loop) L — Launch a first draft with strict constraints. E — Evaluate using tests, plots, and quick sanity checks (mass balance, units, stability numbers). I — Iterate with targeted prompts: Examples: “Diagnose: Why does concentration become negative near the boundary? Propose two fixes and implement the safer one.” “Add input validation: stop with a clear message when CFL &gt; 0.9 or when D &lt; 0.” “Refactor into setup_grid(), step_CN(), step_advect(), apply_decay(). Return a named list.” “Generate three unit tests using testthat for: CFL computation, Neumann boundary, non-negative concentration.” “Write a 5-line docstring explaining assumptions and limitations for non-experts.” 3.11 ADE Prompt Library &amp; Activity Our goal is to build and explain a simple Advection–Diffusion (ADE) model in class using careful prompts and iterative refinement. 3.11.1 What you’ll build (at a glance) An R function run_ade(params) that simulates \\[ \\frac{\\partial C}{\\partial t} \\;=\\; D\\,\\frac{\\partial^2 C}{\\partial x^2} \\;-\\; v\\,\\frac{\\partial C}{\\partial x} \\;-\\; k\\,C \\] on \\([0,L]\\) with Dirichlet at \\(x=0\\) and zero-gradient (Neumann) at \\(x=L\\). Built-in checks for CFL and diffusion number \\(r_D\\). Two plots: (i) spatial profiles at selected times, (ii) time series at stations. Short, plain-language documentation of assumptions and limitations. 3.11.2 Spec → Code Write an R function run_ade(params) that simulates the 1-D advection–diffusion equation \\[ \\frac{\\partial C}{\\partial t} = D\\,\\frac{\\partial^2 C}{\\partial x^2} - v\\,\\frac{\\partial C}{\\partial x} - k\\,C \\] on the domain \\([0, L]\\) with Dirichlet at \\(x=0\\) and zero-gradient (Neumann) at \\(x=L\\). Include: a CFL check, diffusion number \\(r_D\\), two plots (profiles and station time-series), and comments suitable for first-year students. 3.11.2.1 Explain assumptions In 5 bullets, state modeling assumptions (dimensionality, parameter constancy, mixing, linear decay) and one situation where each assumption breaks. 3.11.2.2 Stability &amp; units Add a helper diagnostics() that prints CFL, Péclet, and a units table (C in mg/L, v in m/s, D in m²/s, k in 1/s). Warn when CFL &gt; 0.9. 3.11.2.3 Visualization Plot profiles at \\(t=\\) 0, 1, 3, 6 h and time-series at \\(x=\\) 1, 5, 9 km. Use clear axis labels and a legend; keep plotting code under 20 lines. In-class group activity (45–50 min): “Prompt → Plan → Build → Verify” Uses chain-of-thought prompting safely by asking the model to output a plan (step list/pseudocode) before code. Deliverables: prompt(s), plan, R script, two plots, short reflection. 3.11.2.4 Common pitfalls &amp; how to prompt around them Vague goals → meandering code → “Limit the solution to one function run_ade; ≤ 80 lines; include two plots and a diagnostics print.” Hidden assumptions → “List all assumptions you made; mark each as ‘required’ or ‘replaceable’.” Boundary errors → “Explain, in words, how you implement Dirichlet at \\(x=0\\) and zero-gradient at \\(x=L\\); then show the exact index operations.” Numerical instability → “Compute and print CFL and \\(r_D\\); if CFL &gt; 0.9, automatically shrink dt and state the new value.” Over-fancy output → “No external packages beyond ggplot2/dplyr; avoid themes; keep defaults.” 3.12 Critical Reflection: When to Use AI Tools Deciding when to use an LLM is not always straightforward. These tools offer powerful benefits but also come with risks and ethical concerns. As environmental scientists, we need to think critically about how AI fits into our workflows and how it might affect the quality, accessibility, and trustworthiness of science. 3.12.1 Benefits LLMs can accelerate many parts of the research process. They provide speed, helping draft code, summaries, or explanations in seconds. They also increase accessibility, lowering the entry barrier for students or collaborators who may not yet have advanced coding or writing skills. LLMs often spark creativity, generating new ways of framing a problem or suggesting approaches we might not have considered. Finally, they offer support for non-experts: a student new to modeling can use an LLM to explain equations or code in plain language, building confidence and understanding more quickly. 3.12.2 Risks and Limits At the same time, AI-generated content comes with important limitations. The most pressing concern is accuracy: LLMs can produce convincing but incorrect answers. This problem, often called hallucination, makes it dangerous to rely on AI outputs without verification. Models also inherit biases from their training data, which can shape the tone, assumptions, or inclusivity of their responses. There are also ethical concerns, including the environmental cost of training large models, the potential for plagiarism or misuse, and broader questions about authorship and credit in scientific work. Recognizing these risks is essential for responsible use. 3.12.3 Guidelines for Responsible Use in Environmental Science To make AI a constructive tool rather than a crutch, we can follow a few guidelines: Use AI to support, not replace, expertise. Treat LLMs as collaborators that generate drafts, not as authorities. Verify and validate outputs. Always test AI-generated code and fact-check explanations. Document when AI was used. Transparency helps others understand how results were created. Consider the audience. Decide whether an AI-generated explanation is appropriate for a scientific paper, a classroom activity, or public communication. Reflect on ethics. Think about sustainability, fairness, and responsible authorship when integrating AI into research. Debate — Should We Use AI in Research? Divide into groups. Half the class argues for using LLMs in environmental research, and half argues against. Use examples from your own experience and the guidelines above. After the debate, reflect as a group: Which arguments were most persuasive? What conditions or safeguards make AI use acceptable? Where should we draw the line between helpful assistance and over-reliance? 3.13 Reproducibility, Documentation, and Troubleshooting (Fri) Using LLMs to improve reproducibility: RMarkdown templates, commenting code, documenting decisions. Hands-on: Simulate a simple temperature model in R (linear warming trend + random noise). Use LLMs for: - Suggesting code improvements. - Adding explanations and comments. - Identifying potential reproducibility pitfalls. Reflection: How does AI support or hinder scientific reproducibility? 3.14 First Model - ADE The advection–diffusion equation (ADE) is a fundamental tool in environmental science for describing how substances such as pollutants, heat, or nutrients move and transform in natural systems. It combines three processes — advection (transport by bulk flow), diffusion (spreading due to mixing or molecular motion), and decay (loss by reaction, degradation, or uptake). Together, these processes govern how concentrations change in space and time. 3.14.1 General Form In one spatial dimension, the ADE is written as: \\[ \\frac{\\partial C}{\\partial t} = D \\frac{\\partial^2 C}{\\partial x^2} - v \\frac{\\partial C}{\\partial x} - kC \\] where: - \\(C(x,t)\\) = concentration of the substance at location \\(x\\) and time \\(t\\) - \\(D\\) = diffusion (or dispersion) coefficient \\((\\text{L}^2/\\text{T})\\) - \\(v\\) = advective velocity (bulk flow speed, \\(\\text{L}/\\text{T}\\)) - \\(k\\) = decay rate constant \\((1/\\text{T})\\) This is a partial differential equation (PDE) because it describes how concentration changes both with respect to time (\\(t\\)) and space (\\(x\\)). 3.14.2 Term-by-Term Meaning Diffusion Term \\((D \\frac{\\partial^2 C}{\\partial x^2})\\): Captures the natural tendency of a substance to spread out, whether through molecular diffusion (random particle motion) or turbulent mixing. In rivers, this reflects how pollutants disperse laterally and longitudinally. Advection Term \\((-v \\frac{\\partial C}{\\partial x})\\): Represents bulk transport due to flow. In a river, advection moves pollutants downstream at approximately the mean flow velocity. Decay Term \\((-kC)\\): Accounts for processes that remove the substance over time. Examples include radioactive decay, microbial degradation of organic matter, or chemical reactions that break down contaminants. 3.14.3 Assumptions Behind the ADE Like all models, the ADE relies on simplifying assumptions: 1. Homogeneity of parameters: \\(D\\), \\(v\\), and \\(k\\) are assumed constant in space and time. 2. One-dimensional flow: The river or system is treated as a single streamline, ignoring lateral and vertical variation. 3. Continuum assumption: Concentration is treated as a smooth, continuous field rather than individual particles. 4. Linear processes: Each term acts independently and linearly, with no feedbacks or nonlinear effects. These assumptions make the equation mathematically tractable, but real systems often require adjustments or numerical solutions to capture complexity. 3.14.4 Applications in Environmental Science Rivers and Streams: Tracking the downstream fate of pollutants (e.g., nutrients, heavy metals, thermal plumes). Atmosphere: Modeling dispersion of air pollutants under wind flow and turbulent mixing. Groundwater: Describing contaminant transport through porous media. Oceans and Lakes: Simulating nutrient plumes or thermal pollution. In each case, the relative importance of advection, diffusion, and decay depends on system parameters. A fast river with low diffusion behaves differently than a stagnant pond with strong decay. 3.14.5 Analytical Solutions For simple cases, the ADE has analytical solutions. A classic example is the instantaneous point source (a sudden spill at \\(x=0\\), \\(t=0\\)) in an infinite domain: \\[ C(x,t) = \\frac{M}{\\sqrt{4\\pi D t}} \\exp \\left( -\\frac{(x - vt)^2}{4Dt} - kt \\right) \\] where \\(M\\) is the mass released. This solution shows a Gaussian plume that spreads (due to diffusion), shifts downstream (due to advection), and decreases in height (due to decay). 3.14.6 Numerical Solutions For realistic boundary conditions (finite rivers, variable flows), numerical methods such as finite difference, finite element, or particle tracking are used. These methods discretize space and time, approximating how concentration evolves step by step. 3.14.7 Key Dimensionless Numbers Two ratios help characterize transport: Péclet Number (\\(Pe\\)): \\[ Pe = \\frac{vL}{D} \\] Ratio of advection to diffusion. High \\(Pe\\) means transport is dominated by flow. Damköhler Number (\\(Da\\)): \\[ Da = \\frac{kL}{v} \\] Ratio of reaction/decay to advection. High \\(Da\\) means rapid decay compared to transport. Together, \\(Pe\\) and \\(Da\\) guide whether a pollutant plume will spread, persist, or disappear quickly. 3.14.8 Visualization Low velocity, high diffusion: plume spreads symmetrically around the release point. High velocity, low diffusion: plume moves downstream as a narrow band. Strong decay: plume shrinks and may vanish before traveling far. 3.14.9 Reflection Questions Which term (advection, diffusion, or decay) dominates in a fast-flowing river vs. a still pond? How does increasing \\(D\\) change the shape of a pollution plume? What are the consequences of assuming one-dimensional flow when rivers have significant lateral mixing? How might climate change (altered flow velocities, higher temperatures) affect ADE parameters? "],["week-3-mathematical-models-parameters-functions-and-growth.html", "Chapter 4 Week 3: Mathematical Models – Parameters, Functions, and Growth 4.1 Introduction 4.2 Components of a Mathematical Model 4.3 Types of Mathematical Models 4.4 Growth Models 4.5 Parameters in Growth Models 4.6 Fitting Models to Data", " Chapter 4 Week 3: Mathematical Models – Parameters, Functions, and Growth 4.1 Introduction Mathematical models are purposeful simplifications of real-world systems. They distill complex environmental processes—like population change, heat transfer, carbon cycling, or pollutant transport—into a small set of variables, parameters, and rules. By translating mechanisms into equations or algorithms, models help us forecast likely futures, analyze drivers and feedbacks, and make decisions when data are limited or stakes are high. A good model balances clarity and realism. It should be simple enough to understand and compute, yet detailed enough to capture the processes that matter for the question at hand. Importantly, every model is conditional: its outputs reflect its assumptions, inputs, and uncertainties. Rather than delivering “truth,” models provide structured what-ifs—evidence-based scenarios that inform conservation planning, risk assessment, and policy. In this chapter we will: Name the parts of a model (variables, parameters, functions, initial/boundary conditions) and explain what each does. Compare growth forms (linear, exponential, logistic, decay, thresholds) and match them to ecological stories. Fit models to data and interpret parameters (e.g., growth rate \\(r\\), carrying capacity \\(K\\)) with attention to error and uncertainty. Test end-cases and sensitivity to identify which assumptions/parameters drive outcomes. Connect to decisions, using scenarios to explore management options and communicate uncertainty clearly. By the end, you should be able to look at an environmental problem, choose an appropriate model form, justify its assumptions, estimate its key parameters, and use it responsibly to support real-world decisions. 4.1.1 What is a Mathematical Model? A mathematical model is a framework that encodes assumptions about how a system changes using symbols, functions, and relationships. Within a model, variables represent system states that change over time or space, such as population size \\(P(t)\\), pollutant concentration \\(C(x,t)\\), or temperature \\(T(t)\\). These variables are influenced by parameters, which are fixed values that shape system behavior—for example, the growth rate \\(r\\), the decay constant \\(\\lambda\\), or the carrying capacity \\(K\\). To anchor the model, we specify initial conditions that describe the system at the starting point (e.g., \\(P(0) = P_0\\)) and boundary conditions that define how the system interacts with its environment, such as pollutant inputs at the upstream end of a river. The dynamics of the system are then expressed through equations, whether algebraic, difference, or differential, that link variables and parameters together. Depending on the purpose and data available, models can take many forms: mechanistic or physically based models grounded in process understanding, statistically based models built from data patterns, probability-based models that capture randomness, or hybrids that combine multiple approaches. 4.1.2 Why Models Matter in Environmental Science Environmental systems are complex, operate across multiple scales, and are often studied with limited or imperfect data. Because of this, intuition alone is rarely enough to guide effective decision-making. Mathematical models provide a structured way to translate ecological processes, observations, and hypotheses into a form that can be tested, refined, and applied. They act as a bridge between raw data and actionable insight. Models help us to: Explain assumptions about processes. For example, a snowmelt model can explicitly represent how warming temperatures drive runoff timing, making clear what processes are assumed to matter most. Forecast future states under different scenarios. With population models, we can project whether an endangered species will recover or decline under various management strategies. Diagnose key drivers and feedbacks. Climate-carbon cycle models, for instance, allow us to separate the influence of emissions, land-use change, and natural sinks in shaping atmospheric CO₂ levels. Design &amp; Manage interventions. Harvest models can reveal what fishing quotas are sustainable, while restoration models can test whether habitat improvements are sufficient to support species recovery. Quantify uncertainty and communicate confidence. No prediction is perfect, but models allow us to show a range of outcomes—helping managers and policymakers make informed decisions even when data are limited. Decision Contexts Regulation: Will pollutant concentrations in a river exceed safe drinking-water thresholds under typical or extreme flow conditions? Conservation: What harvest rate or protected-area size will keep a fish population near its target biomass while still allowing human use? Climate adaptation: How might different greenhouse gas emissions scenarios affect the number of extreme-heat days in a city, and what does this imply for public health planning? Restoration: If wetlands are reconnected to a floodplain, how long will it take for bird populations to rebound, and what level of uncertainty surrounds that estimate? In short, models matter because they provide a laboratory for ideas—a place where scientists and decision-makers can explore “what if” questions, test assumptions, and evaluate possible futures before they unfold in the real world. 4.1.3 The Balance Between Simplification and Realism When building a mathematical model, one of the most important choices is how much detail to include. Every model is a simplification of reality, but the degree of simplification can make the difference between a model that is clear and useful, and one that is either too crude to be informative or too complicated to be tractable. Simplification benefits Simplified models strip away many of the messy details of real-world systems in order to focus on core dynamics. Because they involve fewer parameters, they are easier to build with limited data, faster to compute, and easier to explain to diverse audiences. For instance, an exponential growth equation can help illustrate why invasive species may spread rapidly, even if it does not capture every factor influencing their establishment. Realism benefits More realistic models incorporate mechanisms, thresholds, or feedbacks that can matter in specific contexts. They may include spatial variation, seasonality, or nonlinear processes that strongly affect system outcomes. Realistic models can therefore give more accurate predictions for particular systems, especially when management or policy depends on fine-scale details. Trade-offs Every choice involves a trade-off. A model that is too simple risks underfitting—failing to capture critical dynamics. A model that is too complex risks overfitting—matching noise in the data rather than true processes. More detailed models also require more data and can be more sensitive to parameter uncertainty. Striking the right balance means aligning the model with the purpose: is it meant for teaching a concept, exploring scenarios broadly, or providing site-specific policy advice? Example. Consider modeling the population of salmon in a river system. A simplified logistic growth model may capture the overall idea that the population grows quickly at first and then levels off at a carrying capacity due to limited habitat. This is useful for teaching and for exploring general harvest rules. But for management decisions about a particular salmon run, a more realistic model may be needed—one that accounts for age structure, seasonal migration, water temperature, and spawning habitat availability. The simple model is easier to communicate and requires fewer data, while the realistic model is harder to build but can give more reliable forecasts for that river system. Common Pitfalls Treating model outputs as absolute facts instead of scenario-conditioned projections. Adding unnecessary complexity that the data cannot support. Ignoring uncertainty in parameters, model structure, or future scenarios, which can lead to misplaced confidence in results. In practice, good modelers acknowledge these trade-offs openly and design models that are “as simple as possible, but not simpler” for the problem at hand. 4.1.4 Example: Population Growth Population growth is one of the most fundamental processes in ecology, and models of growth have been central to understanding species invasions, conservation, and sustainable harvest management. Two of the most widely used forms are the exponential and logistic models. Exponential model \\[ P(t) = P_0 e^{rt} \\] In this model, the rate of change is proportional to the current population size. This leads to accelerating growth: the larger the population gets, the faster it increases. Ecological interpretation: Exponential growth captures the early stages of colonization or invasion, when resources are abundant and limiting factors such as predation or competition are minimal. Parameter: The intrinsic growth rate \\(r\\) determines how rapidly the population increases. The doubling time is given by \\[ t_d = \\frac{\\ln 2}{r}, \\] meaning populations with larger \\(r\\) double in size much faster. Logistic model \\[ P(t) = \\frac{K}{1 + A e^{-rt}}, \\quad A=\\frac{K-P_0}{P_0}. \\] The logistic model modifies exponential growth by adding a carrying capacity \\(K\\), representing the maximum sustainable population size given environmental limits such as food, space, or habitat quality. Initial condition: \\(P(0) = P_0\\). Dynamics: Growth is initially fast (similar to exponential growth), but slows as resources become limited. Eventually, the population levels off near \\(K\\), producing an S-shaped (sigmoid) curve. Insights Population dynamics are sensitive to small changes in parameters. For example, a slightly higher \\(r\\) may cause a species to reach \\(K\\) much faster, while changes in \\(K\\) shift the long-term ceiling. The logistic form highlights the role of feedbacks: the growth rate effectively decreases as population size increases. These models are directly applicable to conservation and harvest decisions, helping managers set quotas, predict recovery times, or evaluate the risk of extinction. Illustrative example. Imagine an invasive mussel species introduced into a new lake. At first, the population grows nearly exponentially—food is plentiful and there are no natural predators—so the number of mussels doubles every few months. However, as the population expands, food resources become depleted and available habitat fills. At this point, exponential growth overestimates reality, and the logistic model provides a better description: growth slows and the population stabilizes around a carrying capacity determined by the lake’s productivity. Conversely, in conservation contexts, a logistic model can help predict recovery. Suppose a fish population in a marine reserve is protected from harvest. The logistic model allows managers to estimate how long it will take the population to rebuild to near carrying capacity, and to test different scenarios (e.g., reopening harvest at different times). These insights show how relatively simple growth models can guide real-world environmental decisions. 4.1.5 The Modeling Cycle Define the scientific or decision question. Identify variables, processes, and scales. Choose model type and specify equations, ICs, and BCs. Estimate parameters from literature or data. Calibrate to part of the data and validate with another. Analyze sensitivity and uncertainty. Run scenarios to inform decisions. Iterate as new data or insights emerge. Models are rarely ‘completed’. The modeling development cycle results in updates to the model each iteration. Trying to produce a perfect model the first time through the cycle is not the goal. We learn with every development cycle, so we teh goal is always to get to a testable workable model, run it, test it, rethink it, improve it and repeat. 4.1.6 Choosing the Right Level of Detail A central challenge in modeling is deciding how much detail is enough. Every model is an abstraction of reality, but the level of abstraction should be tailored to the problem being addressed. Too simple, and the model may miss key dynamics; too complex, and it may become impossible to calibrate, explain, or use in decision-making. Match complexity to purpose The right level of detail depends on what the model is meant to achieve. For teaching or communication, simple models are often best. An exponential or logistic growth model can clearly illustrate fundamental ideas like compounding growth or resource limits without overwhelming the learner. For policy or regulation, more detail is often necessary. A government agency setting water-quality standards might need a physically based pollutant transport model that accounts for river flow, sediment interactions, and decay rates. Match complexity to data availability A model is only as good as the data available to support it. Overly detailed models with many parameters may look realistic but are prone to overparameterization, fitting noise instead of signal, if the necessary data are sparse or uncertain. For example, if only basic annual counts of a fish population are available, a logistic growth model may be the best fit. Trying to build an age-structured model with dozens of parameters would not be justified without high-resolution demographic data. Match complexity to computation needs The practical use of a model also depends on how much time and computational power is available. When running thousands of simulations for scenario testing or sensitivity analysis, simpler models (e.g., reduced-form climate models) may be preferable. More complex models (e.g., coupled climate–carbon cycle simulations) may provide greater realism but are too slow to be run repeatedly in real time. Example. Consider modeling coastal flooding risk. A simple model might assume a uniform sea-level rise plus storm surge probability to give a quick estimate of risk for city planning workshops. A more detailed model might incorporate high-resolution topography, tide–surge interactions, and climate projections for use in designing levees or evacuation plans. Both models serve important roles, but their usefulness depends on context—what decisions are being made, what data exist, and how quickly results are needed. In practice, good modeling involves finding the “sweet spot” where the model is complex enough to capture the essential dynamics but simple enough to be applied, tested, and communicated effectively. 4.1.7 Summary Mathematical models are simplified yet powerful tools that help us connect environmental processes to mathematical structures. They provide a way to translate complexity into variables, parameters, and equations that can be analyzed, tested, and applied to real-world decisions. Good models balance simplicity—which aids clarity, computation, and communication—with realism, which improves accuracy and relevance for specific contexts. Models matter because they allow us to explain processes, forecast future states, diagnose key drivers, design management strategies, and quantify uncertainty. They are not absolute truths but structured “what-if” experiments that reveal how assumptions and inputs shape outcomes. Examples such as population growth show how different model structures tell different ecological stories: exponential growth captures early invasions, while logistic growth highlights resource limits and feedbacks. Other examples, like climate–carbon models or pollutant transport equations, illustrate how models can range from very simple to highly detailed, depending on the purpose, data availability, and computational needs. Ultimately, the art of modeling lies in choosing the right level of detail—complex enough to capture the dynamics that matter, but simple enough to remain tractable and interpretable. When used thoughtfully, models serve as both conceptual laboratories for testing ideas and practical tools for guiding conservation, regulation, and climate adaptation decisions. 4.2 Components of a Mathematical Model Every mathematical model is built from a common set of ingredients. These components—variables, parameters, functions, initial conditions, and boundary conditions—define what the model represents, how it behaves, and how we interpret its results. Understanding these pieces is essential before moving on to growth models or more advanced formulations. 4.2.1 Variables Variables are the quantities in a model that can change. They capture the dynamic aspects of a system. Independent variables are inputs that we control or that act as the “clock” or “map” of the system. Time is the most common independent variable, but space (location along a river, depth in a lake, elevation on a mountain) often serves the same role. Dependent variables are the outputs that depend on the independent variables. For instance, population size depends on time, dissolved oxygen concentration depends on depth, and temperature depends on both latitude and elevation. In notation, we usually write dependent variables as functions of independent ones: \\[ P(t) \\quad \\text{population size over time}, \\qquad C(x,t) \\quad \\text{pollutant concentration in space and time}. \\] 4.2.2 Parameters Parameters are fixed values that influence how the system behaves. Unlike variables, they are usually assumed to remain constant during the simulation of a model, but they strongly shape its outcomes. Growth rate (\\(r\\)) determines how fast a population or process increases. Carrying capacity (\\(K\\)) sets the maximum sustainable population size in a logistic model. Decay constant (\\(\\lambda\\)) defines the speed of exponential decay, such as how quickly pollutants break down or isotopes lose radioactivity. Parameters are often unknown and must be estimated from data, experiments, or literature. Changing parameter values can drastically alter model behavior, which makes sensitivity analysis a crucial step in modeling. Example. Radiocarbon dating relies on the fact that carbon-14 decays at a fixed rate. The decay constant \\(\\lambda\\) gives the half-life of the isotope and allows us to estimate the age of archaeological samples. Do parameters always stay constant? Not necessarily. While many models assume fixed parameters for simplicity, parameters can themselves vary through time or depend on environmental conditions: A fish population’s growth rate (\\(r\\)) might drop during drought years when food is scarce. The carrying capacity (\\(K\\)) of a wetland may shift seasonally as water levels rise and fall. The decay constant (\\(\\lambda\\)) for a pollutant could change with temperature or pH. When parameters vary during a simulation, the model becomes more realistic but also more complex. These cases often require functions (e.g., \\(K(t)\\), \\(r(T)\\)) instead of constants, which better reflect dynamic environmental conditions. This introduces an important modeling choice: keep parameters constant for tractability, or allow them to vary to capture richer system behavior. 4.2.3 Functions Functions define the mathematical relationship between inputs (independent variables) and outputs (dependent variables). They are the backbone of a model, expressing how change happens. Deterministic functions produce the same output every time for a given input (e.g., a logistic growth equation). Stochastic functions include randomness, producing different outputs each time even with the same inputs (e.g., rainfall arrival modeled as a random process). Linear functions imply proportionality, such as a young tree’s height increasing linearly with age. Nonlinear functions capture thresholds, feedbacks, or saturation effects, such as logistic population growth leveling off at a carrying capacity. Example. A tree might grow roughly linearly in height during its early years but eventually slow as it approaches a biological maximum, a shift that requires a nonlinear model to represent correctly. 4.2.4 Initial Conditions Initial conditions specify the starting state of the system. Without them, many equations have infinitely many possible solutions, so they are essential for uniqueness. Purpose. They “anchor” the model at time zero. Examples. \\(P(0)=100\\): a salmon population of 100 individuals at the beginning of the study. \\(C(x,0)=0\\): no pollutant present in the river at the start. Changing the initial condition can dramatically alter system outcomes, especially in nonlinear systems. For example, a small population may fail to establish due to demographic stochasticity, while a slightly larger one might grow to carrying capacity. 4.2.5 Boundary Conditions Boundary conditions describe what happens at the edges of the system in space or time. They are critical when we model processes that unfold in rivers, landscapes, or the atmosphere, because what happens at the boundaries influences the interior. Dirichlet conditions fix the value at a boundary (e.g., the pollutant concentration at the upstream end of a river is set by an industrial discharge). Neumann conditions fix the rate of change or flux at a boundary (e.g., no heat flow across an insulated wall). Mixed conditions combine aspects of both (e.g., pollutant concentration at a boundary depends partly on inflow and partly on exchange with the environment). Example. In modeling stream pollution, we might set a Dirichlet condition at the upstream boundary to represent the known concentration coming in from a factory, and a Neumann condition at the downstream end to represent no net flux leaving the system beyond the reach of interest. Boundary and initial conditions together ensure that the mathematical problem is well-posed, meaning that the model has a unique and physically meaningful solution. 4.3 Types of Mathematical Models Mathematical models can be grouped in different ways depending on how they are constructed, what assumptions they make, and what role they serve. In environmental science, it is especially useful to distinguish between conceptual, empirical, mechanistic, physically based, statistically based, probability-based, and discrete/continuous models. Each of these types has strengths, limitations, and contexts where they are most useful. 4.3.1 Conceptual Models Conceptual models are qualitative representations of systems. Rather than focusing on detailed equations or precise numerical predictions, they capture the structure of a system—its main components and how they are connected. Format. Conceptual models often take the form of flow diagrams, influence charts, or causal loop diagrams. These visual tools show arrows linking drivers, processes, and outcomes, emphasizing relationships rather than calculations. Purpose. The goal is to build a shared mental map of the system, clarify assumptions, and decide which processes are most important to represent. By stripping away complexity, conceptual models help teams focus on the “big picture.” Strengths. They are simple, intuitive, and accessible to diverse audiences, making them powerful tools for communication, brainstorming, and scoping before moving into quantitative modeling. Limitations. Because they lack equations and numbers, conceptual models cannot generate predictions or quantify uncertainty. They are best viewed as a starting point, not an end product. Example. A watershed conceptual model might show precipitation leading to runoff, infiltration, groundwater recharge, and river flow, without specifying the mathematical relationships. Such a diagram makes clear that rainfall drives multiple pathways of water movement, and that groundwater recharge links surface and subsurface systems. This helps scientists, managers, and stakeholders agree on what processes matter before investing in a quantitative hydrological model. Broader use. Conceptual models are widely applied in ecology and environmental science. For example: In conservation planning, a conceptual model might link habitat loss, predator pressure, and food availability to species survival. In climate science, diagrams often depict the carbon cycle, showing how carbon moves among the atmosphere, biosphere, and oceans. In restoration ecology, managers might sketch how reconnecting wetlands to floodplains influences nutrient cycling, fish migration, and water quality. In all of these cases, the conceptual model serves as a foundation: it identifies key components, reveals possible feedbacks, and ensures that stakeholders have a common understanding before quantitative equations are developed. 4.3.2 Empirical Models Empirical models are constructed directly from observed data and aim to capture patterns without necessarily explaining the underlying processes. They are often built using regression, curve-fitting, or other statistical approaches. Strengths. Empirical models are quick to build when data are plentiful and can be highly accurate within the range of observed conditions. They are particularly useful for prediction tasks where explanation is less important. Limitations. Because they are not tied to underlying mechanisms, empirical models may fail when applied outside the range of the data (poor extrapolation). They also provide little insight into why a system behaves the way it does. Example. A regression model might link crop yield to rainfall amount, showing that higher rainfall generally leads to higher yields up to a point. While the model captures the correlation, it does not explain the underlying processes of soil moisture dynamics, plant physiology, or nutrient cycling. Empirical models are often a first step in modeling, helping reveal relationships that can later inspire more mechanistic or physically based models. 4.3.3 Mechanistic Models Mechanistic (or process-based) models attempt to represent the actual physical, chemical, or biological mechanisms driving a system. They are usually expressed in terms of equations that represent process rates or interactions. Strengths. Mechanistic models provide deeper explanatory power by focusing on why a system behaves in a particular way. Because they are based on processes, they can often be applied to new situations outside the range of existing data. Limitations. These models often require more assumptions, more data, and more expertise to build and calibrate. Example. A predator–prey model (Lotka–Volterra equations) represents how predator consumption rates depend on prey abundance, and how prey populations decline or increase depending on predator pressure. Such models capture the interaction mechanisms rather than just observed correlations. Mechanistic models are particularly valuable when the goal is understanding causation, not just prediction. 4.3.4 Physically Based Models Physically based models are a subset of mechanistic models that rely explicitly on fundamental physical laws, such as conservation of mass, energy, and momentum. Strengths. Because they are grounded in universal principles, they are often considered more “trustworthy” and generalizable. They can be applied to different systems without retraining or refitting, as long as the physics remain the same. Limitations. They are often data-intensive and computationally demanding, which can make them challenging to apply in data-limited or resource-constrained contexts. Example. The advection–dispersion equation in hydrology models pollutant transport in groundwater. It explicitly represents the physical processes of advection (movement with flow), dispersion (spreading), and decay. Physically based models are essential in hydrology, climate science, and atmospheric modeling, where conservation principles govern large-scale dynamics. 4.3.5 Statistically Based Models Statistically based models focus on patterns and correlations in observed data rather than explicit mechanisms. They can include regression, generalized linear models, and modern statistical learning methods. Strengths. They are flexible, relatively quick to build, and effective at handling noisy or messy datasets. They are often useful when mechanisms are poorly understood but data are available. Limitations. These models can lack interpretability, may be limited to the conditions where data were collected, and can be prone to spurious correlations. Example. A multiple regression predicting air quality index (AQI) from temperature, wind speed, and traffic counts. This model may predict AQI well in urban areas with similar conditions, but may not generalize to other contexts or explain the underlying atmospheric chemistry. 4.3.6 Hybrid Models Hybrid models combine mechanistic or physically based components with statistical or empirical elements. Increasingly, they also incorporate machine learning to approximate processes that are too complex to model explicitly. Strengths. They balance explanatory depth with predictive accuracy, making them powerful tools for modern environmental modeling. They can use physics where mechanisms are well understood and statistics where processes are uncertain. Limitations. Hybrids can be complex to implement, requiring both strong process knowledge and advanced statistical or computational skills. Example. A climate model might use physical laws to simulate radiation balance while representing cloud formation statistically, since clouds are too complex to simulate explicitly at global scales. Hybrid approaches are especially useful for large, complex systems where no single modeling approach is sufficient. 4.3.7 Probability-Based Models Probability-based models incorporate randomness and uncertainty explicitly into the modeling process. Types. Stochastic models: allow variables to fluctuate randomly (e.g., rainfall arrivals modeled as a Poisson process). Monte Carlo simulations: use repeated random sampling to explore uncertainty in parameters or inputs. Markov models: describe transitions between discrete states with probabilities (e.g., land cover change from forest to agriculture). Strengths. These models are well-suited for risk analysis and decision-making under uncertainty, since they provide probabilities rather than deterministic outcomes. Limitations. They require careful specification of probability distributions, and results can be sensitive to those assumptions. Example. Wildfire spread models often use probabilistic elements to capture random ignition events, wind shifts, and fuel variability. While no one can predict the exact path of a fire, probability-based models estimate likely outcomes and risks. 4.3.8 Discrete vs Continuous Models The choice between discrete and continuous models depends on how processes unfold and how data are collected. Discrete models describe systems in stepwise updates, often expressed as difference equations. Example. Annual updates of a fish population with seasonal reproduction, where population counts are only recorded once a year. Continuous models describe change as a smooth process in time or space, often expressed as differential equations. Example. Logistic growth of a population or chemical reaction kinetics, where changes occur continuously over time. Comparison. Discrete models are natural when processes occur in pulses (e.g., seasonal breeding) or when data are only periodic. Continuous models are appropriate when change is ongoing and smooth (e.g., temperature variation through the day). In practice, modelers often switch between discrete and continuous formulations depending on data availability, computational needs, and the nature of the process being studied. 4.3.9 Summary These categories—conceptual, empirical, mechanistic, physically based, statistically based, hybrid, probability-based, discrete, and continuous—are not mutually exclusive. Many real-world models blend features from several categories. For example, a water quality model might use physically based equations for pollutant transport (continuous), include a statistical relationship for uncertain decay rates, and apply Monte Carlo simulations to capture uncertainty. The art of modeling lies in choosing the right combination of approaches that best fits the scientific or management question at hand. 4.4 Growth Models Growth models describe how quantities change over time. They are among the most widely used mathematical models in environmental science because they capture how populations expand, resources are consumed, or pollutants accumulate and decline. Each model makes assumptions about what drives growth, what limits it, and whether change is smooth or sudden. Below we introduce several canonical growth forms. 4.4.1 Linear Growth The simplest growth model is linear growth, where change occurs at a constant rate: \\[ y(t) = y_0 + rt \\] Here, \\(y_0\\) is the initial value and \\(r\\) is the constant rate of change. The relationship is straightforward: every unit of time adds the same amount. Environmental example. If a forest loses 1,000 hectares per year due to logging, the total deforested area increases linearly with time. Strengths and limitations. Linear growth is intuitive and easy to apply, but it rarely captures long-term dynamics because most systems eventually accelerate, decelerate, or fluctuate rather than changing at a constant rate. 4.4.2 Exponential Growth Exponential growth occurs when the rate of change is proportional to the current size: \\[ y(t) = y_0 e^{rt} \\] \\(r\\) is the intrinsic growth rate, and \\(y_0\\) is the initial value. Growth accelerates as the quantity increases, leading to the famous “hockey-stick” curve. Environmental example. An invasive species introduced to a new habitat may grow exponentially at first, since resources are abundant and predators are absent. Key property. Exponential growth has a doubling time given by: \\[ t_d = \\frac{\\ln 2}{r} \\] Strengths and limitations. Exponential models are powerful for short-term predictions but unrealistic over long periods, since no population or pollutant can grow without bound. 4.4.3 Logistic Growth To incorporate resource limits, the logistic growth model modifies exponential growth by adding a ceiling: \\[ y(t) = \\frac{K}{1 + A e^{-rt}}, \\quad A = \\frac{K - y_0}{y_0} \\] \\(K\\) is the carrying capacity, the maximum sustainable level. \\(r\\) is the intrinsic growth rate. \\(y_0\\) sets the initial condition. Environmental example. A fish population in a lake may grow quickly when small but will slow as competition for food intensifies, eventually stabilizing around a carrying capacity. Key features. Growth is fast at first, then slows as \\(y(t)\\) approaches \\(K\\). Produces an S-shaped (sigmoid) curve. Sensitive to both the growth rate \\(r\\) and the initial population \\(y_0\\). Strengths and limitations. Logistic models capture the idea of limits and feedbacks, making them more realistic than exponential growth. However, real systems may not have fixed carrying capacities—they can change seasonally or due to human interventions. 4.4.4 Decay Models Not all processes involve growth. Many environmental problems involve decay, where a quantity decreases over time, often following an exponential law: \\[ y(t) = y_0 e^{-\\lambda t} \\] \\(\\lambda\\) is the decay constant. \\(y_0\\) is the initial value at \\(t=0\\). Environmental examples. Pollutant concentrations decline after a cleanup effort. Radioactive isotopes lose mass at a fixed half-life. Key property. The half-life is given by: \\[ t_{1/2} = \\frac{\\ln 2}{\\lambda} \\] Strengths and limitations. Decay models are excellent first approximations but may oversimplify when multiple processes (e.g., mixing, resuspension, or nonlinear reactions) occur simultaneously. 4.4.5 Piecewise / Threshold Models Some systems behave differently depending on conditions. Piecewise or threshold models capture these shifts by switching between rules depending on whether a threshold is crossed. Example function: \\[ f(x) = \\begin{cases} 0 &amp; \\text{if } T \\leq 0^\\circ C \\\\ r(T) &amp; \\text{if } T &gt; 0^\\circ C \\end{cases} \\] Environmental example. Snowpack remains stable when air temperatures are below freezing but begins to melt rapidly once temperatures exceed 0 °C. Similarly, algal blooms may only occur when nutrient concentrations exceed a critical threshold. Strengths and limitations. Threshold models are powerful for representing abrupt changes, but they can be difficult to calibrate because identifying the exact threshold requires detailed data. 4.4.6 Putting It All Together Each of these growth models represents a different story of change: Linear growth assumes constant addition. Exponential growth assumes self-reinforcing acceleration. Logistic growth assumes feedback and limits. Decay assumes continual decline. Threshold models assume sudden shifts once a condition is met. In environmental modeling, these forms are often combined or extended. For instance, population models might include logistic growth with seasonal thresholds, while pollutant decay models might combine exponential decline with inflow terms. Choosing the right growth model is less about mathematical elegance and more about capturing the essential dynamics of the system under study. 4.5 Parameters in Growth Models Growth models are defined not just by their structure (linear, exponential, logistic, etc.) but also by their parameters. Parameters are the constants that govern how the system behaves. In growth models, the most common parameters include the intrinsic growth rate (\\(r\\)), the carrying capacity (\\(K\\)), and the decay constant (\\(\\lambda\\)). By adjusting these parameters, the same mathematical structure can represent very different ecological or environmental realities. 4.5.1 The Role of Parameters Growth rate (\\(r\\)) determines how quickly a population expands or a pollutant concentration changes. A higher \\(r\\) means faster growth or decline. Carrying capacity (\\(K\\)) represents the maximum sustainable size of a population given resource constraints. It can change with environmental conditions such as food availability, habitat size, or climate. Decay constant (\\(\\lambda\\)) dictates how quickly a substance or population decreases, often through chemical breakdown, radioactive decay, or mortality. Without parameters, models would be abstract formulas with no connection to the real world. Parameter values bring models to life by grounding them in observed systems. 4.5.2 Sensitivity Analysis A key step in working with models is asking: how sensitive is the outcome to the values of the parameters? If a small change in \\(r\\) leads to a large change in population size over time, then accurate estimation of \\(r\\) is crucial. If the outcome hardly changes when \\(K\\) is varied, then the model may be robust to uncertainty in that parameter. Sensitivity analysis helps us: 1. Identify which parameters matter most. 2. Focus data collection on the most influential parameters. 3. Understand the uncertainty in model predictions. Example. A fish population with a growth rate \\(r=0.2\\) per year reaches its carrying capacity in about 20 years. If \\(r\\) is actually 0.25, the same population may reach capacity in only 15 years. Such differences have direct management implications for harvest rules. 4.5.3 Visualization with Parameter Sweeps One effective way to explore parameter influence is through parameter sweeps—systematically varying one parameter while holding others constant and plotting the results. Plotting logistic growth curves for several values of \\(r\\) shows how faster growth accelerates the time to reach carrying capacity. Plotting curves for different \\(K\\) values illustrates how the ceiling of growth changes. Parameter sweeps reveal families of curves and make the sensitivity of the system visually clear. Environmental example. Suppose two wetlands support frog populations with the same growth rate (\\(r\\)) but different carrying capacities (\\(K=500\\) vs. \\(K=1000\\)). A visualization would show both populations growing rapidly at first, but one leveling off sooner due to lower resource limits. 4.5.4 Comparing Competing Species Parameters also allow us to compare species or systems in meaningful ways. Example: Competing plants. Two plant species colonize a disturbed site. Species A has a higher growth rate (\\(r\\)) but a lower carrying capacity (\\(K\\)) due to limited resource efficiency. Species B has a slower growth rate but a higher carrying capacity. Which species dominates depends on the timescale of interest: in the short term, Species A may dominate, but in the long term, Species B may outlast it. Example: Invasive vs native fish. An invasive fish species may have a larger \\(r\\), allowing it to spread quickly, while a native species may be closer to carrying capacity. Understanding both parameters can guide whether intervention is urgent. 4.5.5 Summary Parameters are the levers of growth models. By adjusting \\(r\\), \\(K\\), or \\(\\lambda\\), the same mathematical structure can represent radically different ecological stories. Sensitivity analysis and parameter sweeps help identify which parameters drive system behavior, while comparative studies show how parameter differences can explain species competition and coexistence. In practice, estimating parameters from data is one of the most challenging but most important aspects of environmental modeling. 4.6 Fitting Models to Data So far we have introduced growth models as equations with parameters like \\(r\\), \\(K\\), or \\(\\lambda\\). But how do we actually determine the values of these parameters for a real-world system? This is where fitting models to data comes in. By comparing model predictions with observational data, we can estimate parameter values, assess how well the model represents reality, and quantify uncertainty. 4.6.1 From Observations to Parameters Environmental systems are usually monitored through field measurements, laboratory experiments, or remote sensing. These observations give us time series or spatial data that can be compared with model outputs. Example. Suppose we have a dataset of fish population counts from annual surveys of a lake. We can use these observations to estimate the growth rate \\(r\\) and carrying capacity \\(K\\) in a logistic model. Our first decision is which model suits this situation. Once we decide on a model form, we then need to determine the best parameter set. The goal is to select the right model and adjust model parameters so that the model trajectory matches the observed data as closely as possible. 4.6.2 Methods of Parameter Estimation Several approaches exist for estimating parameters from data. The choice depends on the model type, the data available, and the assumptions we are willing to make. Regression methods. Linear regression is used when the model can be written in a straight-line form. Nonlinear regression extends this to curves like logistic or exponential models. Least squares estimation. One of the most common approaches is to minimize the sum of squared errors (SSE) between observed values and model predictions: \\[ SSE = \\sum_{i=1}^n \\big(y_{\\text{obs},i} - y_{\\text{model},i}\\big)^2 \\] The parameter values that minimize the SSE are considered the best fit. Maximum likelihood estimation (MLE). A more general method that finds parameters most likely to produce the observed data under a given probability distribution. Often used when data are noisy or when errors do not follow a simple normal distribution. 4.6.3 Uncertainty and Error No parameter estimate is exact. Uncertainty arises from measurement error, sampling variability, and model assumptions. Good modeling practice requires quantifying and communicating this uncertainty. Confidence intervals around parameter estimates show the plausible range of values. Residual analysis (examining the differences between observed and predicted values) helps detect patterns that indicate model misfit. Cross-validation (fitting the model to part of the data and testing it on the rest) helps assess predictive power. In some cases, uncertainty in parameters can be propagated through the model using techniques such as Monte Carlo simulation, producing a range of possible outcomes rather than a single prediction. 4.6.4 Summary Fitting models to data is the bridge between theory and reality. Observations allow us to estimate parameters, while regression and least squares methods provide systematic ways to find the best fit. A logistic fit to population data is a classic example, but the same principles apply to pollutant decay, climate trends, or forest growth. Always remember that parameter estimates come with uncertainty, and acknowledging error is just as important as reporting the “best” values. "],["probabilistic-modeling-embracing-uncertainty-in-environmental-systems.html", "Chapter 5 Probabilistic Modeling: Embracing Uncertainty in Environmental Systems 5.1 Introduction: Why Probabilities Matter 5.2 Sources of Uncertainty 5.3 Primer: Probability, PDFs, and CDFs 5.4 Visualizing Probability Distributions 5.5 Representing Uncertainty with Probability Distributions 5.6 Deterministic vs Stochastic Models 5.7 Monte Carlo Simulation 5.8 Markov and Transition Models 5.9 Communicating Probabilistic Results 5.10 Summary and Reflection", " Chapter 5 Probabilistic Modeling: Embracing Uncertainty in Environmental Systems Environmental systems are complex and variable. No two days of rainfall, no two migration seasons, and no two pollution events are exactly alike. Traditional deterministic models give a single outcome for a given set of inputs, but real-world systems are rarely that predictable. This chapter introduces probabilistic modeling—an approach that explicitly represents uncertainty, randomness, and variability. You’ll learn how to use probability distributions, random sampling, and simulation to describe and analyze environmental systems where outcomes are not fixed but occur within a range of possibilities. 5.1 Introduction: Why Probabilities Matter Deterministic models are powerful but limited—they produce only one trajectory of change. In environmental systems, variability is the rule, not the exception. Probabilistic models help us ask questions in terms of likelihoods rather than certainties. Examples: What is the probability that river flow will exceed flood stage next year? How likely is it that a pollutant will stay above a safe concentration threshold? What are the chances a species population will persist for 50 years? By framing predictions probabilistically, we not only acknowledge uncertainty but also make models more informative and realistic for decision-making. 5.2 Sources of Uncertainty Uncertainty enters environmental models from several directions. Recognizing its sources helps us model it appropriately. Aleatory uncertainty refers to natural randomness—unpredictable fluctuations inherent to the system. Weather, genetic variation, or random seed dispersal are examples. Epistemic uncertainty comes from lack of knowledge. We might not know the true growth rate of a species or the exact decay constant of a pollutant. Model structural uncertainty arises from simplifications and missing processes. Measurement error comes from noisy or biased observations. In practice, all four interact. A rainfall–runoff model, for example, has aleatory uncertainty in rainfall, epistemic uncertainty in soil properties, and structural uncertainty in how infiltration is represented. 5.3 Primer: Probability, PDFs, and CDFs Before we build probabilistic models, we need to understand the language of probability. Probability gives us a way to quantify uncertainty—the likelihood of different outcomes. 5.3.1 Probability Basics A random variable represents a quantity that can take on different values, each with some probability. For discrete variables, we use \\(P(X = x)\\). For continuous variables, we consider ranges: \\(P(a &lt; X &lt; b)\\). Example: Let \\(R\\) be daily rainfall. \\(R\\) can take any non-negative value, and we might ask, “What is the probability that rainfall tomorrow exceeds 10 mm?” 5.3.2 Probability Density Function (PDF) A probability density function (PDF) describes how probability is distributed over the range of possible values. \\[ P(a &lt; X &lt; b) = \\int_a^b f(x)\\,dx = 1 \\] The total area under the curve equals 1. The height of \\(f(x)\\) at any point shows relative likelihood, not actual probability. Environmental interpretation: Rainfall PDFs are often skewed right—many dry days, fewer wet ones, and rare extreme storms. Activity: PDFs For each of the distributions below, interpret the pdfs with respect to the system variables they represent. Think about the features of the pdf curve and what they mean with respect to the variable. Activity: CDFs For each of the distributions below, interpret the cdfs with respect to the system variables they represent. Think about the features of the cdf curve and what they mean with respect to the variable. 5.3.3 Cumulative Distribution Function (CDF) The cumulative distribution function (CDF) gives the probability that a variable is less than or equal to a value \\(x\\): \\[ F(x) = P(X \\le x) = \\int_{-\\infty}^{x} f(t)\\,dt \\] CDFs rise from 0 to 1, showing cumulative probability. If \\(F(10) = 0.8\\), there’s an 80% chance rainfall will be below 10 mm on a given day. How to see this visually. The shaded area under the PDF from the minimum up to a threshold \\(x_0\\) is exactly the CDF value \\(F(x_0)\\). The plot below shows (left) a right-skewed rainfall PDF with area shaded up to \\(x_0\\), and (right) the corresponding CDF with the point \\((x_0, F(x_0))\\) highlighted. 5.3.4 Discrete vs Continuous Distributions Probability models can describe two broad types of random variables — discrete and continuous — depending on whether outcomes take on isolated values or form a continuum. 5.3.4.1 Discrete Distributions Discrete variables take on countable values — often whole numbers that represent distinct events or outcomes. Examples include the number of storms in a season, the number of trees in a plot, or the number of salmon successfully migrating past a dam. These are described by a probability mass function (PMF), which gives the probability of each possible outcome: \\[ P(X = x_i) \\] The sum of all probabilities across possible outcomes equals 1. For instance, if \\(X\\) is the number of successful seed germinations out of 20 seeds with a germination probability of 0.7, \\(X\\) follows a Binomial distribution. Similarly, if \\(X\\) represents the number of storms in a year, it may follow a Poisson distribution, which models the number of events that occur within a fixed interval given an average rate. Discrete distributions are often used for event counts, success/failure outcomes, and presence/absence data in environmental systems. 5.3.4.2 Continuous Distributions Continuous variables, on the other hand, can take on any value within a range — often representing measurements such as rainfall depth, temperature, river flow, or wind speed. These are described by a probability density function (PDF), which defines the relative likelihood of observing values near a given point. Because there are infinitely many possible values, the probability of observing any exact value is zero; instead, we compute probabilities over intervals: \\[ P(a &lt; X &lt; b) = \\int_a^b f(x)\\,dx \\] Common continuous distributions in environmental science include: The Normal distribution, used for temperature anomalies or measurement errors. The Exponential distribution, often used for modeling time between events like rainfall or wildfires. The Gamma distribution, commonly used for rainfall depth or pollutant concentration. Continuous models are particularly useful when representing natural variability and measurement uncertainty in environmental systems. 5.3.4.3 Summary Comparison Type Example Variable Representation Example Distribution Visualization Discrete Number of storms per year Probability Mass Function (PMF) Binomial, Poisson Bars for individual outcomes Continuous Rainfall depth, temperature Probability Density Function (PDF) Normal, Exponential, Gamma Smooth continuous curve In practice: Many environmental datasets blur this distinction. For example, rainfall measured to the nearest millimeter is technically discrete but is often modeled as continuous because the measurement resolution is fine enough to approximate a continuum. 5.3.5 Moments: Mean, Variance, and Shape Moments describe the key numerical characteristics of a probability distribution — they quantify its center, spread, asymmetry, and tail behavior. In environmental systems, moments help us summarize how variable, extreme, or asymmetric natural processes can be — for example, how storm intensity may increase even if the mean annual rainfall stays constant. Mean (\\(\\mu\\)) — the expected value or long-term average. It represents the “center of mass” of the distribution: \\[ \\mu = E[X] = \\int_{-\\infty}^{\\infty} x\\,f(x)\\,dx \\] Variance (\\(\\sigma^2\\)) — measures the spread or dispersion around the mean: \\[ \\sigma^2 = E[(X - \\mu)^2] \\] A high variance indicates that outcomes fluctuate widely (e.g., highly variable daily precipitation). Skewness — captures asymmetry. Right-skewed distributions (positive skew) have long tails toward large values, as seen in rainfall or pollutant concentrations. Kurtosis — measures the heaviness of the tails relative to a normal distribution. High kurtosis implies more frequent extreme events — like rare but severe floods. Moments collectively describe how variable and extreme environmental conditions can be, guiding modelers toward more realistic representations of uncertainty and risk. 5.3.5.1 Visualizing Moments on a PDF and CDF The annotated plots below show how these statistical moments appear on a probability distribution. We use a Gamma distribution as an example for daily rainfall (mm) — a right-skewed, non-negative continuous variable common in environmental modeling. This plot shows a Gamma probability density function (PDF) modeling daily rainfall. Key annotations: Mean, Median, and Mode — Indicate the central tendency measures; note the skewed shape where mean &gt; median &gt; mode. Central 90% Interval (blue shaded) — Encloses the middle 90% of all rainfall outcomes; only 5% fall below and 5% above this range. Exceedance Probability (red shaded) — The probability that rainfall exceeds a threshold \\(x_0\\), here approximately 0.155 (15.5%). The plot emphasizes how rainfall distributions are often positively skewed, with most days having modest rainfall and a few days of heavy rain. 5.4 Visualizing Probability Distributions Visuals make distributions intuitive: - PDFs/PMFs show which values are most likely (shape, skew, tails). - CDFs show the cumulative probability up to any value. Below are common distributions in environmental modeling. For each distribution, you’ll see the equation, parameters, use cases, and two separate plots — PDF/PMF first, CDF second. 5.4.1 Normal (Gaussian) The Normal distribution, also called the Gaussian distribution, is one of the most fundamental probability models in environmental science and statistics. It describes random variables that tend to cluster around a mean value, with smaller probabilities for values farther away from that mean. Many natural processes — such as temperature variation, measurement error, and small perturbations around equilibrium — often follow this bell-shaped pattern due to the Central Limit Theorem, which states that the sum of many small independent effects tends to be normally distributed. Probability Density Function (PDF) \\[ f(x\\mid \\mu,\\sigma)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] The function is symmetric around the mean \\(\\mu\\). The height of the curve at any \\(x\\) represents the relative likelihood of that value. The spread is controlled by the standard deviation \\(\\sigma\\): larger \\(\\sigma\\) values create flatter, wider curves; smaller \\(\\sigma\\) values create narrower peaks. Parameters: \\(\\mu\\): Mean — the central tendency or expected value of the distribution. \\(\\sigma&gt;0\\): Standard deviation — a measure of variability or spread around the mean. Key properties: About 68% of values fall within one standard deviation of the mean (\\(\\mu \\pm \\sigma\\)). About 95% within two (\\(\\mu \\pm 2\\sigma\\)). About 99.7% within three (\\(\\mu \\pm 3\\sigma\\)). The distribution is defined for all real numbers (\\(-\\infty &lt; x &lt; \\infty\\)). Environmental examples: Temperature anomalies: Daily or annual temperature deviations from a long-term mean are often approximately Normal, especially after removing seasonal cycles. Measurement errors: Instrument noise and random observational error frequently follow Normal patterns due to the aggregation of many small random effects. Biological growth: Traits like plant height or leaf size may approximate a Normal distribution in large, uniform populations. 5.4.2 Lognormal The Lognormal distribution models positive quantities whose logarithm is normally distributed. In other words, if \\(Y = \\ln(X)\\) follows a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\(X\\) follows a Lognormal distribution. This distribution is right-skewed, meaning it captures phenomena where small values are common but large, extreme values occasionally occur. Such asymmetry is typical of environmental data like rainfall, pollutant concentration, or river discharge — where values cannot go below zero, yet large events can occasionally dominate the total. Probability Density Function (PDF) \\[ f(x\\mid \\mu,\\sigma)=\\frac{1}{x\\,\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right),\\quad x&gt;0 \\] Parameters: \\(\\mu\\): Mean of the log-transformed variable (\\(\\ln X\\)), determining the central tendency in log-space. \\(\\sigma&gt;0\\): Standard deviation of \\(\\ln X\\), controlling the spread and skewness. Larger \\(\\sigma\\) values make the distribution more skewed and heavy-tailed. Key properties: Mean \\(E[X] = e^{\\mu+\\sigma^2/2}\\) Median \\(= e^{\\mu}\\) Mode \\(= e^{\\mu-\\sigma^2}\\) Defined only for \\(x&gt;0\\). Environmental examples: Rainfall amounts: Many small precipitation events occur, but occasional intense storms create a long right tail. River discharge: Streamflow over time shows frequent moderate flows and rare floods. Air pollutants (PM₂.₅, ozone): Concentrations are positive, right-skewed, and vary multiplicatively with meteorological conditions. 5.4.3 Gamma The Gamma distribution is a flexible model for positive, continuous data that are right-skewed. It’s often used to represent waiting times, rates, and environmental magnitudes like rainfall totals, pollutant concentrations, or groundwater recharge rates. The Gamma family includes a range of shapes: When the shape parameter \\(k\\) is small, the distribution is highly skewed (many small events, few large ones). As \\(k\\) increases, the curve becomes more symmetric and approaches a Normal shape. This flexibility makes the Gamma distribution one of the most common in environmental and hydrological modeling. Probability Density Function (PDF) \\[ f(x\\mid k,\\theta)=\\frac{1}{\\Gamma(k)\\,\\theta^k}\\,x^{k-1}e^{-x/\\theta},\\quad x&gt;0 \\] \\(k\\): Shape parameter — controls the skewness. \\(\\theta\\): Scale parameter — stretches or compresses the distribution horizontally. \\(\\Gamma(k)\\) is the gamma function, a continuous extension of the factorial. Key properties: Mean \\(E[X] = k\\theta\\) Variance \\(Var[X] = k\\theta^2\\) Mode (for \\(k&gt;1\\)) \\(= (k-1)\\theta\\) Skewness \\(= 2/\\sqrt{k}\\) (decreases as \\(k\\) increases). Environmental examples: Rainfall amounts: Daily or monthly precipitation totals often follow a Gamma distribution — many small rain events and occasional heavy storms. Pollutant concentrations: Skewed positive values due to occasional high-emission days. Hydrologic response times: Waiting time between flow events or recharge occurrences. 5.4.4 Exponential The Exponential distribution is one of the simplest continuous probability models. It describes the time between random, independent events occurring at a constant average rate — such as the time until the next rainfall, the duration between earthquakes, or the decay of a pollutant. Mathematically, it is a special case of the Gamma distribution where the shape parameter \\(k = 1\\). Despite its simplicity, the exponential model provides powerful insight into systems governed by memoryless processes, meaning the probability of an event occurring in the future is independent of how much time has already passed. Probability Density Function (PDF) \\[ f(x\\mid \\lambda)=\\lambda e^{-\\lambda x}, \\quad x \\ge 0 \\] Parameters: \\(\\lambda&gt;0\\): Rate parameter, representing the average frequency of events per unit time. Mean \\(E[X] = 1/\\lambda\\) Variance \\(Var[X] = 1/\\lambda^2\\) Key property: The Exponential distribution is memoryless: \\[ P(X &gt; s + t \\mid X &gt; s) = P(X &gt; t) \\] This makes it ideal for modeling systems with constant hazard rates, such as radioactive decay or Poisson event arrivals. Environmental examples: Time between rainfall events: Days between consecutive storms often follow an exponential pattern during a rainy season. Pollutant decay: Concentration of a dissolved chemical decreasing exponentially over time due to natural degradation or dilution. Streamflow recovery: Time between flow spikes after storm events. 5.4.5 Weibull The Weibull distribution is a versatile model that generalizes the Exponential distribution by introducing a shape parameter. It’s widely used in environmental and engineering applications to represent phenomena such as wind speeds, drought durations, extreme wave heights, or time-to-failure processes. When the shape parameter \\(k = 1\\), the Weibull reduces to the Exponential distribution, meaning events occur at a constant rate. When \\(k \\ne 1\\), the event rate changes over time — increasing for \\(k &gt; 1\\) (aging systems) or decreasing for \\(k &lt; 1\\) (infant mortality or early failures). Probability Density Function (PDF) \\[ f(x\\mid k,\\lambda)=\\frac{k}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\!\\left[-\\left(\\frac{x}{\\lambda}\\right)^k\\right], \\quad x\\ge 0 \\] Parameters: \\(k&gt;0\\): Shape parameter, controls the slope and skewness. \\(k &lt; 1\\): decreasing rate (events most likely early). \\(k = 1\\): constant rate (Exponential case). \\(k &gt; 1\\): increasing rate (events more likely later). \\(\\lambda&gt;0\\): Scale parameter, stretches the distribution horizontally; larger \\(\\lambda\\) means longer time scales or higher magnitudes. Key properties: Mean \\(E[X] = \\lambda\\,\\Gamma(1 + 1/k)\\) Variance \\(Var[X] = \\lambda^2 [\\Gamma(1 + 2/k) - \\Gamma(1 + 1/k)^2]\\) Mode (for \\(k &gt; 1\\)) \\(= \\lambda[(k-1)/k]^{1/k}\\) Environmental examples: Wind speeds: Weibull provides excellent fits to observed wind speed distributions — used in wind energy assessment. Drought durations: The shape parameter \\(k\\) captures the likelihood of prolonged dry spells. Wave heights or storm durations: Often right-skewed and bounded below by zero, well-suited to Weibull modeling. Failure time models: Used for lifespan analysis of sensors, equipment, or biological survival times. 5.4.6 Poisson (Discrete Counts) The Poisson distribution models the probability of a given number of discrete events occurring in a fixed interval of time or space, assuming events occur independently and at a constant average rate. It’s a cornerstone of environmental and ecological modeling — ideal for situations where we count occurrences, such as the number of storms per year, lightning strikes per month, or invasive species arrivals at a site. Probability Mass Function (PMF) \\[ P(X = x \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\quad x = 0,1,2,\\ldots \\] Parameters: \\(\\lambda&gt;0\\): Rate parameter — the average number of occurrences per interval. Mean \\(E[X] = \\lambda\\) Variance \\(Var[X] = \\lambda\\) Key properties: Models counts (integer values). Appropriate when events are rare and independent. The variance equals the mean — an important diagnostic property for checking fit. Environmental examples: Storm frequency: Number of rainstorms per month or year. Wildfire ignitions: Count of new fires detected in a region during summer. Earthquake counts: Number of earthquakes above a threshold magnitude per year. Pollution events: Number of exceedances over a safe limit within a week. 5.4.7 Gumbel (Extreme Value Type I): Emphasizing Fat Tails In extreme-value work we care about the upper tail: rare, very large events (e.g., record floods). Compared to a Normal distribution (whose tail probability decays roughly like \\(\\exp(-x^2)\\)), the Gumbel tail decays exponentially: \\[ \\bar F(x)=1-F(x)\\approx \\exp\\!\\left(-\\frac{x-\\mu}{\\beta}\\right)\\quad\\text{for large }x, \\] which is heavier (fatter) than the Normal’s “thin” tail. A fatter tail means higher probability of extremes than you’d expect if you (incorrectly) used a Normal model. Two visual cues help students see this: Tail shading (PDF &amp; CDF): Shade the region beyond high quantiles (e.g., 95th and 99th percentiles). Return level marker: Show the \\(T\\)-year return level \\(x_T\\) (e.g., \\(T=100\\) years), which satisfies \\(F(x_T)=1-1/T\\). On the CDF this is a horizontal line at \\(1-1/T\\) and a vertical line at \\(x_T\\). 5.4.8 Quick guidance on when to use which Normal: deviations around a mean; symmetric noise; aggregated processes. Lognormal: positive and right-skewed (products of factors) — flows, concentrations. Gamma: flexible right-skew for amounts like rainfall depth. Exponential: waiting times with memoryless property (simple storm arrivals). Weibull: wind speed, lifetimes; shape controls tail and peak. Poisson: counts of events per interval (storms, fires). Gumbel: block maxima (flood peaks, annual max rainfall) when GEV shape ≈ 0. 5.4.9 From Probability to Modeling In deterministic models, parameters are fixed values. In probabilistic models, those parameters are random variables drawn from PDFs that describe uncertainty. Running the same model many times with randomly drawn parameters produces a distribution of outcomes rather than a single number. The result is a richer understanding of what could happen—and with what likelihood. Reflection Why might it be more informative to say “There is a 25% chance that nitrate concentrations will exceed the safe limit” rather than “Nitrate concentrations will be 12 mg/L next month”? 5.5 Representing Uncertainty with Probability Distributions Probability distributions allow us to mathematically describe and model uncertainty. Common distributions in environmental modeling include: Normal distribution: temperature fluctuations around a mean. Exponential distribution: time between random rainfall events. Poisson distribution: number of storms per year. Binomial distribution: number of successful seed germinations out of \\(n\\) attempts. Uniform distribution: when all values within a range are equally likely. Selecting an appropriate distribution depends on the process being modeled. For instance, rainfall is often modeled with a Gamma or Weibull distribution to capture its right-skewed nature. 5.6 Deterministic vs Stochastic Models A deterministic model produces the same outcome every time for the same inputs. A stochastic model includes random variation—each run produces a slightly different result. Example: \\[ N_{t+1} = rN_t(1 - N_t/K) \\] is deterministic, while \\[ N_{t+1} = rN_t(1 - N_t/K) + \\varepsilon_t \\] is stochastic, where \\(\\varepsilon_t\\) represents random environmental noise. Stochasticity can be added to: Parameters: growth rate \\(r\\) varies yearly. Inputs: rainfall or temperature fluctuate randomly. Processes: survival or reproduction are probabilistic. Deterministic models show the mean trajectory; stochastic models show the variability around that trajectory. 5.7 Monte Carlo Simulation Monte Carlo simulation is the workhorse of probabilistic modeling — a powerful way to explore uncertainty when outcomes depend on random events. Its name comes from Monte Carlo, Monaco, home to one of the world’s most famous casinos. Just as dice rolls, card draws, and slot spins involve chance, Monte Carlo methods rely on repeated random sampling to understand what outcomes are likely — and how variable they are. 5.7.1 How it works Define probability distributions for uncertain inputs (like dice rolls, card values, or interest rates). Randomly sample values from those distributions. Run the model or “game” using those sampled values. Repeat the process many times — hundreds or thousands of rounds. Analyze the resulting spread of outcomes to see probabilities, risks, and likely ranges. 5.7.2 Example: Simulating Casino Winnings Imagine you’re playing a simple casino game — betting $10 on red in roulette. You have an 18 out of 38 chance (in American roulette) of winning $10, and a 20 out of 38 chance of losing $10. We can simulate this: Each spin = one random trial. The outcome is either +10 or −10. Repeat for, say, 1,000 spins and record your cumulative winnings. 5.7.3 Monte Carlo in action By running this simulation thousands of times: You can estimate your expected return (average winnings per spin). You can visualize the distribution of possible final balances after 1,000 spins. You’ll likely see a bell-shaped spread centered below $0 — meaning the casino wins in the long run. Each individual player’s outcome is uncertain, but across thousands of simulated players, patterns emerge. This is the power of Monte Carlo simulation: it turns randomness into predictable probabilities. 5.7.4 Takeaway Monte Carlo simulations aren’t limited to casinos — they’re used anywhere uncertainty matters: from finance (forecasting returns) to engineering (safety margins) to science (climate models). The key idea: Instead of solving uncertainty analytically, we simulate it repeatedly and let probability reveal the shape of possible futures. 5.8 Markov and Transition Models Many environmental systems move among a finite set of states over time (e.g., land cover: forest, agriculture, urban; ecosystem health: healthy, stressed, degraded). Markov models capture these dynamics when the next state depends only on the current state, not on the full history (the Markov property). 5.8.1 Core Pieces States: \\(S = \\{1, \\dots, K\\}\\) (e.g., Forest, Agriculture, Urban). Time: typically discrete steps (year, decade, season). Transition matrix \\(\\mathbf{P}\\) (row-stochastic): \\[ P_{ij} = \\Pr\\{X_{t+1} = j \\mid X_t = i\\}, \\quad \\text{where } \\sum_j P_{ij} = 1. \\] State distribution (row vector) at time \\(t\\): \\[ \\boldsymbol{\\pi}_t = [\\Pr(X_t=1), \\dots, \\Pr(X_t=K)]. \\] Forward projection: \\[ \\boldsymbol{\\pi}_{t+1} = \\boldsymbol{\\pi}_t \\mathbf{P}, \\quad \\boldsymbol{\\pi}_{t+h} = \\boldsymbol{\\pi}_t \\mathbf{P}^h \\] 5.8.2 Example Transition Matrix (Annual) From  To Forest Agriculture Urban Forest 0.85 0.10 0.05 Agriculture 0.05 0.85 0.10 Urban 0.00 0.10 0.90 Each row represents the current state, and each column represents the next state. The diagonal entries (like 0.85 for Forest → Forest) show persistence or stability. Multi-step behavior: - \\(\\mathbf{P}^2\\): two-year transitions. - \\(\\mathbf{P}^{10}\\): ten-year transitions. 5.8.3 From “What’s Next?” to “Where Does It Settle?” If the chain is irreducible (all states eventually communicate) and aperiodic (no fixed cycle, often ensured by diagonal entries &gt; 0), then there exists a unique stationary distribution \\(\\boldsymbol{\\pi}^*\\) such that: \\[ \\boldsymbol{\\pi}^* = \\boldsymbol{\\pi}^* \\mathbf{P}, \\quad \\sum_j \\pi^*_j = 1 \\] Interpretation: The stationary distribution represents the long-run proportion of time (or area) spent in each state, independent of where the system started. For the matrix above: \\[ \\boldsymbol{\\pi}^* \\approx [0.133, 0.400, 0.467] \\] So in the long run, the landscape tends toward: - 13.3% Forest - 40.0% Agriculture - 46.7% Urban Even if the system starts as 100% Forest, it will gradually move toward this equilibrium mix. 5.8.4 Reading the Matrix Like an Ecologist or Planner Stability / Persistence: High diagonal values (\\(P_{ii}\\)) indicate long-term stability. Urban is very persistent here (0.90). Flux Pathways: Off-diagonal entries reveal where transitions are most likely (Agriculture → Urban at 0.10). Policy Sensitivity: Adjusting a few key entries—like lowering Agriculture → Urban—can shift the entire long-term balance. 5.8.5 Special Structures Absorbing states: \\(P_{kk} = 1\\) (e.g., Urban, if no reforestation possible). Once entered, the system stays there. The fundamental matrix \\(\\mathbf{N} = (\\mathbf{I} - \\mathbf{Q})^{-1}\\) gives expected times to absorption. Metastable states: Very high self-transition probability, but not strictly absorbing (Urban with 0.90). Time-varying chains: \\(\\mathbf{P}_t\\) changes over time (e.g., policy or climate effects). Continuous-time chains: Transitions occur continuously, governed by a rate matrix \\(\\mathbf{Q}\\) where \\(\\mathbf{P}(t) = e^{t\\mathbf{Q}}\\). 5.8.6 Estimating Transition Matrices Classify each site’s state at two time steps (e.g., land cover maps). Count transitions \\(N_{ij}\\) from \\(i \\to j\\). Normalize by row: \\[ \\widehat{P}_{ij} = \\frac{N_{ij}}{\\sum_j N_{ij}}. \\] Quantify uncertainty: Rows follow a multinomial distribution. Use Dirichlet priors for Bayesian inference or bootstrap for intervals. Check assumptions: Does the Markov property hold (no dependence on earlier states)? Is the time step appropriate? Are there misclassification errors (use Hidden Markov Models if so)? 5.8.7 What Markov Models Answer Well Short-term forecasts: \\(\\boldsymbol{\\pi}_0 \\mathbf{P}^h\\) Long-run equilibrium: \\(\\boldsymbol{\\pi}^*\\) Sensitivity analysis: Which transitions most affect long-run outcomes Expected times: Probability or expected time to reach certain states (e.g., &gt;50% Urban) 5.8.8 Environmental Applications Land cover change: Forecast forest loss or urban expansion; test reforestation policies. Species occupancy: States = present/absent; transitions reflect colonization or extinction rates. Fire regimes: States = unburned, recently burned, recovering; estimate recurrence intervals. Climate regimes: El Niño, La Niña, Neutral; evaluate persistence and switching probabilities. Water quality: Good, Impaired, Hypoxic; estimate risks of degradation and recovery likelihood. 5.8.9 Common Pitfalls and Fixes Path dependence: Add “memory” by defining expanded states (Forest-young vs Forest-old). Nonstationarity: Use time-varying \\(\\mathbf{P}_t\\) matrices under different scenarios. Spatial heterogeneity: Fit region-specific matrices or spatially varying coefficients. Classification error: Employ Hidden Markov Models (HMMs) to account for uncertainty in observed states. 5.8.10 Mini Worked Example Starting with 100% Forest (\\(\\boldsymbol{\\pi}_0 = [1, 0, 0]\\)): Time Step Forest Agriculture Urban Year 0 1.000 0.000 0.000 Year 5 0.477 0.305 0.218 Year 10 0.276 0.378 0.347 Long Run 0.133 0.400 0.467 Interpretation: Even a fully forested landscape trends toward an urban–agricultural equilibrium under current transition rates. If we increase Forest persistence (e.g., \\(P_{\\text{Forest,Forest}}\\) from 0.85 → 0.92), the long-run forest fraction rises substantially—showing how policy changes shift long-term stability. 5.8.11 Key Takeaway Markov and transition models provide a simple yet powerful framework for studying change over time in systems with distinct states. They show not only where a system is likely to go next but also its long-term equilibrium, stability, and sensitivity to intervention. Whether tracking forests, species, or climate regimes, these models transform observations of “change” into quantitative forecasts of persistence and transformation. 5.9 Communicating Probabilistic Results Probabilistic results are often harder to communicate than single-number predictions, but they are more honest and useful. Good practices: Report ranges or confidence intervals instead of single values. Use visualizations such as: Histograms: frequency of outcomes. Fan plots: spread of model trajectories over time. CDFs: probability of exceeding thresholds. Phrase results as probabilities: “There’s a 30% chance of flood levels exceeding 2 m.” “90% of simulations predict temperatures rising above 1.5 °C by 2050.” Communicating uncertainty transparently fosters trust and supports risk-aware decisions. 5.10 Summary and Reflection Environmental systems are inherently variable and uncertain. Probabilistic models capture that variability by representing parameters, inputs, and outcomes as distributions rather than fixed values. Key ideas from this chapter: Probability distributions describe uncertainty quantitatively. PDFs and CDFs form the foundation for stochastic modeling. Monte Carlo simulations explore possible futures through random sampling. Markov models describe transitions between discrete system states. Probabilistic models shift focus from exact predictions to likelihoods and risks. By embracing uncertainty rather than avoiding it, environmental scientists build models that are both more realistic and more useful for decision-making under changing and unpredictable conditions. "],["app-wk1.html", "Appendix A Project Week 1 – The ‘why’ and the ‘what’", " Appendix A Project Week 1 – The ‘why’ and the ‘what’ \\(Y^n\\) Pick a system and ask why about some part you are interested Then ask a why again Keep asking why till you cant find the answer anymore Quick Demo Why does day length change? → Because of the seasons. As Earth orbits the Sun, sometimes your hemisphere is tilted toward the Sun (summer, longer days) and sometimes away (winter, shorter days). Why do we have seasons? → Because of the tilt of Earth’s axis. Earth’s axis is tilted about 23.5° relative to its orbit. This tilt changes how high the Sun appears in the sky and how long its path lasts each day. Why is Earth tilted? → Because of a giant collision in the early solar system. A Mars-sized body (often called Theia) likely struck Earth 4.5 billion years ago. This impact knocked Earth off a straight-up orientation and also produced the Moon. Why did that collision happen? → Because the early solar system was chaotic. When the Sun first formed, space around it was full of rocky planetesimals (early building blocks of planets). Their orbits overlapped, and gravity pulled them into frequent, violent collisions. Why was there a disk of planetesimals in the first place? → Because the solar system formed from a collapsing nebula. A cloud of gas and dust collapsed under gravity. As it collapsed, conservation of angular momentum caused the material to flatten into a spinning disk. Inside that disk, clumps grew into planetesimals and then planets. Why does the Earth spin in the first place? → Because of angular momentum inherited from the disk. The collapsing nebula was already rotating. As clumps of matter formed into planets, they retained that spin. Collisions and impacts modified Earth’s rotation rate and axis, but didn’t stop the overall spin. Why did the nebula collapse? → Because of gravity and outside triggers. Dense regions of interstellar gas clouds naturally collapse under their own weight. This process may have been accelerated by shock waves from a nearby supernova explosion. Why was there a cloud of gas and dust? → Because of earlier generations of stars. Stars burn fuel and die. Supernovae scatter their contents into space, creating gas and dust clouds rich in heavy elements. Our solar system is made of this recycled stardust. Why do stars form and die? → Because of gravity and nuclear fusion. Gravity compresses gas until fusion ignites. Fusion powers stars until the fuel is gone, at which point they evolve into white dwarfs, neutron stars, or black holes. Why does nuclear fusion work? → Because of the fundamental forces of nature. Fusion is governed by gravity, electromagnetism, and the strong/weak nuclear forces. These make it possible for nuclei to fuse and release energy. Why do these fundamental forces exist, and why do they have the values they do? → That is an open question. Physics describes and measures these forces, but doesn’t explain why they exist at all. This is the frontier where science meets philosophy and cosmology. \\(Y^{11}\\) Task 1 - Group of 5 Workshop your individual ideas with your group Spend 3 mins on each person (I’ll set a timer) Ask y’s Task 2 - 1 Line Description Take a couple minutes and try to write down - in one line - where your idea is at. This is not your final project topic - just a progress report Post this after class in the discussion opened for project ideas [P&amp;P] Comment on 3 posts - ask another specific why? [P&amp;P] Task 3 - Network Find the like minded people in the room Team up? Couple your models? Share thoughts "],["workbook-week-1-what-is-modeling.html", "Appendix B Workbook Week 1: What is Modeling?", " Appendix B Workbook Week 1: What is Modeling? Class Discussion Lets build a working definition of the central components of this course. In your groups see if you can define Coding Models/Modeling Simulation Systems Logic Syntax A key skill in being a modeler is the ability of abstraction Sketching Exercise A sketch is a visual model. Task: Draw a picture of a chair (1min) Share your picture with those around you What assumptions did you all make in your model Which sketch was right? Systems Thinking - what do we mean by this? Lets do my phd in 5 mins - plants in desert systems - lets think about this as a system. Pseudo code The art of writing good instructions I’m a robot, and you want me to toast a bagel and put Vegemite on it. Write me the pseudo code to make this happen. This is what happens when you give bad instructions: This is what’s possible with complete instructions: Group activity - Dice We want to know what the most likely total you would get from rolling two 6 sided die. Write some pseudo code to collect data to help you work this out. Guessing Game Write pseudo code that would count how many guesses you would take to find a number between 1 and 10. Needle in a Haystack In the Dan Brown novel ‘Angels and Demons’ the Vatican is about to be blown up by a antimatter device. In the movie, the bad guys has a webcam on the device. Someone has the idea of systematically cutting power to the different sectors of the power grid to narrow down the location. They dismiss is because they work out there isn’t enough time to cycle through all the sectors. But you took my class and actually make this idea work. Lets assume: We have 1,048,576 possible sectors. It takes 1hr for a sector to respond to a change What’s the longest time it would take to find the sector? What is the shortest time it would take to be sure you found the sector? Write some pseudo code "],["workbook-week-2-llms.html", "Appendix C Workbook Week 2: LLMs C.1 How to Build Complixity into a Problem C.2 LLMs and Modeling Support C.3 Friday Discussion - AI, Society &amp; the Environment", " Appendix C Workbook Week 2: LLMs C.1 How to Build Complixity into a Problem We briefly discussed the need to layer complexity when trying to model. Lets work through an example of that. Banksy’s Balloon Model Have you ever wondered what happens to a helium filled balloon that is released? What we will do is build a mathematical model of the balloon to help answer this question. What controls the height at which a the balloon will climb? \\[B(z) = f(\\] Abstraction What is the simplest form of the problem we could solve? What assumptions could we make to help us get started? Neutral Buoyancy Given the table below and the density information about helium. What height would the balloon get to? At sea-level conditions (about \\(T = 288\\,\\text{K}\\), \\(P = 101{,}325\\,\\text{Pa}\\)): Helium density: \\[ \\rho_{\\text{He}} \\approx 0.1785\\ \\text{kg/m}^3 \\] Evaluate and then add complexity Does this feel right? What assumptions did we make that might have been too simplistic? What math model could we apply to add complexity? Ideal Gas Law The ideal gas law relates the pressure, volume, temperature, and number of moles of a gas: \\[ PV = nRT \\] Definitions \\(P\\): Pressure of the gas (Pa or atm) \\(V\\): Volume of the gas (m³ or L) \\(n\\): Number of moles of gas (mol) \\(R\\): Ideal gas constant \\(8.314\\ \\text{J·mol}^{-1}\\text{·K}^{-1}\\) (SI units) \\(0.08206\\ \\text{L·atm·mol}^{-1}\\text{·K}^{-1}\\) (common chemistry units) \\(T\\): Absolute temperature (Kelvin, K) Notes - The equation assumes an ideal gas (no inter-molecular forces, particles take up negligible space). - Works well for helium and other light gases at normal temperatures and pressures. - Can be rearranged into useful forms, e.g. density: Flexible Balloon Let’s let the volume of the balloon change - removing the rigid balloon requirement. \\[ PV = nRT \\] Rearranging for n: \\[ n = \\frac{PV}{RT} \\] We know n can’t change as the balloon isn’t leaking. So we can think of the balloon in two places \\[ n_{msl} = n_{top} \\] so plug the rest in \\[ \\frac{P_1V_1}{T_1}=\\frac{P_2V_2}{T_2} \\] Ask ourselves what changes based on out assumptions What is going to happen to the volume of the balloon as it climbs? What happens to the density of helium if the volume increases? \\[ \\rho = \\frac{M}{V} \\] Evaluate - Does this make sense? The next thing is to model where the balloon will land. This tool uses the near term forecast as well as the balloon’s parameters to determine the most likely trajectory. SondeHub Flight Predictor C.2 LLMs and Modeling Support C.2.1 Learning Objectives By the end of this week, students should be able to: Explain what large language models (LLMs) are and how they can support simulation and coding. Apply prompt engineering techniques to improve model development. Use LLMs to re-frame and clarify environmental modeling challenges. Critically evaluate when and how it is appropriate to use AI tools in science. Incorporate LLMs into workflows for reproducibility, documentation, and troubleshooting in R. C.2.2 Coding warmup Pseudo-code and r script activity Create a script that fits a line of best fit to the following string of 10 numbers 6,1,7,2,3,3,9,3,3,0 Create the flexibility in the code to fit a nth order polynomial of your choosing. Before you run build an expectation What do expect the graph to look like with n=1 n=5 n=9 n=12 What evaluation tools/outputs could you create so that you can ‘test’ the output? Compare your expectations with your output Compare your outputs with the people around you C.2.3 What is a Large Language Model 1 min Discussion What is a LLM and how does it work? Class Discussion What are the dangers of highly parameterized model? Pros and cons of parameter counts? C.2.4 Pros and Cons of High-Parameter Models High-parameter (or “high-complexity”) models — like very high-degree polynomials, deep neural networks with many layers, or regression models with lots of predictors — have clear advantages and drawbacks. C.2.4.1 ✅ Pros Flexibility &amp; Expressiveness Can capture very complex relationships, including nonlinear patterns that simple models would miss. For example: a 9th-degree polynomial can fit 10 points exactly. Low Training Error With enough parameters, the model can drive error on the training set down to nearly zero. Useful if your goal is interpolation of the given data rather than generalization. Captures Subtle Structure Sometimes, especially with rich datasets, complexity helps reveal real underlying trends that simpler models would smooth over. C.2.4.2 ❌ Cons Overfitting The model fits noise as if it were signal. Predictions on new data are often unstable and inaccurate. Interpretability High-degree polynomials or models with many coefficients are hard to interpret or explain. Coefficients may be large, unstable, or counter-intuitive. Numerical Instability High-order polynomials can produce NAs or huge coefficients due to ill-conditioning. Small changes in input lead to large swings in output. Computational Cost More parameters = more computation, longer training, and sometimes risk of convergence issues. Generalization Risk High training accuracy doesn’t guarantee real-world usefulness. Models may fail badly outside the range of training data. C.2.5 Parameters in Large Language Models (LLMs) Large Language Models (LLMs) are defined in part by the number of parameters they contain — the trainable weights in their neural networks. These parameters are like knobs the model adjusts during training to learn patterns in data. C.2.5.1 ⚙️ Parameters in Modern LLMs GPT-2 (2019) → ~1.5 billion parameters GPT-3 (2020) → 175 billion parameters PaLM (Google, 2022) → 540 billion parameters GPT-4 (2023) → parameter count not officially disclosed, but estimates suggest hundreds of billions to over a trillion GPT-4 Turbo (2023, OpenAI API) → optimized variant, size undisclosed, but still in the “hundreds of billions” range Anthropic’s Claude 3 (2024) → not public, but assumed similar scale (hundreds of billions) Gemini Ultra (Google DeepMind, 2024) → also undisclosed, estimated trillion-scale C.2.5.2  What “Parameters” Mean Each parameter is just a number (a weight) that influences how input tokens get transformed through the layers of the neural net. More parameters = more capacity to model complex relationships, but also: Requires more data to train Much more compute (training GPT-3 took thousands of GPUs for weeks) Can increase risk of overfitting if not carefully regularized C.2.5.3  Trend in LLM Growth 2018–2020 → billions of parameters 2021–2023 → hundreds of billions 2024 onward → trillion+ parameter models (but with a shift toward efficiency — smaller models trained better) Contextualizing these large numbers 1 million seconds –&gt; 11.6 days 1 billion seconds –&gt; 31.7 years (~1.5 of your lifetimes) 1 trillion seconds –&gt; 31,700 years (~1,500 your lifetimes) C.2.5.4  Table: LLMs and Parameter Counts Model Year Parameters (approx.) Notes GPT-2 2019 1.5B First widely known OpenAI LLM GPT-3 2020 175B Major leap in scale PaLM (Google) 2022 540B Pathways Language Model GPT-4 2023 100B–1T (est.) Exact number undisclosed GPT-4 Turbo 2023 100B+ (est.) Optimized API variant Claude 3 (Anthropic) 2024 100B+ (est.) Scale similar to GPT-4 Gemini Ultra (Google) 2024 1T+ (est.) Trillion-scale model ✅ Summary: Modern LLMs like GPT-4, Claude 3, or Gemini are likely running in the hundreds of billions to trillions of parameters range. C.2.6 Capabilities and Limits of LLMs Discussion: What are the Capabilities and Limits of LLMs Reflection Prompt Capabilities and Limits of LLMs ✅ Capabilities of LLMs Generate readable text in many styles Scientific summaries Conversational explanations Adapt tone for peers, policymakers, or the public Produce and troubleshoot code Works across multiple languages (R, Python, MATLAB) Draft starter scripts, find syntax errors, explore alternatives Summarization tools Condense long articles, datasets, or equations Highlight key insights and trends Translate technical content into plain language Make specialized knowledge understandable to non-experts Support communication of environmental science to diverse audiences ⚠️ Limits of LLMs Hallucination Can produce text that sounds plausible but is factually wrong Bias in training data May reproduce stereotypes or skew perspectives Lack of true reasoning/understanding Predicts patterns statistically, not by scientific comprehension Explanations may oversimplify or omit key assumptions Reproducibility challenges Same prompt can yield different outputs Hard to fully standardize in scientific workflows Which of the capabilities described here could have supported your work? Which limitations would you need to watch out for? How might you balance the efficiency of using an LLM with the need for accuracy and scientific rigor? C.2.7 LLMs in environmental modeling workflows Activity: Explain a Complex Model with Stepwise Prompting Google Doc For Group Notes We’ll use stepwise (chain-of-thought–style) prompting to unpack a very complex partial differential equation into clear, audience-appropriate language without asking the AI to reveal its private reasoning. The goal is to force a structured, term-by-term explanation and surface assumptions. Note: we are purposefully using a complex example here so that we can really see the value and dangers of utilizing a LLM for environmental modeling. Model The Advection–Diffusion (or Dispersion) Equation for pollutant transport in a river: \\[ \\frac{\\partial C}{\\partial t} = D \\frac{\\partial^2 C}{\\partial x^2} - v \\frac{\\partial C}{\\partial x} - kC \\] - \\(C\\): concentration at position \\(x\\) and time \\(t\\) - \\(D\\): diffusion coefficient (mixing) - \\(v\\): flow velocity (downstream transport) - \\(k\\): decay rate (removal) Step 1 — Your Own Explanation Write a plain-language explanation for a non-scientist audience (e.g., a community group). If you have no idea whats going on - take a guess. Go term by term and see if you can decipher whats going on. Step 2 — Baseline AI Explanation Ask an LLM for a plain-language explanation. Save the response. Baseline prompt: Explain the equation below in plain language for a non-scientist audience. \\[ \\frac{\\partial C}{\\partial t} = D \\frac{\\partial^2 C}{\\partial x^2} - v \\frac{\\partial C}{\\partial x} - kC \\] Keep it to 6–8 sentences. Take a second here and compare your result with those at your table? Are thy identical? Step 3 — Stepwise Prompting (Structured Sections) Now force structure so the AI unpacks complexity term-by-term and surfaces assumptions. Stepwise prompt template (copy-paste) Explain the equation below using labeled sections. Do not show your internal reasoning; present only your final explanation. Sections (use headings): 1) Term-by-term meaning — explain each term in one sentence. 2) Physical interpretation — connect each term to a river process with a brief analogy. 3) Assumptions — list key modeling assumptions (e.g., dimensionality, parameter constancy, uniform mixing). 4) Units &amp; parameters — specify typical units for \\(C, D, v, k\\). 5) Edge cases — describe what happens if \\(D=0\\), \\(v=0\\), or \\(k=0\\). 6) Plain-language summary — 3 sentences for a public audience. Equation: \\[ \\frac{\\partial C}{\\partial t} = D \\frac{\\partial^2 C}{\\partial x^2} - v \\frac{\\partial C}{\\partial x} - kC \\] Step 4 — Compare &amp; Critique Clarity: Which version (baseline vs. stepwise) is clearer and why? Completeness: Did the stepwise version expose assumptions or units the baseline missed? Accuracy: Note any incorrect claims or overconfidence. Most importantly - which version did you learn something from? Step 5 — Constraint Refinement Re-prompt with tighter constraints to match a specific audience. Audience-tuning examples Policy brief style (≤150 words, 8th-grade reading level). Technical appendix style (include parameter ranges and citations placeholder). Infographic caption style (≤90 words, 3 bullets + 1 summary sentence). How did it do translating complex ideas? Extension (optional) Ask the AI to propose a simple diagram description (no image needed): axes, arrows for diffusion/advection, and a decay curve. Use this as a storyboard for a figure you might create later. C.3 Friday Discussion - AI, Society &amp; the Environment Students will rotate through 6 stations, discussing and writing responses to each prompt. Station 1 – Environmental Applications Prompt: How could LLMs help in environmental science (climate modeling, biodiversity tracking, sustainability research)? Use Description / Findings Role of AI/LLMs Citation Automated ecological data extraction LLMs used to parse ecological literature 50× faster than humans, with &gt; 90% accuracy for categorical data. Text mining &amp; knowledge extraction Nature (2024) Biodiversity commitments vs renewables tradeoffs LLM + GIS framework to compare biodiversity promises vs real-world impacts in renewable energy projects. Synthesizing documents with spatial data Purdue (2024) Policy &amp; governance support LLM-based chatbot assisting with biodiversity treaty policy interpretation and decision-making. Policy Q&amp;A, summarization &amp; interpretation Nature (2025) Land-use / biodiversity predictions Cambridge “Terra” AI tool predicts biodiversity impacts of land-use, supporting policy tradeoffs. Modeling + scenario analysis Cambridge (2025) Biodiversity &amp; conservation AI helps with species detection, habitat mapping, and biodiversity understanding. Pattern recognition (images, acoustics, mapping) OSU Imageomics (2025) Risks &amp; benefits review Review article on how LLMs can support environmental participation but also bring risks. Framing debates, generating text &amp; synthesis ACS EST (2023) Station 2 – Risks in Science &amp; Policy Prompt: What are the risks if AI models mislead scientists, policymakers, or the public about environmental issues? Station 3 – Environmental Footprint of AI Prompt: LLMs require huge amounts of energy and water to run. Is their environmental cost justified by their benefits? Why or why not? Water Use Sources: https://watercalculator.org/; Lawrence Berkeley National Labs Scenario Liters per person per year People needed to reach 1B liters/year Direct household use ~114,000 L ~8,800 people Full water footprint (direct + virtual) ~2,842,000 L ~350 people Station 4 – Learning &amp; Academic Integrity Prompt: How should students and researchers use AI responsibly in their work? Where’s the line between help and cheating? Tokens processed - why the drop in the June? Station 5 – Equity &amp; Bias Prompt: Who risks being excluded? How might biases in LLMs affect society and science? Disparity / Exclusion / Bias How / Why Solution? Station 6 – Future of Work &amp; Society Prompt: How might AI change jobs, communication, and decision-making in the next 10 years? What should never be automated? "],["workbook-week-3-models-and-growth.html", "Appendix D Workbook Week 3: Models and Growth D.1 Learning Goals D.2 Lab: Building Your Own Population Model D.3 Readme: Interactive Linear Fit — Cod Population (Shiny) D.4 Readme: Interactive Logistic Fit — Cod Population (Shiny) D.5 Readme: Logistic Growth with Constant Harvesting &amp; Delayed Start — Cod Population (Shiny) D.6 Readme: Logistic with Harvesting &amp; Stochastic Bad-Year Shocks — Cod Population (Shiny) D.7 Understanding the Central Limit Theorem", " Appendix D Workbook Week 3: Models and Growth D.1 Learning Goals In this chapter, you’ll build both conceptual understanding and practical modeling skills. By the end, you should be able to: Explain the structure of a mathematical model Identify variables, parameters, functions, initial conditions, and boundary conditions in an environmental system. Construct a conceptual model Create a diagram or influence map showing the main components and feedbacks in a system (e.g., a fishery, carbon cycle, or pollutant pathway). Translate that conceptual model into a quantitative form—an equation or simulation—that captures its essential dynamics. Simulate system behavior Work directly with a provided R model (linear, exponential, or logistic) to explore how changes in parameters such as the growth rate (\\(r\\)) or carrying capacity (\\(K\\)) affect system outcomes. Fit models to data Use observational data (e.g., cod fishery population time series) to estimate parameters, visualize model fits, and interpret model accuracy (RMSE, residuals). Interpret model parameters ecologically Describe what each parameter represents in real terms—e.g., how \\(r\\) relates to reproduction or how \\(K\\) reflects habitat limits. Explore uncertainty and sensitivity Conduct parameter sweeps and “what-if” simulations to see which assumptions most influence model predictions. Evaluate models as decision tools Use the logistic-harvest model to test management scenarios (e.g., sustainable vs. overharvest conditions). Discuss the implications of stochasticity (“bad years”) for population resilience. Reflect on the modeling process Compare your conceptual model with the mathematical one and explain what each form helps you understand. Articulate how model simplicity versus realism affects insight and decision-making.  Purpose of this section: You’ll not only learn how to fit and simulate models, but also how to think like a modeler—starting from a conceptual diagram of a system, translating it into equations, running simulations, and interpreting what the results tell us about environmental processes and management trade-offs. Working Definitions In your own words define the following Variable Parameter Function Initial Condition Boundary Condition Activity: Build a Conceptual Model — “Water Levels in Lake Washington”  Purpose: To practice thinking like a modeler by building a conceptual model of water balance in Lake Washington. You’ll identify the main inputs, outputs, controls, and feedbacks that determine how lake levels rise and fall — just as we do in environmental modeling.  Background: Lake Washington’s water level is managed through the Hiram M. Chittenden (Ballard) Locks, which connect the lake to Puget Sound. Its level fluctuates with precipitation, river inflow, groundwater exchange, evaporation, and controlled outflow to the Locks. In summer, levels are lowered to protect dock structures and fish migration; in winter, levels rise naturally with rainfall. In short — Lake Washington is a dynamic system influenced by climate, management, and human use.  Part 1: Brainstorm the System: In small groups, sketch a conceptual model (boxes and arrows) of Lake Washington’s water balance. Stocks (state variables): Inflows: Outflows: Drivers and Controls:  Part 2: Identify Feedbacks Positive feedbacks (reinforcing): Negative feedbacks (stabilizing): Label feedbacks with (+) or (–) arrows in your diagram.  Part 3: Translate to a Quantitative Model Now imagine you had to model lake level \\(L(t)\\) mathematically. Try an write a functional relationship for the system. Identify your output variable and its relation to the input variables  Part 4: Reflect Discuss as a group: Which factors are natural and which are human-controlled? How might climate change affect this system? How could your conceptual model inform management decisions about flood control, salmon passage, or recreation? If you were to create a simulation model of this system - what kind of model category would it fall into? If you were to simulate this system in R, what assumptions would you need to make?  Takeaway: Conceptual models help simplify complex systems like Lake Washington into a few key relationships. Before we ever write an equation or a line of code, conceptual modeling helps us clarify what processes matter — and how they interact. Introduction to Curve Fitting In science and engineering, we often collect data that show a clear trend — but rarely follow a perfect mathematical formula. Curve fitting is the process of finding a mathematical function that best describes the relationship between variables in a dataset. By fitting a curve, we can: Summarize complex data with a simple equation. Estimate values between or beyond measured points (interpolation and extrapolation). Compare theoretical models to real-world observations. Identify patterns that reveal underlying processes — for example, exponential population growth, logistic saturation, or seasonal oscillations. Curve fitting bridges the gap between data and models. It allows us to test hypotheses (“Does this population follow logistic growth?”), estimate parameters (like growth rate or carrying capacity), and assess how well our chosen model represents reality. How Fitting Is Achieved Curve fitting involves adjusting the parameters of a chosen mathematical model so that it best matches the observed data. The goal is to minimize the difference between the model’s predicted values and the actual data points — these differences are called residuals. The most common method is least squares fitting, which minimizes the sum of the squared residuals. For a dataset with observed values \\(y_i\\) and model predictions \\(\\hat{y}_i\\), the sum of squared errors (SSE) is: \\[ SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] The fitting process adjusts model parameters (such as slope, intercept, or growth rate) to make this SSE as small as possible. A related measure is the Root Mean Square Error (RMSE), which provides an interpretable measure of average error in the same units as the data: \\[ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\] While the least squares method minimizes the total squared error, the RMSE summarizes how far, on average, the predictions are from the observed data. Lower RMSE values indicate a better fit. In practice, curve fitting tools in R, Python, or MATLAB use numerical optimization algorithms to minimize SSE or RMSE automatically, allowing us to identify the parameters that best describe the observed trend. Curve Fitting Example - CO2 # Load data, skipping metadata lines that start with &#39;#&#39; co2_data &lt;- read.table(&quot;Models/data/co2_data.txt&quot;, header = FALSE, comment.char = &quot;#&quot;, col.names = c(&quot;Year&quot;, &quot;Month&quot;, &quot;DecimalDate&quot;, &quot;CO2_Monthly&quot;, &quot;CO2_Deseason&quot;, &quot;NumDays&quot;, &quot;StdDev&quot;, &quot;Uncertainty&quot;)) # Check the first few rows head(co2_data) ## Year Month DecimalDate CO2_Monthly CO2_Deseason NumDays StdDev Uncertainty ## 1 1958 3 1958 315.7 314.4 -1 -9.99 -0.99 ## 2 1958 4 1958 317.4 315.2 -1 -9.99 -0.99 ## 3 1958 5 1958 317.5 314.7 -1 -9.99 -0.99 ## 4 1958 6 1958 317.3 315.1 -1 -9.99 -0.99 ## 5 1958 7 1959 315.9 315.2 -1 -9.99 -0.99 ## 6 1958 8 1959 314.9 316.2 -1 -9.99 -0.99 library(ggplot2) ggplot(co2_data, aes(x = DecimalDate, y = CO2_Monthly)) + geom_line(color = &quot;steelblue&quot;) + labs(title = &quot;Mauna Loa C02 Record&quot;, x = &quot;Year&quot;, y = expression(&quot;CO&quot;[2]*&quot; concentration (ppm)&quot;)) + theme_minimal() Fit a linear model By Eye Open and run curve_fit_co2_1.R Try to fit a trend line? What would make this task easier? Fit a linear model By Eye Using Residuals Open and run curve_fit_co2_2.R How do the residuals help? What are you aiming for? Fit a linear model to a Subset of the Data Open and run curve_fit_co2_3.R This app includes a magic butting that fits the parameters for you. Play around, see if you can fit a line to the last 10 years What is happening to the residuals? Ideas on how to fix this? Fit an exponential model to a Subset of the Data Open and run curve_fit_co2_4.R Play around, see if you can fit a model to the last 10 years What is happening to the residuals? Ideas on how to fix this? Fit an complex model Open and run curve_fit_co2_5.R We’ve been layering complexity onto this model Adding a sinusoid is a modeling trick - given the residuals have a sinusoidal shape, that gives us a hint that whatever we are ‘missing’ in our model could be represented as a sinusoid. Play around D.2 Lab: Building Your Own Population Model Overview In this lab, you will build, test, and refine population models using real or simulated ecological data. By the end, you should be able to: Load and visualize population data Construct a conceptual model of population dynamics Fit and compare different population growth models Interpret model parameters in ecological terms Extend models to include harvesting and environmental variability Task 1 – Load and Explore the Data Load the dataset into R and create a simple plot to visualize the population through time. Look for general patterns (growth, plateaus, declines, disturbances) and write down your observations. Reflection: What trends do you see? Task 2 – Build a Conceptual Model Draw a conceptual diagram (no equations yet). Use boxes for key variables and arrows for relationships or feedbacks. Reflection: What controls population growth? What limits it? What external factors could influence it? Task 3 – Start Simple: Linear Growth Model Write pseudo code to fit a linear growth model to the data. Use a stepwise prompting strategy — begin with a basic line, then add complexity: Add interactive sliders to adjust parameters. Add diagnostic metrics (e.g., RMSE) to assess fit quality. Allow model fitting to subsets of the data. Reflection: Why start with a linear model? Are there time periods where the population appears roughly linear? What questions could you answer with a linear model? Task 4 – Extend to Logistic Growth Replace the linear model with a logistic model: \\[ P(t) = \\frac{K}{1 + A e^{-r t}} \\] Before coding, analyze this function analytically: What happens as each parameter (\\(A\\), \\(K\\), \\(r\\)) becomes small, large, or negative? What are reasonable ecological constraints on these parameters? What does each parameter represent in real-world terms? Reflection: Summarize your reasoning and expectations before testing numerically. Task 5 – Fit the Logistic Model Using the same stepwise prompting strategy, implement and test the logistic model numerically. Compare numerical results to your analytical expectations. Add plot features (e.g., equilibrium lines, shaded regions) to support your interpretation. Reflection: Did your results confirm your intuition? How could you visualize uncertainty or residuals? Task 6 – Introduce Harvesting Scenario: This species of fish was overfished in the 1970s. In the 1980s, fishing was banned to allow recovery. Modify your logistic model to include a harvesting term. Explore how harvesting rate affects long-term equilibrium and recovery trajectories. Reflection: - How does equilibrium population change with harvesting rate \\(H\\)? - What happens if you extrapolate beyond your data? Task 7 – Parameter Sensitivity (Sweep Analysis) Write pseudo code to explore parameter sensitivity using loops. Create visualizations showing: Equilibrium population vs. harvesting rate Sensitivity of population recovery to historical or future harvests Be creative—surface plots, contour maps, or animations are all valid. Reflection: What parameters is the system most sensitive to? What management implications might this have? Task 8 – Adding Environmental Variability Now lets question one of our model assumptions: Is \\(r\\) (the intrinsic growth rate) really constant? Create pseudo code that allows \\(r\\) to vary between good years and bad years: In a good year, \\(r\\) stays the same. In a bad year, \\(r\\) is scaled by a factor between 0 and 1. Add sliders to adjust: - The frequency of bad years - The scaling factor for \\(r\\) Reflection: - How does stochasticity affect population resilience? - How could models like this support adaptive management or conservation policy? D.3 Readme: Interactive Linear Fit — Cod Population (Shiny) This section introduces the Interactive Linear Model App designed to explore the Cod population dataset. The accompanying README and source code provide an overview of the app’s structure and highlight the key code components that power its functionality, including data loading, model fitting, and visualization. An interactive Shiny app for tuning a linear model by eye and comparing it to the ordinary least squares (OLS) best fit using RMSE as the reference metric. All displayed values and sliders use 3 significant figures. What this app does Loads a cod population time series (Year, Pop) and creates a centered time variable \\(t = \\text{Year} - \\min(\\text{Year})\\) Lets users interactively adjust a linear model: \\(\\text{Pop}(t) = a + b \\cdot t\\) Shows your equation and RMSE in real time and overlays the OLS (min-RMSE) line as a dashed reference Includes a “Snap to OLS” button to set sliders to the best-fit coefficients If data/cod_timeseries.csv is missing, the app will fall back to synthetic demo data (noted in the UI) Why this is useful Builds intuition for model form, slope/intercept interpretation, and error metrics (RMSE) Demonstrates the value of centering time (numerical stability, interpretability) Encourages hands-on exploration before relying on automated fitting Requirements R (≥ 4.2 recommended) Packages: shiny, tidyverse (specifically readr, dplyr, and ggplot2) Install once: install.packages(c(&quot;shiny&quot;, &quot;tidyverse&quot;)) File structure #your-project/ #├─ app.R #└─ data/ # └─ cod_timeseries.csv # Your dataset (optional; app falls back if missing) Expected CSV schema Required columns: Year, Pop Example (header + 5 rows): csv Year,Pop 1960,2.31 1961,2.45 1962,2.38 1963,2.52 1964,2.60 Note: Years don’t need to start at 0; the app internally creates \\(t = \\text{Year} - \\min(\\text{Year})\\). How to run From the project directory: shiny::runApp(&quot;app.R&quot;) Or open app.R in RStudio and click Run App. App controls &amp; outputs Sliders: Intercept (a) – baseline population at the first observed year (since \\(t = 0\\) there) Slope (b) – rate of change in population per year Snap to OLS – sets a, b to the least-squares solution Equation &amp; RMSE (your line) – updates dynamically Equation &amp; RMSE (OLS) – fixed reference based on lm(Pop ~ t) Plot layers: Points → observed data Faint line → observed connection over time Solid line → your current (a, b) Dashed line → OLS reference line Under the hood (key functions) Data load: read_csv(&quot;data/cod_timeseries.csv&quot;, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) Centering time: t = Year - min(Year) RMSE: \\(\\text{RMSE} = \\sqrt{\\text{mean}\\big((\\text{obs} - \\text{pred})^2\\big)}\\) OLS reference: lm(Pop ~ t, data = cod) Customization tips Change y-axis label: Edit labs(y = \"Population (units)\") in the plot section. Adjust slider ranges: Modify the a_rng and b_rng logic near the top of the script. Colors/line styles: Tweak the geom_line(...) aesthetics in renderPlot. Troubleshooting Error: “Could not find function ‘read_csv’” → Install and load tidyverse: install.packages(&quot;tidyverse&quot;) library(tidyverse) Blank app / load errors: Check that data/cod_timeseries.csv exists and has Year,Pop headers. If missing, the app uses synthetic demo data. Unexpected RMSE values: Check data units and missing values. RMSE is computed with na.rm = TRUE. Source Code # app.R # Interactive linear fit tuner (with best-RMSE reference) for cod population data # All displayed values and sliders use 3 significant figures library(shiny) library(tidyverse) # --------------------------- # Data load (with safe fallback) # --------------------------- load_cod_data &lt;- function() { path &lt;- &quot;data/cod_timeseries.csv&quot; read_csv(path, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) } cod &lt;- load_cod_data() |&gt; mutate(t = Year - min(Year)) # --------------------------- # Helpers # --------------------------- rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2, na.rm = TRUE)) # OLS (min-RMSE for linear model) ols &lt;- lm(Pop ~ t, data = cod) a_best &lt;- unname(coef(ols)[1]) b_best &lt;- unname(coef(ols)[2]) best_rmse &lt;- rmse(cod$Pop, fitted(ols)) # Slider ranges (reasonable defaults) t_range &lt;- range(cod$t, na.rm = TRUE) pop_rng &lt;- range(cod$Pop, na.rm = TRUE) a_rng &lt;- c(pop_rng[1] - 0.5 * diff(pop_rng), pop_rng[2] + 0.5 * diff(pop_rng)) b_scale &lt;- ifelse(diff(t_range) &gt; 0, diff(pop_rng) / diff(t_range), 1) b_rng &lt;- c(-3, 3) * b_scale # --------------------------- # UI # --------------------------- ui &lt;- fluidPage( titlePanel(&quot;Interactive Linear Fit (3 sig figs): Cod Population&quot;), sidebarLayout( sidebarPanel( helpText(&quot;Model: Pop(t) = a + b * t with t = Year - min(Year)&quot;), sliderInput(&quot;a&quot;, &quot;Intercept (a):&quot;, min = signif(a_rng[1], 3), max = signif(a_rng[2], 3), value = signif(a_best, 3), step = signif(diff(a_rng) / 200, 3)), sliderInput(&quot;b&quot;, &quot;Slope (b):&quot;, min = signif(b_rng[1], 3), max = signif(b_rng[2], 3), value = signif(b_best, 3), step = signif(diff(b_rng) / 200, 3)), actionButton(&quot;snap&quot;, &quot;Snap to OLS (min RMSE)&quot;), hr(), strong(&quot;Your line:&quot;), verbatimTextOutput(&quot;eqn_user&quot;, placeholder = TRUE), div(&quot;RMSE (your line):&quot;), h3(textOutput(&quot;rmse_user&quot;), style = &quot;margin-top:-8px;&quot;), hr(), strong(&quot;OLS reference (dashed line):&quot;), verbatimTextOutput(&quot;eqn_best&quot;, placeholder = TRUE), div(&quot;RMSE (OLS):&quot;), h4(textOutput(&quot;rmse_best&quot;), style = &quot;margin-top:-8px;&quot;) ), mainPanel( plotOutput(&quot;fitplot&quot;, height = 480), br(), tags$small(em( if (file.exists(&quot;data/cod_timeseries.csv&quot;)) { &quot;Loaded data/cod_timeseries.csv&quot; } else { &quot;No data/cod_timeseries.csv found — using synthetic demo data.&quot; } )) ) ) ) # --------------------------- # Server # --------------------------- server &lt;- function(input, output, session) { # Snap sliders to OLS on click observeEvent(input$snap, { updateSliderInput(session, &quot;a&quot;, value = signif(a_best, 3)) updateSliderInput(session, &quot;b&quot;, value = signif(b_best, 3)) }) # Predictions for user&#39;s sliders preds_user &lt;- reactive({ tibble( Year = cod$Year, t = cod$t, Pred = input$a + input$b * cod$t ) }) # Text outputs output$eqn_user &lt;- renderText({ paste0(&quot;Pop(t) = &quot;, signif(input$a, 3), &quot; + &quot;, signif(input$b, 3), &quot; * t&quot;) }) output$rmse_user &lt;- renderText({ sprintf(&quot;%.3f&quot;, rmse(cod$Pop, preds_user()$Pred)) }) output$eqn_best &lt;- renderText({ paste0(&quot;Pop(t) = &quot;, signif(a_best, 3), &quot; + &quot;, signif(b_best, 3), &quot; * t&quot;) }) output$rmse_best &lt;- renderText({ sprintf(&quot;%.3f&quot;, best_rmse) }) # Plot output$fitplot &lt;- renderPlot({ # Smooth lines for display grid &lt;- tibble( Year = seq(min(cod$Year), max(cod$Year), length.out = 400) ) |&gt; mutate(t = Year - min(cod$Year), Pred_user = input$a + input$b * t, Pred_best = a_best + b_best * t) ggplot(cod, aes(Year, Pop)) + geom_point(size = 2, alpha = 0.9) + geom_line(alpha = 0.35) + # User-selected line geom_line(data = grid, aes(y = Pred_user), linewidth = 1, color = &quot;#0072B2&quot;) + # OLS (best fit) geom_line(data = grid, aes(y = Pred_best), linewidth = 1, linetype = &quot;dashed&quot;, color = &quot;grey40&quot;) + labs( title = &quot;Cod Population with Interactive Linear Fit&quot;, subtitle = paste0( &quot;a = &quot;, signif(input$a, 3), &quot;, b = &quot;, signif(input$b, 3), &quot; | RMSE (yours) = &quot;, sprintf(&quot;%.3f&quot;, rmse(cod$Pop, preds_user()$Pred)), &quot; | RMSE (OLS) = &quot;, sprintf(&quot;%.3f&quot;, best_rmse) ), x = &quot;Year&quot;, y = &quot;Population (units)&quot; ) + theme_classic() }) } shinyApp(ui, server) D.4 Readme: Interactive Logistic Fit — Cod Population (Shiny) This section introduces the Interactive Logistic Model App designed to explore the Cod population dataset. The accompanying README and source code provide an overview of the app’s structure and highlight the key code components that power its functionality, including data loading, nonlinear model fitting, and visualization. An interactive Shiny app for tuning a logistic growth model by eye and comparing it to the nonlinear least squares (base R nls) best fit using RMSE as the reference metric. All sliders and displayed values use 3 significant figures. What this app does Loads a cod population time series (Year, Pop) and creates a centered time variable \\(t = \\text{Year} - \\min(\\text{Year})\\) Lets users interactively adjust a logistic model: \\(\\text{Pop}(t) = \\dfrac{K}{1 + A\\,e^{-r t}}\\) Computes and displays your equation and RMSE in real time; overlays the best-fit (nls) curve as a dashed reference Provides a “Snap to Best Fit (nls)” button to set sliders to the converged nls parameters (if convergence succeeds) Notes in the UI if data/cod_timeseries.csv is missing (no synthetic data is auto-generated in this script) Why this is useful Builds intuition for carrying capacity \\(K\\), initial position \\(A\\), and growth rate \\(r\\) in logistic dynamics Demonstrates practical nonlinear curve fitting with base R nls and comparison via RMSE Encourages exploratory model-based reasoning beyond straight lines Requirements R (≥ 4.2 recommended) Packages: shiny, tidyverse (specifically readr, dplyr, ggplot2) Install once: install.packages(c(&quot;shiny&quot;, &quot;tidyverse&quot;)) File structure #your-project/ #├─ app.R #└─ data/ # └─ cod_timeseries.csv # Your dataset (optional; app falls back if missing) Expected CSV schema Required columns: Year, Pop Example (header + 5 rows): csv Year,Pop 1960,2.31 1961,2.45 1962,2.38 1963,2.52 1964,2.60 Note: The app internally creates \\(t = \\text{Year} - \\min(\\text{Year})\\). Years need not start at 0. How to run From the project directory: shiny::runApp(&quot;app_logistic_base.R&quot;) Or open app_logistic_base.R in RStudio and click Run App. App controls &amp; outputs Sliders: K (carrying capacity) – asymptotic upper bound A (initial position) – positions the curve at \\(t=0\\) relative to \\(K\\) r (growth rate) – controls how quickly the curve rises (sign permits decline if negative) Snap to Best Fit (nls) – sets K, A, r to the converged nls solution (if available) Equation &amp; RMSE (your curve) – updates as you move sliders Equation &amp; RMSE (best fit) – fixed reference from nls (or a message if nls failed to converge) Plot layers: Points → observed data Faint line → observed connection over time Solid line → your current logistic curve Dashed line → best-fit logistic curve (if nls converged) Under the hood (key functions) Data load: load_cod_data &lt;- function() { path &lt;- &quot;data/cod_timeseries.csv&quot; read_csv(path, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) } cod &lt;- load_cod_data() |&gt; mutate(t = Year - min(Year)) Model + RMSE: logistic_fun &lt;- function(t, K, A, r) K / (1 + A * exp(-r * t)) rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2, na.rm = TRUE)) Heuristic starts for nls: \\(K_{\\text{start}} = 1.2 \\times \\max(\\text{Pop})\\) \\(A_{\\text{start}} = K_{\\text{start}} / (P_0 + 10^{-6}) - 1\\), where \\(P_0\\) is Pop at \\(t=0\\) (fallback to min if missing) \\(r_{\\text{start}} = 0.1\\) Nonlinear fit (base R): fit_best &lt;- tryCatch( nls( Pop ~ K / (1 + A * exp(-r * t)), data = cod, start = list(K = K_start, A = A_start, r = r_start), control = nls.control(maxiter = 2000, warnOnly = TRUE) ), error = function(e) NULL ) Slider ranges (rough scale): \\(K \\in [0.5\\max(\\text{Pop}), 3\\max(\\text{Pop})]\\), \\(A \\in [10^{-3}, 20]\\), \\(r \\in [-1, 1]\\) Customization tips Axis/labels/theme: Edit the labs(...) and theme_*() calls inside the renderPlot section. Starting values: Adjust K_start, A_start, r_start heuristics to better match your dataset. Convergence behavior: Tweak nls.control(maxiter = 2000, warnOnly = TRUE) if fits are unstable. Slider steps &amp; ranges: Refine step and range calculations for smoother tuning. Troubleshooting File missing error (cod_timeseries.csv) Place your dataset at data/cod_timeseries.csv with columns Year,Pop. If you want the app to render without the file, wrap the load in if (file.exists(...)) and set a fallback tibble, or mark the example chunk with eval=FALSE in Bookdown. nls fails to converge Try “Snap to Best Fit” again after nudging sliders toward reasonable values. Adjust starting values (K_start, A_start, r_start) to be closer to the data. Narrow slider ranges if exploration goes into unrealistic regimes. Unexpected RMSE values Check data units, outliers, and missing values (RMSE uses na.rm = TRUE). Source Code # app_logistic_base.R # Interactive logistic fit (base R nls only) for cod population data # Sliders and outputs use 3 significant figures library(shiny) library(tidyverse) # --------------------------- # Data load (with safe fallback) # --------------------------- load_cod_data &lt;- function() { path &lt;- &quot;data/cod_timeseries.csv&quot; read_csv(path, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) } cod &lt;- load_cod_data() |&gt; mutate(t = Year - min(Year)) # --------------------------- # Helpers # --------------------------- rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2, na.rm = TRUE)) logistic_fun &lt;- function(t, K, A, r) K / (1 + A * exp(-r * t)) # Heuristic starting values for nls and slider defaults pop_max &lt;- max(cod$Pop, na.rm = TRUE) pop_min &lt;- min(cod$Pop, na.rm = TRUE) K_start &lt;- 1.2 * pop_max r_start &lt;- 0.1 P0 &lt;- cod$Pop[cod$t == 0][1] if (is.na(P0)) P0 &lt;- pop_min A_start &lt;- (K_start / (P0 + 1e-6)) - 1 if (!is.finite(A_start) || A_start &lt;= 0) A_start &lt;- 1 # Fit best logistic model (nls) fit_best &lt;- tryCatch( nls( Pop ~ K / (1 + A * exp(-r * t)), data = cod, start = list(K = K_start, A = A_start, r = r_start), control = nls.control(maxiter = 2000, warnOnly = TRUE) ), error = function(e) NULL ) has_best &lt;- !is.null(fit_best) best_par &lt;- if (has_best) as.list(coef(fit_best)) else list(K = K_start, A = A_start, r = r_start) best_rmse &lt;- if (has_best) rmse(cod$Pop, fitted(fit_best)) else NA_real_ # Slider ranges (use rough scale spacing) K_rng &lt;- c(0.5 * pop_max, 3 * pop_max) A_rng &lt;- c(1e-3, 20) r_rng &lt;- c(-1, 1) # --------------------------- # UI # --------------------------- ui &lt;- fluidPage( titlePanel(&quot;Interactive Logistic Fit (Base nls): Cod Population&quot;), sidebarLayout( sidebarPanel( helpText(HTML(&quot;Model: &lt;b&gt;Pop(t) = K / (1 + A e&lt;sup&gt;-r t&lt;/sup&gt;)&lt;/b&gt;, with t = Year - min(Year)&quot;)), sliderInput(&quot;K&quot;, &quot;K (carrying capacity):&quot;, min = signif(K_rng[1], 3), max = signif(K_rng[2], 3), value = signif(best_par$K, 3), step = signif(diff(K_rng)/200, 3)), sliderInput(&quot;A&quot;, &quot;A (initial position):&quot;, min = signif(A_rng[1], 3), max = signif(A_rng[2], 3), value = signif(best_par$A, 3), step = 0.01), sliderInput(&quot;r&quot;, &quot;r (growth rate):&quot;, min = signif(r_rng[1], 3), max = signif(r_rng[2], 3), value = signif(best_par$r, 3), step = 0.001), actionButton(&quot;snap&quot;, &quot;Snap to Best Fit (nls)&quot;), hr(), strong(&quot;Your logistic curve:&quot;), verbatimTextOutput(&quot;eqn_user&quot;, placeholder = TRUE), div(&quot;RMSE (your curve):&quot;), h3(textOutput(&quot;rmse_user&quot;), style = &quot;margin-top:-8px;&quot;), hr(), strong(&quot;Best fit (dashed line):&quot;), verbatimTextOutput(&quot;eqn_best&quot;, placeholder = TRUE), div(&quot;RMSE (best fit):&quot;), h4(textOutput(&quot;rmse_best&quot;), style = &quot;margin-top:-8px;&quot;) ), mainPanel( plotOutput(&quot;fitplot&quot;, height = 500), br(), tags$small(em( if (file.exists(&quot;data/cod_timeseries.csv&quot;)) { &quot;Loaded data/cod_timeseries.csv&quot; } else { &quot;No data/cod_timeseries.csv found — using synthetic demo data.&quot; } )) ) ) ) # --------------------------- # Server # --------------------------- server &lt;- function(input, output, session) { observeEvent(input$snap, { updateSliderInput(session, &quot;K&quot;, value = signif(best_par$K, 3)) updateSliderInput(session, &quot;A&quot;, value = signif(best_par$A, 3)) updateSliderInput(session, &quot;r&quot;, value = signif(best_par$r, 3)) }) preds_user &lt;- reactive({ tibble( Year = cod$Year, t = cod$t, Pred = logistic_fun(cod$t, input$K, input$A, input$r) ) }) output$eqn_user &lt;- renderText({ paste0(&quot;Pop(t) = &quot;, signif(input$K, 3), &quot; / (1 + &quot;, signif(input$A, 3), &quot; * exp(-&quot;, signif(input$r, 3), &quot; * t))&quot;) }) output$rmse_user &lt;- renderText({ sprintf(&quot;%.3f&quot;, rmse(cod$Pop, preds_user()$Pred)) }) output$eqn_best &lt;- renderText({ if (has_best) { paste0(&quot;Pop(t) = &quot;, signif(best_par$K, 3), &quot; / (1 + &quot;, signif(best_par$A, 3), &quot; * exp(-&quot;, signif(best_par$r, 3), &quot; * t))&quot;) } else { &quot;Best fit unavailable (nls did not converge). Try adjusting sliders.&quot; } }) output$rmse_best &lt;- renderText({ if (has_best) sprintf(&quot;%.3f&quot;, best_rmse) else &quot;—&quot; }) output$fitplot &lt;- renderPlot({ grid &lt;- tibble(Year = seq(min(cod$Year), max(cod$Year), length.out = 400)) |&gt; mutate(t = Year - min(cod$Year), Pred_user = logistic_fun(t, input$K, input$A, input$r), Pred_best = if (has_best) logistic_fun(t, best_par$K, best_par$A, best_par$r) else NA_real_) ggplot(cod, aes(Year, Pop)) + geom_point(size = 2, alpha = 0.9) + geom_line(alpha = 0.35) + geom_line(data = grid, aes(y = Pred_user), linewidth = 1, color = &quot;#0072B2&quot;) + { if (has_best) geom_line(data = grid, aes(y = Pred_best), linewidth = 1, linetype = &quot;dashed&quot;, color = &quot;grey40&quot;) } + labs( title = &quot;Cod Population with Interactive Logistic Fit (Base nls)&quot;, subtitle = paste0( &quot;K = &quot;, signif(input$K, 3), &quot;, A = &quot;, signif(input$A, 3), &quot;, r = &quot;, signif(input$r, 3) ), x = &quot;Year&quot;, y = &quot;Population (units)&quot; ) + theme_classic() }) } shinyApp(ui, server) D.5 Readme: Logistic Growth with Constant Harvesting &amp; Delayed Start — Cod Population (Shiny) This section introduces the Logistic Harvesting App designed to explore the Cod population dataset under a constant harvest rate with an optional delayed start to harvesting. The accompanying README and source code provide an overview of the app’s structure and highlight the key code components that power its functionality, including data loading, baseline logistic fitting (no harvest), dynamic simulation with harvesting, and visualization. An interactive Shiny app for exploring a logistic growth model with constant harvesting \\(H\\) and a start delay. The app: - Fits a baseline logistic curve with no harvest (via base R nls) to estimate \\(K, A, r\\). - Simulates population trajectories when a constant harvest \\(H\\) is applied starting at a user-chosen time. - Reports whether the chosen \\(H\\) is sustainable (i.e., \\(H \\le H_{\\text{MSY}} = rK/4\\)) and shows the corresponding equilibrium when it exists. All displayed values and sliders use 3 significant figures. What this app does Loads a cod population time series (Year, Pop) and creates a centered time variable \\(t = \\text{Year} - \\min(\\text{Year})\\) Fits a baseline logistic model (with \\(H=0\\)) to estimate parameters: \\[ \\text{Pop}(t) = \\frac{K}{1 + A\\,e^{-r t}} \\] Simulates a harvested logistic system using an explicit time-step scheme: \\[ \\frac{dP}{dt} = rP\\left(1 - \\frac{P}{K}\\right) - H,\\quad \\text{with harvest starting at } t \\ge H_{\\text{start}} \\] Lets users interactively adjust harvest rate \\(H\\), initial population \\(P_0\\), harvest start delay, and simulation length Displays the baseline logistic fit (dashed), the simulated trajectory (solid), and an equilibrium line if \\(H \\le H_{\\text{MSY}}\\) Why this is useful Builds intuition for logistic dynamics under constant harvesting, including the concept of Maximum Sustainable Yield \\(H_{\\text{MSY}} = rK/4\\) Demonstrates how start timing of harvest changes outcomes (e.g., temporary recovery vs. immediate pressure) Encourages exploration of parameter sensitivity and management trade-offs (e.g., higher \\(H\\) vs. sustainability) Requirements R (≥ 4.2 recommended) Packages: shiny, tidyverse (specifically readr, dplyr, ggplot2) Install once: install.packages(c(&quot;shiny&quot;, &quot;tidyverse&quot;)) File structure #your-project/ #├─ app.R #└─ data/ # └─ cod_timeseries.csv # Your dataset (optional; app falls back if missing) Expected CSV schema Required columns: Year, Pop Example (header + 5 rows): csv Year,Pop 1960,2.31 1961,2.45 1962,2.38 1963,2.52 1964,2.60 Note: The app internally creates \\(t = \\text{Year} - \\min(\\text{Year})\\). Years need not start at 0. How to run From the project directory: shiny::runApp(&quot;app_harvest_P0_delay.R&quot;) Or open app_harvest_P0_delay.R in RStudio and click Run App. App controls &amp; outputs Sliders: H (harvest rate) – constant removal rate (units/year), applied only when time ≥ Harvest begins after Harvest begins after (years) – delay before harvesting starts (applied on the time axis) P₀ (initial population) – starting population at \\(t=0\\) for the simulation Simulation horizon (years) – total years to simulate forward from the first observed year Fixed model parameters (from H=0 fit): K, r, A – parameters estimated by nls on the baseline logistic (no harvest) P₀ (baseline) – implied from \\(K\\) and \\(A\\) at \\(t=0\\), shown for reference H_MSY = rK/4 – displayed for quick comparison with the chosen \\(H\\) Outcome text: Indicates if \\(H \\le H_{\\text{MSY}}\\) (Sustainable) and shows the positive equilibrium value when it exists; otherwise warns of Unsustainable harvest (no positive equilibrium) Plot layers: - Points/line → observed data - Dashed line → baseline logistic fit (H = 0) - Solid line → simulated trajectory under chosen \\(H\\), \\(P_0\\), and delay - Vertical dashed red line → harvest start time - Dot-dashed horizontal line → equilibrium (if sustainable and after harvest begins) Under the hood (key functions) Data load: load_cod_data &lt;- function() { path &lt;- &quot;data/cod_timeseries.csv&quot; read_csv(path, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) } cod &lt;- load_cod_data() |&gt; mutate(t = Year - min(Year)) Model definitions &amp; helper: logistic_fun &lt;- function(t, K, A, r) K / (1 + A * exp(-r * t)) dP_dt &lt;- function(P, r, K, H) r * P * (1 - P / K) - H rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2, na.rm = TRUE)) Simulation with delayed harvest (explicit Euler): simulate_harvest &lt;- function(P0, r, K, H, H_start = 0, years = 40, dt = 0.1, year0 = 0) { n &lt;- max(2, ceiling(years / dt) + 1) t &lt;- seq(0, years, length.out = n) P &lt;- numeric(n) P[1] &lt;- max(0, P0) for (i in 1:(n - 1)) { H_eff &lt;- ifelse(t[i] &gt;= H_start, H, 0) P[i + 1] &lt;- max(0, P[i] + dt * dP_dt(P[i], r, K, H_eff)) } tibble(time = t, Year = year0 + t, Pop = P) } Baseline fit (H = 0) using base R nls: fit_logis &lt;- tryCatch( nls( Pop ~ K / (1 + A * exp(-r * t)), data = cod, start = list(K = K_start, A = A_start, r = r_start), control = nls.control(maxiter = 2000, warnOnly = TRUE) ), error = function(e) NULL ) Key derived quantities: Implied \\(P_0\\) from baseline fit: \\(P_{0,\\text{fit}} = \\dfrac{K}{1 + A}\\) Maximum Sustainable Yield: \\(H_{\\text{MSY}} = \\dfrac{rK}{4}\\) Positive equilibrium under constant \\(H\\) (when \\(H \\le rK/4\\)): \\[ P^\\* = \\frac{K}{2}\\left(1 + \\sqrt{1 - \\frac{4H}{rK}}\\right) \\] (the larger, stable equilibrium used in the plot overlay) Customization tips Time step &amp; stability: Adjust dt in simulate_harvest() (smaller dt → smoother/safer; larger dt → faster but may overshoot). Parameter ranges &amp; steps: Tune slider min/max/step to match your data’s scale and desired precision. Starting values for baseline fit: Modify K_start, A_start, r_start heuristics for more robust nls convergence. Appearance: Update labs(...), line types/colors, and theme_*() to match your visual style. Troubleshooting File missing error (cod_timeseries.csv) Place your dataset at data/cod_timeseries.csv with columns Year,Pop. If documenting in Bookdown (and not actually running code): mark example chunks with eval=FALSE. For renders that must continue even without data, wrap loads in if (file.exists(...)) or set chunk option error=TRUE. nls fails to converge (baseline logistic) Nudge slider guesses toward plausible values and hit Snap to Best Fit again. Adjust starting values; constrain ranges closer to observed \\(\\text{Pop}\\). Increase maxiter or provide better-informed initial guesses. Population collapses immediately under harvest Check if \\(H &gt; H_{\\text{MSY}}\\). If so, no positive equilibrium exists; consider lowering \\(H\\) or delaying start longer. Jittery simulation curve Decrease dt in simulate_harvest() (e.g., from 0.1 to 0.05) for a smoother trajectory. Source Code # app_harvest_P0_delay.R # Logistic growth with constant harvesting + delayed start (Adjust H, P₀, and start delay) # All displayed values use 3 significant figures library(shiny) library(tidyverse) # --------------------------- # Data load (with safe fallback) # --------------------------- load_cod_data &lt;- function() { path &lt;- &quot;data/cod_timeseries.csv&quot; read_csv(path, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) } cod &lt;- load_cod_data() |&gt; mutate(t = Year - min(Year)) # --------------------------- # Helpers # --------------------------- rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2, na.rm = TRUE)) logistic_fun &lt;- function(t, K, A, r) K / (1 + A * exp(-r * t)) dP_dt &lt;- function(P, r, K, H) r * P * (1 - P / K) - H # Delayed-harvest simulator: harvesting turns on when t &gt;= H_start simulate_harvest &lt;- function(P0, r, K, H, H_start = 0, years = 40, dt = 0.1, year0 = 0) { n &lt;- max(2, ceiling(years / dt) + 1) t &lt;- seq(0, years, length.out = n) P &lt;- numeric(n) P[1] &lt;- max(0, P0) for (i in 1:(n - 1)) { H_eff &lt;- ifelse(t[i] &gt;= H_start, H, 0) P[i + 1] &lt;- max(0, P[i] + dt * dP_dt(P[i], r, K, H_eff)) } tibble(time = t, Year = year0 + t, Pop = P) } # --------------------------- # Fit logistic with H = 0 # --------------------------- pop_max &lt;- max(cod$Pop, na.rm = TRUE) pop_min &lt;- min(cod$Pop, na.rm = TRUE) K_start &lt;- 1.2 * pop_max r_start &lt;- 0.1 P0_obs &lt;- cod$Pop[cod$t == 0][1] if (is.na(P0_obs)) P0_obs &lt;- pop_min A_start &lt;- (K_start / (P0_obs + 1e-6)) - 1 if (!is.finite(A_start) || A_start &lt;= 0) A_start &lt;- 1 fit_logis &lt;- tryCatch( nls( Pop ~ K / (1 + A * exp(-r * t)), data = cod, start = list(K = K_start, A = A_start, r = r_start), control = nls.control(maxiter = 2000, warnOnly = TRUE) ), error = function(e) NULL ) if (!is.null(fit_logis)) { pars &lt;- as.list(coef(fit_logis)) K_fix &lt;- pars$K A_fix &lt;- pars$A r_fix &lt;- pars$r P0_fix &lt;- K_fix / (1 + A_fix) rmse_best &lt;- rmse(cod$Pop, logistic_fun(cod$t, K_fix, A_fix, r_fix)) } else { K_fix &lt;- K_start; A_fix &lt;- A_start; r_fix &lt;- r_start P0_fix &lt;- K_fix / (1 + A_fix) rmse_best &lt;- NA_real_ } H_MSY_fix &lt;- r_fix * K_fix / 4 # --------------------------- # UI # --------------------------- ui &lt;- fluidPage( titlePanel(&quot;Logistic Growth with Constant Harvesting (Adjust H, P₀, and Harvest Start)&quot;), sidebarLayout( sidebarPanel( helpText(HTML( &quot;Baseline (H=0) fitted logistic model:&lt;br/&gt;&quot;, &quot;&lt;b&gt;Pop(t) = K / (1 + A e&lt;sup&gt;-r t&lt;/sup&gt;)&lt;/b&gt;&lt;br/&gt;&quot;, &quot;Adjust harvest rate &lt;b&gt;H&lt;/b&gt;, initial population &lt;b&gt;P&lt;sub&gt;0&lt;/sub&gt;&lt;/b&gt;, &quot;, &quot;and when harvesting begins.&quot; )), sliderInput( &quot;H&quot;, &quot;Harvest rate H (units/year):&quot;, min = 0, max = signif(1.5 * H_MSY_fix, 3), value = 0, step = signif((1.5 * H_MSY_fix) / 200, 3) ), sliderInput( &quot;H_start&quot;, &quot;Harvest begins after (years):&quot;, min = 0, max = 100, value = 0, step = 1 ), sliderInput( &quot;P0&quot;, &quot;Initial population P₀:&quot;, min = 0, max = signif(1.2 * K_fix, 3), value = signif(P0_fix, 3), step = signif((1.2 * K_fix) / 200, 3) ), sliderInput( &quot;years&quot;, &quot;Simulation horizon (years):&quot;, min = 5, max = 100, value = 40, step = 1 ), hr(), strong(&quot;Fixed model parameters (from H=0 fit):&quot;), tags$div(HTML( paste0( &quot;K = &quot;, signif(K_fix, 3), &quot;&lt;br/&gt;&quot;, &quot;r = &quot;, signif(r_fix, 3), &quot;&lt;br/&gt;&quot;, &quot;A = &quot;, signif(A_fix, 3), &quot; (implies baseline P&lt;sub&gt;0&lt;/sub&gt; = &quot;, signif(P0_fix, 3), &quot;)&lt;br/&gt;&quot;, &quot;H&lt;sub&gt;MSY&lt;/sub&gt; = &quot;, signif(H_MSY_fix, 3) ) )), hr(), strong(&quot;Outcome:&quot;), textOutput(&quot;outcome_text&quot;) ), mainPanel( plotOutput(&quot;simplot&quot;, height = 520), br(), tags$small(em( if (file.exists(&quot;data/cod_timeseries.csv&quot;)) { &quot;Loaded data/cod_timeseries.csv (points).&quot; } else { &quot;No data/cod_timeseries.csv found — using synthetic demo data (points).&quot; } )) ) ) ) # --------------------------- # Server # --------------------------- server &lt;- function(input, output, session) { msy &lt;- reactive({ r_fix * K_fix / 4 }) outcome_label &lt;- reactive({ H &lt;- input$H if (H &lt;= msy()) { Pstar &lt;- (K_fix / 2) * (1 + sqrt(1 - 4 * H / (r_fix * K_fix))) paste0(&quot;Sustainable (once harvesting starts): equilibrium ~ &quot;, signif(Pstar, 3)) } else { &quot;Unsustainable: no positive equilibrium (collapse likely once harvesting starts)&quot; } }) output$outcome_text &lt;- renderText(outcome_label()) sim &lt;- reactive({ simulate_harvest( P0 = input$P0, r = r_fix, K = K_fix, H = input$H, H_start = input$H_start, years = input$years, dt = 0.1, year0 = min(cod$Year) ) }) output$simplot &lt;- renderPlot({ df_sim &lt;- sim() year_min &lt;- min(cod$Year) year_max &lt;- year_min + input$years grid_fit &lt;- tibble( Year = seq(min(cod$Year), max(cod$Year), length.out = 400) ) |&gt; mutate(t = Year - min(cod$Year), Pop_fit = logistic_fun(t, K_fix, A_fix, r_fix)) # Sustainable equilibrium value (if any) for the chosen H eq_y &lt;- NA_real_ if (input$H &lt;= msy()) { eq_y &lt;- (K_fix / 2) * (1 + sqrt(1 - 4 * input$H / (r_fix * K_fix))) } # Build plot p &lt;- ggplot() + geom_point(data = cod, aes(Year, Pop), size = 2, alpha = 0.85) + geom_line(data = cod, aes(Year, Pop), alpha = 0.25) + geom_line(data = grid_fit, aes(Year, Pop_fit), linetype = &quot;dashed&quot;, color = &quot;grey40&quot;, linewidth = 1) + geom_line(data = df_sim, aes(Year, Pop), linewidth = 1.2, color = &quot;#0072B2&quot;) + # Harvest start marker geom_vline(xintercept = year_min + input$H_start, linetype = &quot;dashed&quot;, color = &quot;red&quot;, alpha = 0.6) + labs( title = &quot;Logistic Growth with Constant Harvesting (Delayed Start)&quot;, subtitle = paste0( &quot;r = &quot;, signif(r_fix, 3), &quot;, K = &quot;, signif(K_fix, 3), &quot; | H = &quot;, signif(input$H, 3), &quot;, P₀ = &quot;, signif(input$P0, 3), &quot; | H starts at t = &quot;, signif(input$H_start, 3), &quot; yr&quot;, &quot; | H_MSY = &quot;, signif(msy(), 3) ), x = &quot;Year&quot;, y = &quot;Population (units)&quot; ) + theme_classic() # Add equilibrium segment only after harvesting begins and only if sustainable if (is.finite(eq_y)) { x0 &lt;- year_min + input$H_start x1 &lt;- year_max if (x1 &gt; x0) { p &lt;- p + geom_segment(x = x0, xend = x1, y = eq_y, yend = eq_y, linetype = &quot;dotdash&quot;, color = &quot;grey30&quot;) } } p }) } shinyApp(ui, server) D.6 Readme: Logistic with Harvesting &amp; Stochastic Bad-Year Shocks — Cod Population (Shiny) This section introduces the Logistic Harvesting + Bad-Year App designed to explore the Cod population dataset under a constant harvest rate while incorporating stochastic “bad-year” shocks to the intrinsic growth rate \\(r\\). The accompanying README and source code provide an overview of the app’s structure and highlight the key code components that power its functionality, including data loading, baseline (no-harvest) logistic fitting, stochastic simulation with year-level shocks, and visualization (population + time-varying \\(r_{\\text{eff}}(t)\\)). An interactive Shiny app for exploring a logistic growth model with constant harvesting \\(H\\) and random bad-year shocks that reduce \\(r\\) in certain years. The app: - Fits a baseline logistic curve with no harvest (via base R nls) to estimate \\(K, A, r\\) and the implied \\(P_0\\). - Simulates forward with a constant harvest \\(H\\) while randomly designating “bad years” (probability \\(p\\)) in which \\(r\\) is scaled by bad_factor (e.g., 0.5 halves \\(r\\)). - Plots population and the effective growth rate \\(r_{\\text{eff}}(t)\\) in separate panels, and shades bad years for clarity. All displayed values and sliders use 3 significant figures. What this app does Loads a cod population time series (Year, Pop) and creates a centered time variable \\(t = \\text{Year} - \\min(\\text{Year})\\) Fits a baseline logistic model (with \\(H=0\\)) to estimate parameters: \\[ \\text{Pop}(t) = \\frac{K}{1 + A\\,e^{-r t}}, \\quad P_0 = \\frac{K}{1 + A} \\] Simulates a harvested logistic with year-level shocks to \\(r\\): \\[ \\frac{dP}{dt} = r_{\\text{eff}}(t)\\,P\\!\\left(1 - \\frac{P}{K}\\right) - H,\\quad r_{\\text{eff}}(t)= \\begin{cases} r \\cdot \\text{bad\\_factor}, &amp; \\text{in bad years}\\\\ r, &amp; \\text{otherwise} \\end{cases} \\] Keeps \\(P_0\\) fixed from the baseline fit (no slider for \\(P_0\\)) Displays two panels: Population (with observed data and baseline fit) and \\(r_{\\text{eff}}(t)\\) (with bad-year shading) Why this is useful Builds intuition for environmental variability and harvest management under shocks to population growth Illustrates how bad-year frequency and severity (via bad_factor) can reduce sustainable harvest and long-run abundance Encourages exploration of risk (e.g., MSY under baseline vs. “bad-year” conditions) Requirements R (≥ 4.2 recommended) Packages: shiny, tidyverse (specifically readr, dplyr, ggplot2) Install once: install.packages(c(&quot;shiny&quot;, &quot;tidyverse&quot;)) File structure #your-project/ #├─ app.R #└─ data/ # └─ cod_timeseries.csv # Your dataset (optional; app falls back if missing) Expected CSV schema Required columns: Year, Pop Example (header + 5 rows): csv Year,Pop 1960,2.31 1961,2.45 1962,2.38 1963,2.52 1964,2.60 Note: The app internally creates \\(t = \\text{Year} - \\min(\\text{Year})\\). Years need not start at 0. How to run From the project directory: shiny::runApp(&quot;app_logistic_harvest_badyears.R&quot;) Or open app_logistic_harvest_badyears.R in RStudio and click Run App. App controls &amp; outputs Sliders &amp; inputs: H (harvest rate) – constant removal rate (units/year) Simulation horizon (years) – total simulated length from the first observed year Bad-year probability \\(p\\) – chance that any given year is “bad” Bad-year multiplier on r (bad_factor) – scales \\(r\\) during bad years (e.g., 0.5 halves \\(r\\)) Random seed – ensures reproducibility of bad-year draws Resimulate – redraws with the same settings (and seed) Fixed parameters (from H = 0 fit): K, r, A, and implied \\(P_0 = K/(1 + A)\\) RMSE(H=0 fit) for the baseline logistic reference Population panel: Points/line → observed data Dashed line → baseline logistic fit (H = 0) Solid line → simulated population under chosen \\(H\\), \\(p\\), and bad_factor Light red shading → years drawn as “bad” \\(r_{\\text{eff}}(t)\\) panel: Step plot of the time-varying effective growth rate Dashed line at baseline \\(r\\), dotted line at \\(r \\cdot \\text{bad\\_factor}\\) Same bad-year shading for alignment with the population panel Under the hood (key functions) Data load &amp; baseline fit (H = 0): load_cod_data &lt;- function() { path &lt;- &quot;data/cod_timeseries.csv&quot; read_csv(path, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) } cod &lt;- load_cod_data() |&gt; mutate(t = Year - min(Year)) logistic_fun &lt;- function(t, K, A, r) K / (1 + A * exp(-r * t)) rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2, na.rm = TRUE)) # Heuristic starts, then nls to estimate K, A, r and implied P0 Bad-year stochastic simulator (explicit Euler): simulate_badyears &lt;- function(P0, r, K, H, years, dt, year0, by_p, bad_factor, seed = NULL, clip_nonneg = TRUE) { if (!is.null(seed)) set.seed(seed) n &lt;- max(2, ceiling(years / dt) + 1) t &lt;- seq(0, years, length.out = n) P &lt;- numeric(n); P[1] &lt;- max(0, P0) yrs_idx &lt;- floor(t) # year index from start (0,1,2,...) unique_years &lt;- unique(yrs_idx) bad_year_flags &lt;- rbinom(length(unique_years), 1, by_p) == 1 names(bad_year_flags) &lt;- as.character(unique_years) r_eff_vec &lt;- numeric(n) for (i in 1:(n - 1)) { yk &lt;- as.character(yrs_idx[i]) r_eff &lt;- if (bad_year_flags[yk]) r * bad_factor else r r_eff_vec[i] &lt;- r_eff dP &lt;- r_eff * P[i] * (1 - P[i] / K) - H P[i + 1] &lt;- if (clip_nonneg) max(0, P[i] + dt * dP) else P[i] + dt * dP } # last step r_eff yk_last &lt;- as.character(yrs_idx[n]) r_eff_vec[n] &lt;- if (bad_year_flags[yk_last]) r * bad_factor else r tibble( time = t, Year = year0 + t, Pop = P, r_eff = r_eff_vec, bad_year = bad_year_flags[as.character(floor(t))] ) } MSY intuition (not directly enforced): Baseline \\(H_{\\text{MSY}} = rK/4\\). Under bad years, the effective sustainable harvest is typically lower (e.g., \\(r \\cdot \\text{bad\\_factor}\\) suggests a reduced MSY for those years). Customization tips Shock granularity: Switch from yearly shocks (floor(t)) to seasonal or multi-year by changing how you map t to shock periods. Intensity &amp; probability: Adjust bad_factor bounds and by_p slider to match your case study. Numerics: Decrease dt for smoother/stabler integration; increase for speed (with caution). Visualization: Modify shading color/alpha, line types, and themes to your house style. Troubleshooting File missing error (cod_timeseries.csv) Place your dataset at data/cod_timeseries.csv with columns Year,Pop. If documenting in Bookdown (and not executing), mark example chunks with eval=FALSE. To build despite missing files, check existence and return a placeholder tibble, or set chunk option error=TRUE. nls fails to converge (baseline fit) Adjust initial guesses \\(K_{\\text{start}}, A_{\\text{start}}, r_{\\text{start}}\\) to match your data scale. Constrain ranges/clean outliers; increase maxiter. Population drops to zero quickly High \\(H\\), frequent/severe bad years (high by_p, low bad_factor), or both. Reduce \\(H\\) and/or bad-year severity/frequency. Reproducibility of shocks Set the Random seed control; click Resimulate to refresh with same settings. D.6.1 Source Code # app_logistic_harvest_badyears.R # Logistic growth with harvesting and stochastic &quot;bad-year&quot; shocks to r # - P0 is fixed from the H = 0 logistic fit (no slider) # - Second panel shows r_eff(t) over time # - All displayed values use 3 significant figures library(shiny) library(tidyverse) # --------------------------- # Data load (with safe fallback) # --------------------------- load_cod_data &lt;- function() { path &lt;- &quot;data/cod_timeseries.csv&quot; read_csv(path, show_col_types = FALSE) |&gt; select(Year, Pop) |&gt; arrange(Year) } cod &lt;- load_cod_data() |&gt; mutate(t = Year - min(Year)) # centered time for fitting # --------------------------- # Helpers # --------------------------- rmse &lt;- function(obs, pred) sqrt(mean((obs - pred)^2, na.rm = TRUE)) logistic_fun &lt;- function(t, K, A, r) K / (1 + A * exp(-r * t)) # Euler integration of dP/dt = r_eff(t)*P*(1 - P/K) - H # Returns full series including r_eff(t) and bad_year flag simulate_badyears &lt;- function(P0, r, K, H, years, dt, year0, by_p, bad_factor, seed = NULL, clip_nonneg = TRUE) { if (!is.null(seed)) set.seed(seed) n &lt;- max(2, ceiling(years / dt) + 1) t &lt;- seq(0, years, length.out = n) P &lt;- numeric(n) P[1] &lt;- max(0, P0) # Year indices (0,1,2,...) from start; draw bad years once per whole year yrs_idx &lt;- floor(t) unique_years &lt;- unique(yrs_idx) bad_year_flags &lt;- rbinom(length(unique_years), size = 1, prob = by_p) == 1 names(bad_year_flags) &lt;- as.character(unique_years) r_eff_vec &lt;- numeric(n) for (i in 1:(n - 1)) { yk &lt;- as.character(yrs_idx[i]) r_eff &lt;- if (!is.null(bad_year_flags[yk]) &amp;&amp; bad_year_flags[yk]) r * bad_factor else r r_eff_vec[i] &lt;- r_eff dP &lt;- r_eff * P[i] * (1 - P[i] / K) - H P[i + 1] &lt;- P[i] + dt * dP if (clip_nonneg) P[i + 1] &lt;- max(0, P[i + 1]) } # last step r_eff yk_last &lt;- as.character(yrs_idx[n]) r_eff_vec[n] &lt;- if (!is.null(bad_year_flags[yk_last]) &amp;&amp; bad_year_flags[yk_last]) r * bad_factor else r tibble( time = t, Year = year0 + t, Pop = P, r_eff = r_eff_vec, bad_year = bad_year_flags[as.character(floor(t))] ) } # --------------------------- # Fit baseline (H = 0) logistic for defaults (K, r, A, implied P0) # --------------------------- pop_max &lt;- max(cod$Pop, na.rm = TRUE) pop_min &lt;- min(cod$Pop, na.rm = TRUE) K_start &lt;- 1.2 * pop_max r_start &lt;- 0.1 P0_obs &lt;- cod$Pop[cod$t == 0][1] if (is.na(P0_obs)) P0_obs &lt;- pop_min A_start &lt;- (K_start / (P0_obs + 1e-6)) - 1 if (!is.finite(A_start) || A_start &lt;= 0) A_start &lt;- 1 fit_logis &lt;- tryCatch( nls( Pop ~ K / (1 + A * exp(-r * t)), data = cod, start = list(K = K_start, A = A_start, r = r_start), control = nls.control(maxiter = 2000, warnOnly = TRUE) ), error = function(e) NULL ) if (!is.null(fit_logis)) { pars &lt;- as.list(coef(fit_logis)) K_fix &lt;- pars$K A_fix &lt;- pars$A r_fix &lt;- pars$r P0_fix &lt;- K_fix / (1 + A_fix) # implied initial pop at t=0 (H=0 fit) rmse_best &lt;- rmse(cod$Pop, logistic_fun(cod$t, K_fix, A_fix, r_fix)) } else { K_fix &lt;- K_start; A_fix &lt;- A_start; r_fix &lt;- r_start P0_fix &lt;- K_fix / (1 + A_fix) rmse_best &lt;- NA_real_ } H_MSY_baseline &lt;- r_fix * K_fix / 4 # for slider scaling # --------------------------- # UI # --------------------------- ui &lt;- fluidPage( titlePanel(&quot;Logistic with Harvesting and Random Bad-Year Shocks to r (P₀ fixed)&quot;), sidebarLayout( sidebarPanel( helpText(HTML( &quot;Model: &lt;b&gt;dP/dt = r_eff(t) P (1 - P/K) - H&lt;/b&gt;&lt;br/&gt;&quot;, &quot;Baseline &lt;i&gt;r&lt;/i&gt;, &lt;i&gt;K&lt;/i&gt;, and &lt;i&gt;P&lt;sub&gt;0&lt;/sub&gt;&lt;/i&gt; from the H=0 logistic fit.&lt;br/&gt;&quot;, &quot;Each year is &#39;bad&#39; with probability &lt;i&gt;p&lt;/i&gt;; then &lt;i&gt;r&lt;/i&gt; is scaled by &lt;i&gt;bad_factor&lt;/i&gt;.&quot; )), sliderInput(&quot;H&quot;, &quot;Harvest rate H (units/year):&quot;, min = 0, max = signif(1.5 * H_MSY_baseline, 3), value = signif(0.5 * H_MSY_baseline, 3), step = signif((1.5 * H_MSY_baseline) / 200, 3)), sliderInput(&quot;years&quot;, &quot;Simulation horizon (years):&quot;, min = 5, max = 100, value = 40, step = 1), hr(), strong(&quot;Bad-year process on r:&quot;), sliderInput(&quot;by_p&quot;, &quot;Bad-year probability p (per year):&quot;, min = 0, max = 0.7, value = 0.25, step = 0.01), sliderInput(&quot;bad_factor&quot;, &quot;Bad-year multiplier on r (e.g., 0.5 halves r):&quot;, min = 0.05, max = 1.0, value = 0.5, step = 0.01), numericInput(&quot;seed&quot;, &quot;Random seed&quot;, value = 123, min = 1, step = 1), actionButton(&quot;resim&quot;, &quot;Resimulate&quot;), hr(), strong(&quot;Fixed parameters (from H = 0 fit):&quot;), tags$div(HTML( paste0( &quot;K = &quot;, signif(K_fix, 3), &quot;&lt;br/&gt;&quot;, &quot;r = &quot;, signif(r_fix, 3), &quot;&lt;br/&gt;&quot;, &quot;A = &quot;, signif(A_fix, 3), &quot; ⇒ P&lt;sub&gt;0&lt;/sub&gt; (fixed) = &quot;, signif(P0_fix, 3), &quot;&lt;br/&gt;&quot;, &quot;RMSE(H=0 fit) = &quot;, ifelse(is.na(rmse_best), &quot;—&quot;, sprintf(&#39;%.3f&#39;, rmse_best)) ) )) ), mainPanel( plotOutput(&quot;popplot&quot;, height = 420), plotOutput(&quot;rplot&quot;, height = 200), br(), tags$small(em( if (file.exists(&quot;data/cod_timeseries.csv&quot;)) { &quot;Loaded data/cod_timeseries.csv (points).&quot; } else { &quot;No data/cod_timeseries.csv found — using synthetic demo data.&quot; } )) ) ) ) # --------------------------- # Server # --------------------------- server &lt;- function(input, output, session) { # Resimulation trigger to redraw with same settings bump &lt;- reactiveVal(0) observeEvent(input$resim, { bump(isolate(bump()) + 1) }) # Simulate with bad-year shocks (P0 fixed) sim_df &lt;- reactive({ bump() simulate_badyears( P0 = P0_fix, # &lt;-- P0 fixed here r = r_fix, K = K_fix, H = input$H, years = input$years, dt = 0.1, year0 = min(cod$Year), by_p = input$by_p, bad_factor = input$bad_factor, seed = input$seed, clip_nonneg = TRUE ) }) output$popplot &lt;- renderPlot({ df_sim &lt;- sim_df() # Baseline (H=0) best-fit over observed years (for reference) grid_fit &lt;- tibble(Year = seq(min(cod$Year), max(cod$Year), length.out = 400)) |&gt; mutate(t = Year - min(cod$Year), Pop_fit = logistic_fun(t, K_fix, A_fix, r_fix)) # MSY numbers for subtitle Hmsy_base &lt;- r_fix * K_fix / 4 Hmsy_bad &lt;- input$bad_factor * r_fix * K_fix / 4 # Rectangles marking bad years in the simulation bad_years &lt;- df_sim |&gt; mutate(y0 = floor(time)) |&gt; group_by(y0) |&gt; summarise(is_bad = any(bad_year), .groups = &quot;drop&quot;) |&gt; filter(is_bad) |&gt; mutate(xmin = min(cod$Year) + y0, xmax = xmin + 1) ggplot() + { if (nrow(bad_years) &gt; 0) geom_rect(data = bad_years, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), fill = &quot;red&quot;, alpha = 0.08) } + geom_point(data = cod, aes(Year, Pop), size = 2, alpha = 0.85) + geom_line(data = cod, aes(Year, Pop), alpha = 0.25) + geom_line(data = grid_fit, aes(Year, Pop_fit), linetype = &quot;dashed&quot;, color = &quot;grey40&quot;, linewidth = 1) + geom_line(data = df_sim, aes(Year, Pop), linewidth = 1.2, color = &quot;#0072B2&quot;) + labs( title = &quot;Logistic Growth with Harvesting and Bad-Year Shocks to r (P₀ fixed)&quot;, subtitle = paste0( &quot;H = &quot;, signif(input$H, 3), &quot; | MSY baseline = &quot;, signif(Hmsy_base, 3), &quot;, MSY in bad year = &quot;, signif(Hmsy_bad, 3), &quot; | p(bad year) = &quot;, signif(input$by_p, 3), &quot;, factor = &quot;, signif(input$bad_factor, 3) ), x = &quot;Year&quot;, y = &quot;Population (units)&quot; ) + theme_classic() }) output$rplot &lt;- renderPlot({ df_sim &lt;- sim_df() # Build yearly shading again for alignment bad_years &lt;- df_sim |&gt; mutate(y0 = floor(time)) |&gt; group_by(y0) |&gt; summarise(is_bad = any(bad_year), .groups = &quot;drop&quot;) |&gt; filter(is_bad) |&gt; mutate(xmin = min(cod$Year) + y0, xmax = xmin + 1) ggplot() + { if (nrow(bad_years) &gt; 0) geom_rect(data = bad_years, aes(xmin = xmin, xmax = xmax, ymin = -Inf, ymax = Inf), fill = &quot;red&quot;, alpha = 0.08) } + geom_step(data = df_sim, aes(Year, r_eff), linewidth = 1) + geom_hline(yintercept = r_fix, linetype = &quot;dashed&quot;, color = &quot;grey40&quot;) + geom_hline(yintercept = r_fix * input$bad_factor, linetype = &quot;dotted&quot;, color = &quot;grey50&quot;) + labs( title = &quot;Effective Growth Rate r_eff(t) (bad years scale r)&quot;, subtitle = paste0( &quot;Baseline r = &quot;, signif(r_fix, 3), &quot; | Bad-year r = &quot;, signif(r_fix * input$bad_factor, 3) ), x = &quot;Year&quot;, y = &quot;r_eff(t)&quot; ) + theme_classic() }) } shinyApp(ui, server) D.7 Understanding the Central Limit Theorem D.7.1 Why We Care When we collect data from nature, every measurement comes with randomness — rainfall changes day to day, fish populations fluctuate, and even repeated measurements of the same thing vary slightly. Yet when we look at averages of these random events, a remarkable pattern emerges: those averages tend to form a bell-shaped (normal) distribution. This is the essence of the Central Limit Theorem (CLT) — one of the most important results in statistics. D.7.2 Start with a Simple Example In the coin-flip app, you flipped coins and recorded the proportion of Heads in each experiment. Each individual flip is unpredictable — 50% chance of Heads, 50% chance of Tails. But when you flip many coins at once and record the proportion of Heads, those proportions cluster near 0.5. When you repeat that whole experiment many times, and make a histogram of those proportions, the shape looks surprisingly smooth and bell-like. That bell shape isn’t built into the coin — it comes from the averaging process itself. D.7.3 The Central Limit Theorem (Plain-Language Version) When we take many random samples from any population and compute the mean (or proportion) for each sample, the distribution of those sample means will approximate a normal distribution, as long as our samples are large enough. In symbols: \\[ \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i \\quad \\text{is approximately } N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] where: \\(\\mu\\) = population mean \\(\\sigma^2\\) = population variance \\(n\\) = sample size D.7.4 Why It Happens Each sample mean is like an average of many small random influences: Some pushes are high, some are low. When you add them up, extremes cancel out. What remains is the overall trend — most results fall near the middle. This “balancing of randomness” is what makes the sampling distribution of averages look normal, even when the original data aren’t. D.7.5 What You Saw in the App Sample Size (n) Shape of Sampling Distribution 1 Still two spikes: Heads vs. Tails 5 Somewhat bumpy, but already more centered 30 Nearly a perfect bell curve Notice two things: The mean stays around 0.5, no matter how many coins you flip. The spread gets smaller as \\(n\\) increases — large samples give more stable averages. That shrinking spread follows the rule: \\[ \\text{Spread of sample mean} = \\frac{\\text{Population spread}}{\\sqrt{n}} \\] so each time you multiply \\(n\\) by 4, the variation in your averages is cut in half. D.7.6 Real-World Meaning The CLT explains why we can trust statistics based on averages — like: Mean temperature across many days Average fish biomass in repeated surveys Mean exam score across students Even if individual observations are noisy, averages behave predictably. That’s why the normal distribution shows up everywhere in science: height, rainfall, errors, growth rates, and countless other measurements are all the result of many small, random effects added together. D.7.7 Key Takeaways Individual outcomes can be messy; averages are stable. The CLT connects randomness to predictability. As sample size increases: The distribution of sample means becomes more normal. The spread of sample means becomes narrower. This is why the normal curve is the foundation of statistical inference. D.7.8 Quick Reflection Try answering these after exploring the app: What changed when you increased the sample size in your simulation? Why does the histogram of sample proportions become smoother as \\(n\\) grows? Can you think of an environmental process where we rely on averages to make predictions? D.7.9  In a Sentence The Central Limit Theorem is the reason that when we collect enough data — even from a chaotic world — the averages we compute begin to look beautifully predictable. "],["prob-workbook.html", "Appendix E Prob Workbook E.1 Learning Goals E.2 Run the Tree Height Demo App E.3 Lab 3: Exploring NOAA Climate Data E.4 Lab 4: ️ Guided Activity: Exploring Rainfall Variability and Simulation E.5 Lab 5 - Coupling Models E.6 In-Class Activity — Markov Chains and Daily Energy Levels E.7 Lab 6 — Modeling Mosquito Population Dynamics with Markov Chains E.8 Lab 7 - Forest Disturbance Dynamics and Monte Carlo Simulation", " Appendix E Prob Workbook E.1 Learning Goals By the end of this in-class session, you will be able to: Interpret and compare PDFs and CDFs for environmental variables. Choose reasonable probability models (Normal, Lognormal, Gamma, Exponential, Weibull, Poisson, Gumbel) for a given process. Estimate simple distribution parameters from data (quick method-of-moments). E.2 Run the Tree Height Demo App Background The data in this file is from a large sampling of an established forest. 500 tree heights were surveyed. In the overview tab you are given some information on the data. In your own words describe what the following mean to you mean standard deviation (variance) mode (can you determine where it is?) median (can you determine where it is?) Class Discussion How is the kind of data used? What kinds of insights can we extract? How could we use this in modeling? PDFs the math kind Observe the bar graph of probabilities. How does it compare to the histogram on the previous tab? What’s the same / different? There are two fitted curves (distributions) to the data, one is normal the other empirical - what do you think the difference is? Which of the fitted distributions do you think is right? Set the lower and upper bound to 24 What probability do you get for the empirical and theoretical distributions? Set the lower bound to 17 and upper bound to 23 What probability do you get for the empirical and theoretical distributions? Can you explain how the probability is calculated? CDFs Cumulative distributions functions are the accumulation of the pdf curve. Or in mathematical language, the integration of the PDF. Lets see what you remember from calc The point of inflection on the CDF correspons to what on the pdf (its derivative) Examine the slopes of the CDF, describe what is happening from 10 to 30 (in everyday words) Now repeat what we did earlier, set the lower and upper bound to 24 What probability do you get for the empirical and theoretical distributions? Set the lower bound to 17 and upper bound to 23 What probability do you get for the empirical and theoretical distributions? Can you explain how the probability is calculated? Sampling from a CDF How Sampling from a CDF Works The Cumulative Distribution Function (CDF) tells us, for any value \\(x\\), the probability that a random variable is less than or equal to that value — \\[ F(x) = P(X \\le x) \\] If we reverse this process, we can generate new samples that follow the same distribution as the data. Here’s the logic: Draw a random number \\(u\\) between 0 and 1 — this represents a cumulative probability. Find the value \\(x\\) where \\(F(x) = u\\). That value \\(x\\) becomes one of your simulated samples. In other words, each \\(u\\) tells you how far up the CDF to go, and you then read off the corresponding \\(x\\)-value on the horizontal axis. Doing this repeatedly produces data that follow the same shape and probabilities as your original dataset. Try It Yourself Click “Draw 1 sample” to see how a random \\(u\\) maps to a tree height \\(x\\) on the CDF. Draw 10 samples at a time to see how your histogram begins to approximate the original data. Watch the Running KDE (samples) curve below — does it begin to match the Empirical KDE (full data) or the Normal(μ,σ) fit? Reflect: Why does the sample-based distribution become smoother and more similar as the number of samples increases? (Hint: the Law of Large Numbers.) Class discussion What does this kind of statistics / probability allows us to do? E.3 Lab 3: Exploring NOAA Climate Data In this activity, you will work with real-world climate data from the National Centers for Environmental Information (NCEI). Step 1: Download Data Go to the NOAA Climate Data Online (CDO). Under “Data Tools”, select “Search Tool”. Run a search with the search term Seattle. Select a dataset such as Daily Summaries (GHCND). Choose a date range (for example, the past 10 years) and download the data as a .csv file (I’ve donwloaded a file for you on CANVAS) Once you get to the map view, find a station and click the View Full Details link. Pay special attention to ‘data coverage’ - this lets you know how much missing data there is in the time period Step 2: Read the Documentation Before analyzing the data, read the GHCND (Global Historical Climatology Network – Daily) documentation: GHCND Documentation (NOAA) This file describes: The meaning of each column Units Flags that indicate data quality or measurement conditions What units are precipitation and temperature in? Step 3: Raw data exploration - Daily Save the sea_tac.R file to your Models folder Be sure the place the dataset seatac_data.csv in a folder titled data (which should be inside your Models folder) In the Daily Time Series Tab Any outliers? are they real? or are they errors? Do the values make sense? Observe any trends Zoom in on a period of 5 years, what do you notice about daily rainfall? Step 4: Raw data exploration - Annual Hit the Update Annual button What do you notice about the means? Would you say rainfall is stationary? Now hit the Update Distribution button Toggle on the Exclude zero-rain days (hit update) Why do you this we exclude the zeros? What kind of distribution is this? Comment on the tail of this distribution Toggle on the Overlay Period B button Make period B run from 2015 to present Discuss what you observe Step 5: Raw data exploration - Monthly Repeat what you did for the Annual but now in the Monthly Tab Which months are interesting in terms of how they are changing Describe their change in terms of the distribution shifts Step 6: Conceptual Modelling of Rainfall Go back to the Daily Time Series Tab Set the dates from 2024-10-15 to 2025-10-15 In a small group, think about how you would create an synthetic timeseries of rainfall. How could you use the dataset we downloaded? What statistics could we utilize? Any ideas on how we could use probabilities to build a timeseries? E.4 Lab 4: ️ Guided Activity: Exploring Rainfall Variability and Simulation Goal: Use the Sea-Tac Rainfall Explorer app to explore precipitation patterns, fit statistical distributions, and simulate realistic rainfall sequences. Over several short sessions, you will move from describing data → fitting models → testing simulations → reflecting on model strengths and weaknesses. E.4.1 Exploring the Data 1. Start the App Open the Shiny app Sea-Tac Rainfall Explorer. You’ll begin in the Annual Data → Daily precipitation tab. 2. Observe the Daily Record Zoom in and out on the time series to examine wet and dry periods. Note any long dry spells, clustering of wet days, or long-term trends. Prompt 1: What is the longest consecutive dry period you can find? How does it compare to the longest wet period? Prompt 2: Identify one or more years that appear unusually dry or wet. What real-world factors (ENSO, climate trends, etc.) might explain these anomalies? 3. Examine Annual Totals Navigate to Annual Data → Annual totals. The blue bars show annual precipitation; the black line shows a 7-year moving average. Prompt 3: Describe at least two multi-year “wet” and “dry” spells. Estimate their duration in years. Prompt 4: How can you use the moving average to find these ‘spells’? Fitting Statistical Distributions 1. Explore Annual Distribution Fits Open Annual Data → Distribution of annual totals. Select all the distribution fits. Observe how each distribution curve matches (or misses) the histogram. Prompt 5: Which distribution best captures the shape of the annual rainfall data? What feature (e.g., skewness, tail behavior) informs your choice? Prompt 6: Inspect the Fit parameters panel. How do the estimated mean and standard deviation compare across models? 2. Interpret the CDF Look at the CDF plot (right side). Notice how the 10% and 90% lines highlight extreme dry or wet years. Prompt 7: For the each fit, estimate the rainfall amount that marks the 10th and 90th percentile. What do these regions represent? 3. Move to Daily Distributions Explore the Daily Data tabs: Volume: rainfall on wet days Duration: consecutive wet days Interstorm: consecutive dry days Prompt 8: Compare the typical shapes of these three distributions. Which one is most skewed? What physical processes might produce that shape? Put your intuition to work - do these make sense? Simulation and Model Evaluation 1. Simulate a Single Year Go to Simulation → Procedure. Click Sample Step several times and observe how the model builds a time series: First sampling dry days (interstorm), Then wet-spell duration, Then rain volumes. Prompt 9: How realistic does the simulated pattern look compared to the actual daily record? Are wet and dry spells too frequent, too long, or about right? Prompt 10: After clicking Complete 1-Year Simulation, check the summary panel. How does the total simulated rainfall compare to the historical mean? Reset and run again - do things stay the same? are the different? 2. Compare Distributions Scroll to Sampled vs. Historical Distributions. Look at how the simulated (red line) distributions compare with historical histograms. Prompt 11: Which distribution (volume, duration, interstorm) is reproduced most accurately? Which differs most from the historical shape? Reset and run again, do things change? 3. Simulate Multiple Years Move to Simulation → Bulk sim. Choose a 3-year window of historical data. Click Simulate 3 years. Compare the Historical and Simulated plots. Prompt 12: Does the simulation reproduce realistic sequences of wet and dry spells at this multi-year scale? What differences stand out? 4. Add Seasonality Now open Simulation → Bulk Month sim. This simulation varies the distribution parameters by month, capturing seasonal changes in rainfall characteristics. Prompt 13: Compare the Bulk sim and Bulk Month sim results. What visual or statistical differences do you notice? Prompt 14: Which months contribute most to the total rainfall in the simulation? Is that consistent with the historical record? 5. Think Critically — Model Limitations and Extensions Prompt 15: What are the key assumptions behind this rainfall generator? Prompt 16: List two aspects of real rainfall the model ignores. How might you modify the model to include them? Prompt 17: If you were to apply this model to a different station (e.g., Phoenix or Miami), what challenges would you expect? Optional Extensions (for advanced exploration) Parameter Sensitivity: Adjust the Gamma parameters manually (in R) to see how changing shape/rate alters rainfall variability. Spatial Extension: Imagine repeating this analysis for multiple stations across the Pacific Northwest. How might you assess regional coherence in rainfall trends? Climate Change Scenario: Suppose the mean annual precipitation increases by 10%. How would that shift the fitted distributions and simulated outcomes? Wrap-Up Reflection Write a short (1-page) reflection answering: Which part of the rainfall record is most predictable, and which is most stochastic? What does this modeling exercise reveal about the relationship between data and simulation? How could similar probabilistic modeling approaches help in environmental forecasting (e.g., streamflow, drought risk)? Submission: Upload your responses (prompts + reflection) to Canvas. E.5 Lab 5 - Coupling Models In this lab, you’ll explore how forest biomass responds to varying rainfall conditions using a logistic growth model with rainfall-dependent growth rate \\(r_t\\). You’ll use an interactive Shiny app to: Visualize the logistic growth model and its differential equation. Examine rainfall distributions and how they are modeled statistically. Simulate forest growth under different rainfall scenarios — historical, synthetic, and climate-altered. Explore how drought intensity and mortality influence long-term forest dynamics. Background: Logistic Growth and Rainfall Dependence The logistic model describes how biomass or population grows over time under resource limitations: \\[ B(t) = \\frac{K}{1 + \\left(\\frac{K - B_0}{B_0}\\right)e^{-r_t t}} \\] and it’s derivative: \\[ \\frac{dB}{dt} = r_t B \\left(1 - \\frac{B}{K}\\right) \\] Here: \\(B(t)\\): biomass (or population size) at time \\(t\\) \\(r_t\\): intrinsic growth rate, possibly varying with rainfall \\(K\\): carrying capacity of the forest \\(B_0\\): initial biomass In this lab we are going to relax the assumption that \\(r_t\\) is constant and allow it to change each year depending on rainfall \\(R_t\\). Conceptual Exploration: Before Using the App Before interacting with the model, take time to reason through the following questions. Task 1 Situation: You planted a forest on the weekend, you want to model its growth over time. Discuss in pairs or small groups, then record your individual thoughts. 1. How might rainfall influence forest growth? Which aspects of rainfall are most important? (e.g., total annual amount, intensity, timing, variability) Is the relationship likely to be linear, or are there thresholds or saturation points? 2. What timescales of rainfall change matter most? Daily variation? Seasonal cycles? Multi-year droughts? How quickly could a forest respond to a change in rainfall? 3. How could you simplify the problem to start? What assumptions might you make to build a minimal model? How could rainfall data be summarized or reduced to one or two key parameters? Write down one simple conceptual model of your own that links rainfall to forest biomass growth. Use words, a short equation, or a flow diagram. Be prepared to compare your model with the one implemented in the Shiny app. Part 1 — Exploring the Logistic Model Open the “Logistic Growth” tab in the Shiny app. Use the sliders to vary: Growth rate \\(r\\) Carrying capacity \\(K\\) Initial biomass \\(B_0\\) Observe how each parameter affects the curve of \\(B(t)\\). This is just to refamiliarize yourself with how the paramters impact your model. Part 2 — Understanding Rainfall Distributions Navigate to the “Distributions” tab. Examine the fitted Gamma distributions for: Wet-day rainfall volume Wet-spell duration Interstorm duration Note how these variables differ in their means and spread. Task 2 Why is the Gamma distribution more appropriate than a Normal distribution for rainfall-related variables? What might a longer “right tail” in the interstorm duration distribution imply about drought frequency? Do you think these annual distributions help answer the questions you are asking abouut the growth of your forest? Part 3 — Linking Rainfall to Growth In this lab, \\(r_t\\) changes each year depending on rainfall \\(R_t\\), relative to a fixed historical mean \\(\\overline{R}\\): \\[ r_t = r_0 \\min\\!\\big(1 + \\max(\\tfrac{R_t}{\\overline{R}}-1, 0), 1.2\\big) - m \\max\\!\\big(1 - \\tfrac{R_t}{\\overline{R}}, 0\\big) \\] where \\(m\\) controls how strongly drought reduces the growth rate. Task 3 Explain how this equation works. How does it augment \\(r_t\\). Can you reason your way to why each component is included? Examine the limits of \\(r_t\\) Step A: No Rainfall Dependence View the “No Rainfall Dependence” panel. This represents constant \\(r = r_0\\), independent of rainfall. Step B: Historical Rainfall Switch to the “Historical Rainfall” panel. Here, \\(r_t\\) changes annually with observed rainfall. Task 4 Adjust the mortality parameter (m) and note how it affects long-term biomass. Comment on what you see - link it to your understanding of the equation used to model \\(r_t\\). Step C: Synthetic Rainfall Try the “Simulated Annual Rainfall” panel. This version generates rainfall sequences from fitted Gamma distributions. Task 5 When mortality \\(m\\) increases, how does the equilibrium biomass change? - How do wet years versus dry years appear in the growth trajectory? Why might it be useful to compare results using a fixed historical mean \\(\\overline{R}\\)? Part 4 — Exploring Climate Scenarios Use the next two tabs to simulate rainfall under modified climate conditions. Tab 4: Adjust the mean and variance of annual rainfall. Tab 5: Simulate daily rainfall from the fitted Gamma distributions. Task 6 Compare biomass trajectories between these different rainfall regimes. What happens to average biomass when mean rainfall decreases by 10%? How does increased rainfall variability (higher SD) affect stability? Which approach—annual or daily rainfall modeling—produces more realistic dynamics? Part 5 — Ensemble Simulations Use Tab 6 to run multiple stochastic simulations. Click “Add 1 Simulation” or “Add 10 Simulations” to build an ensemble of runs. Each run samples new rainfall sequences. Use the Year X slider to explore the distribution of biomass at that year. The histogram shows biomass across simulations. The dashed line shows a fitted normal distribution. Use “Reset Plot” to clear the ensemble and try new parameter settings. Start with this model setup up m=0 all rainfall characteristics at 0% change run 50 sims move the slider to examine the distribution This represents the forest’s growth with no dependency on rainfall and no changes to rainfall charateristics Task 7 Set the parameters and tell me a modeling story Use the ‘base’ condition to compare your change scenario to What are the implications to the your forest? Part 6 — Reflection Task 8 The reality of future change in the PNW is increased large storm events and longer interstorms - with little to no impact to annual rainfall. This model uses a historical annual mean \\(\\overline{R}\\) to drive the drought stress. Why won’t this show much change given the reality of PNWs future change? How could you adjust (don’t code it) this dependence into the model, what might you change? Learning Outcomes By the end of this lab, you should be able to: Explain how the logistic growth model represents biomass dynamics. Describe how rainfall variability affects intrinsic growth rate \\(r_t\\). Use simulation to evaluate resilience and collapse under drought stress. Connect conceptual modeling with data-driven exploration in a Shiny environment. E.6 In-Class Activity — Markov Chains and Daily Energy Levels In this activity, you’ll explore how a Markov chain models changes between different “states” of energy and focus throughout a typical day. You’ll experience the concept firsthand by simulating transitions between Tired, Awake, Focused, Distracted, and Exhausted states — just like how your real energy levels fluctuate during a long day of classes. Background A Markov chain represents a system that moves between discrete states with certain probabilities. At any time \\(t\\), the system is in one state, and at time \\(t + 1\\), it transitions to another (or stays the same) based on a fixed transition matrix. \\[ \\mathbf{x}_{t+1} = \\mathbf{x}_t P \\] Each row of the matrix represents the current state, and each column the next one. The States State Description Emoji 1 Tired — not awake yet, regretting that all-nighter  2 Awake — caffeine kicking in, alert but not yet productive  3 Focused — fully engaged and working effectively  4 Distracted — scrolling, chatting, or daydreaming  5 Exhausted — mentally checked out; can’t continue  The Base Transition Matrix Below is a sample transition matrix representing one hour in the life of a student: From  To Tired Awake Focused Distracted Exhausted Tired 0.2 0.7 0.0 0.0 0.1 Awake 0.0 0.3 0.5 0.1 0.1 Focused 0.0 0.1 0.6 0.2 0.1 Distracted 0.1 0.2 0.2 0.3 0.2 Exhausted 0.5 0.0 0.0 0.0 0.5 Each row sums to 1. The values represent the probability of moving from one state to another in a single time step (one “hour” in this model). Step 1 — Experience the Model Each student starts as Tired. At each simulated hour, roll a 10-sided die or use a random-number generator. Use the current row of the matrix to determine your new state. Example: A roll of 1-3 means you stay Tired (30% chance). A roll of 4-6 means you go from Tired → Awake (60% chance). A roll of 10 means you become Exhausted (10% chance). What state are you now in? Record that state, move to that state’s row and repeat for the next hour Repeat for 100 hours In the googlesheet record the number of hours you spent in each state Step 2 — Visualize the System Task 1 Which states dominate after several rounds? Does the class settle into a stable distribution (a “steady energy mix”)? What state seems “absorbing” — once you’re there, you rarely leave? Step 3 — Personality Profiles Different people move through their day with different energy dynamics. Below are three alternate transition matrices, each representing a “personality type.”  Over-Caffeinated Optimist From  To Tired Awake Focused Distracted Exhausted Tired 0.1 0.8 0.1 0.0 0.0 Awake 0.0 0.2 0.6 0.1 0.1 Focused 0.0 0.1 0.6 0.2 0.1 Distracted 0.0 0.1 0.3 0.4 0.2 Exhausted 0.5 0.1 0.0 0.0 0.4  Steady Strategist From  To Tired Awake Focused Distracted Exhausted Tired 0.30 0.60 0.00 0.00 0.10 Awake 0.00 0.50 0.30 0.10 0.10 Focused 0.00 0.10 0.70 0.10 0.10 Distracted 0.10 0.30 0.20 0.30 0.10 Exhausted 0.30 0.20 0.00 0.00 0.50  Procrastinating Dreamer From  To Tired Awake Focused Distracted Exhausted Tired 0.40 0.40 0.00 0.10 0.10 Awake 0.10 0.30 0.10 0.40 0.10 Focused 0.00 0.10 0.40 0.30 0.20 Distracted 0.10 0.10 0.10 0.50 0.20 Exhausted 0.50 0.00 0.00 0.00 0.50 Step 4 — Group Simulation Divide into groups Group 1 = Over-Caffeinated Optimists Group 2 = Steady Strategists Group 3 = Procrastinating Dreamers Run your simulations for 20 hours each recording how many hours your spend in each state. Task 3 Which group had the most “focused” time? With the data you now have - can you explain why your distribution of states looks the way it does? Step 5 — Reflect and Connect Task 4 What does the stationary distribution represent in this context? How can personality differences (modeled as probabilities) explain variation in real-world behavior? How is this similar to populations in nature, where different species or individuals have distinct life-history strategies? How could you extend this to an environmental problem? Learning Outcomes By the end of this activity, you should be able to: Explain how a Markov chain models transitions between behavioral or energetic states. Interpret how transition probabilities affect system stability and outcomes. Recognize that different “personalities” (or ecological strategies) can be represented by different transition matrices. Relate the stationary distribution to long-term patterns in both human and ecological systems. E.7 Lab 6 — Modeling Mosquito Population Dynamics with Markov Chains In this lab, you’ll explore how the life cycle of a mosquito can be represented using a Markov chain model — a framework that describes how populations transition between discrete life stages with certain probabilities. You’ll use an interactive Shiny app to: Examine how the transition matrix encodes daily probabilities of survival and development. Simulate changes in mosquito populations across egg, larva, pupa, and adult stages. Explore how mortality and reproduction rates affect population stability and persistence. Investigate how stochastic (random) variation influences population outcomes. Background: Markov Chains and Life Cycles Many organisms pass through a series of life stages, each with characteristic survival and transition probabilities. A Markov chain models these transitions as probabilistic changes from one stage to another. Let \\[ \\mathbf{x}_t = [x_{E,t}, x_{L,t}, x_{P,t}, x_{A,t}, x_{D,t}] \\] represent the number of individuals in each stage — Egg, Larva, Pupa, Adult, Dead — at time \\(t\\). Then, the population evolves according to: \\[ \\mathbf{x}_{t+1} = \\mathbf{x}_t P \\] where \\(P\\) is the transition matrix: \\[ P = \\begin{bmatrix} p_{EE} &amp; p_{EL} &amp; p_{EP} &amp; p_{EA} &amp; p_{ED} \\\\ p_{LE} &amp; p_{LL} &amp; p_{LP} &amp; p_{LA} &amp; p_{LD} \\\\ p_{PE} &amp; p_{PL} &amp; p_{PP} &amp; p_{PA} &amp; p_{PD} \\\\ p_{AE} &amp; p_{AL} &amp; p_{AP} &amp; p_{AA} &amp; p_{AD} \\\\ p_{DE} &amp; p_{DL} &amp; p_{DP} &amp; p_{DA} &amp; p_{DD} \\end{bmatrix} \\] Each row represents the current stage, and each column represents the next stage on the following day. For example, \\(p_{LP}\\) is the probability that a larva becomes a pupa in one day. Life Stages in This Model State Description Typical Duration Color 1 Egg 1–3 days Light blue 2 Larva 5–10 days Orange 3 Pupa 1–2 days Red 4 Adult 1–3 weeks Dark red 5 Dead Absorbing state Gray The Dead state is absorbing — once entered, individuals remain there. The Adult stage contributes new eggs to the population through the reproduction rate parameter. Conceptual Exploration: Before Using the App Task 1 Reflect and discuss: What biological processes determine how mosquitoes move between life stages? Why might daily transition probabilities differ between stages (e.g., larval crowding, temperature effects)? Why do we include a “Dead” state in the model — and what does it mean mathematically for it to be absorbing? How does reproduction act as an external input rather than a transition? Part 1 — Exploring the Transition Matrix Open the “Overview” tab and read through the background. Each row of the transition matrix must sum to 1. The probabilities represent development, survival, and mortality. In the left panel, click on the table cells to edit probabilities (in 0.05 increments). Rows that do not sum to 1 will show a warning message. Task 2 What does a larger diagonal entry \\(p_{ii}\\) imply about a stage’s stability (e.g., larvae remaining larvae)? How does increasing \\(p_{12}\\) (Egg → Larva) change the speed of development? What happens if mortality (probability to Dead) increases in any row? Part 2 — Running the Simulation Keep the initial population at 100 individuals. Set the simulation length to 60 days. Keep mortality at 0.01 and reproduction rate at 0.5 eggs/adult/day. Click “Run Simulation” and watch the population change across stages. Task 3 Which stage dominates early in the simulation? Do you observe steady growth, decline, or oscillations in total population? Why does the adult population not grow indefinitely, even with reproduction? Part 3 — Varying Mortality and Reproduction Now explore how environmental factors alter population outcomes. Increase mortality to 0.05 and rerun the model. Observe how quickly the population declines. Set mortality = 0.01 but increase reproduction rate = 1.0. Compare total population and stage composition over time. Task 4 How sensitive is the total population size to changes in mortality vs. reproduction? Which life stage’s survival appears most critical for population persistence? What environmental scenarios might correspond to high or low mortality? Part 4 — Understanding Stochasticity This model uses multinomial sampling to represent daily survival and transitions. That means even with the same parameters, each run can produce slightly different results. Run the simulation several times with identical settings. Observe how much variation appears in the trajectories. Task 5 What does this variability represent biologically? How might random variation lead to different population fates in small mosquito populations? Why might stochastic models better reflect real-world ecological dynamics than deterministic ones? Part 5 — Experimenting with Development Rates Try altering the transition probabilities between life stages: Increase \\(p_{EL}\\), \\(p_{LP}\\), and \\(p_{PA}\\) to simulate warmer conditions (faster development). Decrease these same probabilities to mimic cooler conditions. Compare total population trends under both scenarios. Task 6 How does temperature (through development rate) affect the timing and amplitude of population peaks? Why might warmer conditions lead to more overlapping generations? Part 6 — Synthesis and Reflection Task 7 What does the long-term population trend look like — stable, growing, or declining? Which parameters (mortality, reproduction, or stage durations) most strongly control equilibrium size? How could this type of model be extended to include disease transmission, predation, or control interventions? Learning Outcomes By the end of this lab, you should be able to: Describe how a Markov chain represents biological life stages. Interpret the transition matrix as a set of daily probabilities of survival and development. Explain how mortality and reproduction affect mosquito population dynamics. Recognize how stochasticity influences ecological outcomes. Apply matrix-based reasoning to analyze complex population processes. E.8 Lab 7 - Forest Disturbance Dynamics and Monte Carlo Simulation In this lab, you’ll explore how disturbance and regrowth interact to shape long-term forest dynamics. Using an interactive Shiny app, you will simulate a forest where each patch grows logistically until a disturbance event (e.g., fire) resets it to a low biomass value. You’ll use this app to: Examine the logistic growth process in a spatial grid. Investigate how disturbance frequency affects total forest biomass. Explore age structure as an emergent property of disturbance and regrowth. Use Monte Carlo simulations to capture stochastic variability in forest trajectories. Background: Logistic Growth and Disturbance At the patch level, forest biomass follows a logistic growth model: \\[ B(t) = \\frac{K}{1 + A e^{-r t}} \\] with differential form: \\[ \\frac{dB}{dt} = rB\\left(1 - \\frac{B}{K}\\right) \\] Here: \\(B(t)\\): biomass at time \\(t\\) \\(K\\): carrying capacity (maximum sustainable biomass) \\(r\\): intrinsic growth rate \\(A = \\frac{K - B_0}{B_0}\\): constant determined by initial biomass \\(B_0\\) Disturbances are modeled as random events that reset biomass to a small fraction of \\(K\\). Each cell (or patch) in the forest grid has a probability \\(p_{disturb}\\) of being disturbed each year. \\[ D_{i,j}(t) \\sim \\text{Bernoulli}(p_{disturb}) \\] When a disturbance occurs, biomass \\(B_{i,j}\\) is reset to \\(B_0 = fK\\), where \\(f\\) is a user-defined fraction. Model Overview At the landscape scale, total forest biomass is the sum of all patches: \\[ B_{total}(t) = \\sum_{i,j} B_{i,j}(t) \\] As the model runs over many years: Individual cells grow according to the logistic equation. Disturbances occur randomly and cause local biomass loss. The forest continually regrows and burns, reaching a dynamic equilibrium. By running the model multiple times (Monte Carlo replicates), we can estimate both expected biomass trajectories and variability among runs: \\[ E[B(t)] \\approx \\frac{1}{N} \\sum_{i=1}^{N} B_i(t) \\] Conceptual Exploration: Before Using the App Task 1 Discuss the following with your group: What types of real-world disturbances might this model represent? Why do forests tend to recover after disturbance instead of collapsing entirely? What assumptions of this model might not hold true in nature (e.g., spatial independence, fixed probabilities)? How might the time since last disturbance (age) influence biomass and fire risk? Part 1 — Exploring Logistic Growth at the Patch Scale Set disturbance probability to 0 to remove stochastic effects. Choose reasonable values for: \\(r = 0.2\\) \\(K = 100\\) Grid size \\(= 10 \\times 10\\) Run the simulation for 100 years. Task 2 Describe how biomass changes over time. What shape does the total biomass curve take, and why? How does the logistic model’s upper limit \\(K\\) control long-term behavior? Part 2 — Introducing Disturbance Set disturbance probability = 0.05. Observe the Grid View as the simulation runs — patches of biomass reset randomly each year. Examine the Age Distribution tab to see how time-since-disturbance changes over time. Task 3 What pattern emerges in the age distribution? Why does the distribution often have many young patches and few very old ones? How does this relate to the disturbance rate \\(p_{disturb}\\)? Part 3 — Adjusting Regrowth After Fire Use the slider for \\(B_0\\) as a fraction of K to adjust how much biomass remains after disturbance. Compare runs where \\(B_0 / K = 0.05\\) versus \\(0.2\\). Keep all other parameters fixed. Task 4 How does changing post-disturbance biomass affect total forest recovery? What happens to average biomass at equilibrium? Which scenario represents a more resilient forest, and why? Part 4 — Total Biomass Over Time Open the “Total Biomass” tab. Run the simulation for 200–250 years to observe how biomass fluctuates over long time spans. Note the emergence of a stable average but variable trajectory. Task 5 Describe the long-term pattern of total biomass. Does the system reach a steady state, oscillate, or remain irregular? What ecological factors could produce similar long-term dynamics? Part 5 — Monte Carlo Simulations Navigate to the “Monte Carlo Simulation” tab. Set replicates = 20 and click “Run Monte Carlo Simulation.” Observe the mean curve (dark green) and the shaded uncertainty region. Task 6 What does the shaded area represent? Why do repeated random simulations produce slightly different biomass trajectories? How does the number of replicates affect the smoothness and reliability of the mean estimate? Part 6 — Exploring Disturbance Frequency Try different disturbance probabilities: 0.01, 0.05, 0.1. Keep all other parameters constant. Compare total biomass and variability across runs. Task 7 How does increasing disturbance frequency change mean biomass and age structure? Does the forest ever fully recover between disturbances? What tradeoff emerges between disturbance frequency and average biomass? Part 7 — Reflection and Application Task 8 How does this model help explain the balance between disturbance and recovery in forest ecosystems? In what ways is this model too simple for real forests? Suggest one modification that would make the model more realistic (e.g., spatial fire spread, variable \\(r\\), or changing climate effects). Learning Outcomes By the end of this lab, you should be able to: Explain how logistic growth and disturbance interact to shape forest dynamics. Interpret spatial and temporal variability in biomass and age structure. Use Monte Carlo methods to quantify uncertainty in ecological simulations. Discuss the balance between resilience and disturbance frequency in sustaining forests. "],["tutorial-getting-started-with-rstudio.html", "Appendix F Tutorial: Getting Started with RStudio F.1 What is RStudio? F.2 The RStudio Interface F.3 Running Code F.4 Projects in RStudio F.5 Working with Packages F.6 Writing and Saving Scripts F.7 Plotting Example F.8 Data Visualization Tutorial F.9 Prompting and Building a Complex Simulation Model - Roulette", " Appendix F Tutorial: Getting Started with RStudio F.1 What is RStudio? R is a programming language for statistics, data analysis, and visualization. RStudio is an integrated development environment (IDE) that makes R easier to use with a friendly interface. F.2 The RStudio Interface When you open RStudio, you’ll usually see four main panes: Source Pane (Top-Left) Where you write and edit R scripts (.R), RMarkdown (.Rmd), or notebooks. You can run code line by line or in chunks. Console (Bottom-Left) Where R actually runs the code. You can type commands directly here for quick tests. Environment/History (Top-Right) Environment: Shows the objects (data, variables, functions) you’ve created. History: Keeps track of commands you’ve previously run. Files/Plots/Packages/Help/Viewer (Bottom-Right) Files: Navigate your project folder. Plots: Displays graphs you generate. Packages: Manage installed R packages. Help: Documentation for R functions. Viewer: Preview HTML outputs (e.g., from RMarkdown). F.3 Running Code Type directly into the Console and hit Enter. Or, write code in the Source Pane and: Run a single line: Ctrl + Enter (Windows) or Cmd + Enter (Mac). Run a whole script: Source button or Ctrl + Shift + Enter. F.4 Projects in RStudio Create a Project to keep related files together. File → New Project → New Directory (or link to an existing folder). Projects make it easier to manage code, datasets, and outputs without breaking file paths. F.5 Working with Packages Packages are collections of R functions, data, and documentation bundled together to extend the capabilities of base R. Think of them like “apps” you install on your phone — R comes with some built-in tools, but packages let you do much more specialized tasks. Why use packages? Packages provide extra functionality for tasks like data visualization, statistical modeling, spatial analysis, or machine learning. Where do they come from? Most packages are shared on CRAN (the Comprehensive R Archive Network), but you can also install from GitHub or other repositories. How do you use them? Install once per computer install.packages(&quot;ggplot2&quot;) Load every session (so R knows to use it) library(ggplot2) Examples of popular packages: ggplot2 → advanced graphics and plots dplyr → data wrangling and manipulation tidyr → reshaping datasets readr → reading CSV and text files shiny → building interactive web apps in R ✅ Key idea: Packages expand what R can do. Installing adds them to your computer, loading makes them available in your current session. F.6 Writing and Saving Scripts When you work in RStudio, you’ll often want to save your code so you can reuse it later, share it with others, or keep a record of what you did. This is where scripts come in. F.6.1 R Scripts (.R files) An R script is a plain text file that contains R code. You can write multiple lines of code and run them whenever you want, instead of typing directly into the Console. To create one: Go to File → New File → R Script (or use Ctrl + Shift + N / Cmd + Shift + N on Mac). To save: Use File → Save As... and give your file a name ending in .R. F.6.2 RMarkdown (.Rmd files) An RMarkdown file combines code, text, and output in one document. Useful for reports, homework assignments, or reproducible research. To create one: File → New File → RMarkdown You can include code chunks (inside triple backticks {r}) along with explanations in plain English. Output can be HTML, PDF, or Word documents. F.7 Plotting Example R makes it easy to create plots and visualize data. Plots always appear in the Plots tab (bottom-right pane in RStudio). F.7.1 Example 1: Simple Scatter Plot You can create your own vectors and plot them. # Create data x &lt;- 1:10 y &lt;- x^2 # Scatter plot plot(x, y, main = &quot;Simple Plot&quot;, xlab = &quot;x&quot;, ylab = &quot;y^2&quot;) This will generate a basic scatter plot of numbers 1–10 against their squares. F.7.2 Example 2: Using a Built-In Dataset (cars) R comes with many built-in datasets. The cars dataset contains two columns: speed → speed of cars (in mph) dist → stopping distances (in feet) We can quickly make a scatter plot to explore the relationship. # Look at the first rows of the dataset head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 # Create a scatter plot plot(cars$speed, cars$dist, main = &quot;Stopping Distance vs Speed&quot;, xlab = &quot;Speed (mph)&quot;, ylab = &quot;Stopping Distance (ft)&quot;, col = &quot;blue&quot;, pch = 19) col = “blue” makes the points blue. pch = 19 makes the points solid circles. F.7.3 Example 3: Adding a Trend Line You can add extra layers to your plots. For instance, let’s fit a simple linear model and add the regression line to the cars plot. # Fit a linear model model &lt;- lm(dist ~ speed, data = cars) # Plot again plot(cars$speed, cars$dist, main = &quot;Stopping Distance vs Speed with Trend Line&quot;, xlab = &quot;Speed (mph)&quot;, ylab = &quot;Stopping Distance (ft)&quot;, col = &quot;darkgreen&quot;, pch = 16) # Add the fitted line abline(model, col = &quot;red&quot;, lwd = 2) F.7.3.1 ggplot2 Version The ggplot2 package provides more control and produces publication-quality graphics. # Load ggplot2 library(ggplot2) # Create scatter plot with regression line ggplot(cars, aes(x = speed, y = dist)) + geom_point(color = &quot;darkgreen&quot;, size = 3) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs( title = &quot;Stopping Distance vs Speed with Trend Line&quot;, x = &quot;Speed (mph)&quot;, y = &quot;Stopping Distance (ft)&quot; ) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ✅ Using base R, you get quick and simple plots. ✅ Using ggplot2, you get more flexible, customizable, and professional-looking plots. F.8 Data Visualization Tutorial The focus of today’s session will be developing your data visualization skills. Visualizations are more than just decoration — they are essential tools for both communication and analysis. A clear, well-designed plot can reveal relationships, trends, and outliers that might be hidden in raw data, helping you generate insights and guide further exploration. At the same time, effective visualizations allow you to communicate those insights to others in a way that is intuitive and impactful, bridging the gap between data and understanding. Work through this tutorial at your own pace: Data visualization – R for Data Science F.9 Prompting and Building a Complex Simulation Model - Roulette Today’s task will be to build a model of a Roulette table. Start simple and just get a the ability to spin and get a number within the range Incorporate the ability to bet on black / red Incorporate the ability to simulate 1000s of spins to see what the probabilistic outcome is Examine the impact of “0” and “00” Build in some playing rules “always bet on black” - or “if red or black comes up 3 times in a row, bet on the other” "]]
